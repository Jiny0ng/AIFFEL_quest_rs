{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEr1i7Opow4A"
      },
      "source": [
        "# 트랜스포머(Transformer)를 GPT-1 모델로 변경\n",
        "\n",
        "이 노트북은 기존의 인코더-디코더(Encoder-Decoder) 구조의 트랜스포머 모델을 GPT-1 논문(\"Improving Language Understanding by Generative Pre-Training\")의 핵심 아이디어를 적용하여 **디코더-온리(Decoder-Only)** 구조로 변경하는 과정을 담고 있습니다.\n",
        "\n",
        "### 주요 구조 변경 사항 및 근거\n",
        "\n",
        "| 변경 사항 | 내용 | 근거 |\n",
        "| :--- | :--- | :--- |\n",
        "| **인코더(Encoder) 블록 제거** | 입력 문장을 인코딩하는 인코더 스택 전체를 제거합니다. | GPT는 번역과 같은 Seq2Seq 과업이 아닌, 주어진 컨텍스트를 바탕으로 다음에 올 단어를 예측하는 **언어 모델(Language Model)**입니다. 따라서 입력과 출력을 분리하여 처리하는 인코더-디코더 구조가 불필요하며, 단일 디코더 블록만으로 구성됩니다. |\n",
        "| **디코더(Decoder) 블록 수정** | 디코더 레이어 내부에 존재하던 **Encoder-Decoder Attention** 부분을 제거합니다. | 인코더가 제거되었으므로, 인코더의 출력 결과를 참조하는 Encoder-Decoder Attention 또한 필요하지 않습니다. GPT의 디코더 블록은 오직 **Masked Self-Attention**과 **Feed-Forward Network** 두 개의 서브-레이어로만 구성됩니다. |\n",
        "| **데이터 전처리 방식 변경** | 질문(Q)과 답변(A)을 별도로 처리하지 않고, `질문 + 구분자 + 답변` 형태의 단일 시퀀스로 통합합니다. | GPT는 **자기회귀(Autoregressive)** 방식으로 이전 단어들을 바탕으로 다음 단어를 예측하며 학습합니다. 질문을 컨텍스트로, 답변을 생성해야 할 텍스트로 인식시키기 위해 두 문장을 자연스럽게 연결한 단일 데이터로 만들어 모델이 문맥을 학습하고 답변을 생성하도록 유도합니다. |\n",
        "| **입력 데이터 구성** | 모델의 최종 입력은 **토큰 임베딩**과 **포지셔널 임베딩(위치 정보)**의 합으로 구성됩니다. | GPT는 RNN과 달리 단어의 순서를 직접 학습할 수 없으므로, 각 토큰의 위치 정보를 임베딩 벡터에 더해줌으로써 모델이 단어의 순서와 위치를 이해할 수 있도록 합니다. |"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/work/transformer_chatbot/data/ && cd ~/work/transformer_chatbot/data/\n",
        "!wget https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L6Rlm-_o9xn",
        "outputId": "4b749e57-9313-4ab8-bd78-8b231ffdd959"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-20 08:41:54--  https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv [following]\n",
            "--2025-08-20 08:41:54--  https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 889842 (869K) [text/plain]\n",
            "Saving to: ‘ChatbotData.csv’\n",
            "\n",
            "ChatbotData.csv     100%[===================>] 868.99K  3.34MB/s    in 0.3s    \n",
            "\n",
            "2025-08-20 08:41:55 (3.34 MB/s) - ‘ChatbotData.csv’ saved [889842/889842]\n",
            "\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWDJakinow4B",
        "outputId": "fc71a333-641b-4f4e-f3e2-5994b01bbea0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n"
          ]
        }
      ],
      "source": [
        "# 1. 라이브러리 설치 및 임포트\n",
        "!pip install sentencepiece\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import sentencepiece as spm\n",
        "\n",
        "import csv\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pehfzW_2ow4C",
        "outputId": "e01d91f6-7646-4962-871d-9eca3d2bcdc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File ‘ChatbotData.csv’ already there; not retrieving.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# 2. 데이터 다운로드 및 토크나이저 학습\n",
        "!wget -nc https://github.com/songys/Chatbot_data/raw/master/ChatbotData.csv\n",
        "\n",
        "def read_raw_data(csv_path):\n",
        "    pairs = []\n",
        "    with open(csv_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader, None) # 헤더 스킵\n",
        "        for row in reader:\n",
        "            if len(row) > 1 and row[0] and row[1]:\n",
        "                pairs.append((row[0], row[1]))\n",
        "    return pairs\n",
        "\n",
        "# 말뭉치 파일 생성\n",
        "corpus_file = \"chatbot_corpus.txt\"\n",
        "raw_pairs = read_raw_data(\"ChatbotData.csv\")\n",
        "with open(corpus_file, 'w', encoding='utf-8') as f:\n",
        "    for q, a in raw_pairs:\n",
        "        f.write(q + \"\\n\")\n",
        "        f.write(a + \"\\n\")\n",
        "\n",
        "# SentencePiece BPE 모델 학습\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    f'--input={corpus_file} --model_prefix=spm_chatbot --vocab_size=8000 '\n",
        "    '--model_type=bpe --max_sentence_length=9999 '\n",
        "    '--pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3'\n",
        ")\n",
        "\n",
        "# 토크나이저 로드\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"spm_chatbot.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "69lHv12row4C"
      },
      "outputs": [],
      "source": [
        "# 3. GPT-1을 위한 데이터셋 및 위치 인코딩 구현\n",
        "\n",
        "def preprocess_sentence(sentence: str) -> str:\n",
        "    sentence = str(sentence).strip()\n",
        "    sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
        "    sentence = re.sub(r\"[\\u200b\\u200c\\u200d\\ufeff]\", \"\", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "\n",
        "class GPT1Dataset(Dataset):\n",
        "    def __init__(self, pairs, sp, max_length=50):\n",
        "        super().__init__()\n",
        "        self.sp = sp\n",
        "        self.max_length = max_length\n",
        "        self.data = []\n",
        "\n",
        "        bos_id = sp.bos_id()\n",
        "        eos_id = sp.eos_id()\n",
        "        pad_id = sp.pad_id()\n",
        "        sep_id = eos_id # 질문과 답변의 구분자로 EOS 토큰 ID를 사용\n",
        "\n",
        "        #토크나이저를 통해 질문+답변 형태로 바꾸어 시퀀스화 해준다.\n",
        "        for q_text, a_text in pairs:\n",
        "            q_text = preprocess_sentence(q_text)\n",
        "            a_text = preprocess_sentence(a_text)\n",
        "\n",
        "            q_ids = sp.EncodeAsIds(q_text)\n",
        "            a_ids = sp.EncodeAsIds(a_text)\n",
        "\n",
        "            #시퀀스 토큰\n",
        "            token_ids = [bos_id] + q_ids + [sep_id] + a_ids + [eos_id]\n",
        "\n",
        "            if len(token_ids) > max_length:\n",
        "                continue\n",
        "\n",
        "            padding = [pad_id] * (max_length - len(token_ids))\n",
        "            input_ids = token_ids + padding\n",
        "            target_ids = input_ids[1:] + [pad_id]\n",
        "\n",
        "            self.data.append({\"input_ids\": input_ids, \"target_ids\": target_ids})\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        input_ids = torch.tensor(sample[\"input_ids\"], dtype=torch.long)\n",
        "        target_ids = torch.tensor(sample[\"target_ids\"], dtype=torch.long)\n",
        "        return input_ids, target_ids\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(position, d_model)\n",
        "        pos = torch.arange(0, position, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDxKGarOow4C",
        "outputId": "05e44238-f31e-449b-84b9-db09b6f6c193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 입력 데이터가 성공적으로 구성되었습니다.\n",
            "\n",
            "--- 샘플 데이터 확인 ---\n",
            "입력 (Input)  : [2, 5566, 6957, 3207, 7063, 3, 4489, 211, 5936, 6916, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "타겟 (Target) : [5566, 6957, 3207, 7063, 3, 4489, 211, 5936, 6916, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "--------------------\n",
            "입력 (Decoded)  : 12시 땡! 하루가 또 가네요.\n",
            "타겟 (Decoded) : 12시 땡! 하루가 또 가네요.\n"
          ]
        }
      ],
      "source": [
        "# 4. 데이터셋 구성 확인\n",
        "pairs = read_raw_data(\"ChatbotData.csv\")\n",
        "gpt_dataset = GPT1Dataset(pairs, sp, max_length=50)\n",
        "dataloader = DataLoader(gpt_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "sample_input, sample_target = next(iter(dataloader))\n",
        "\n",
        "print(\"✅ 입력 데이터가 성공적으로 구성되었습니다.\\n\")\n",
        "print(\"--- 샘플 데이터 확인 ---\")\n",
        "print(f\"입력 (Input)  : {sample_input[0].tolist()}\")\n",
        "print(f\"타겟 (Target) : {sample_target[0].tolist()}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "decoded_input = sp.decode(sample_input[0].tolist())\n",
        "decoded_target = sp.decode(sample_target[0].tolist())\n",
        "\n",
        "print(f\"입력 (Decoded)  : {decoded_input}\")\n",
        "print(f\"타겟 (Decoded) : {decoded_target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ExNYa3ktow4C"
      },
      "outputs": [],
      "source": [
        "# 5. GPT-1 모델 아키텍처 구현\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    matmul_qk = torch.matmul(query, key.transpose(-1, -2))\n",
        "    depth = key.size(-1)\n",
        "    logits = matmul_qk / math.sqrt(depth)\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "    attention_weights = F.softmax(logits, dim=-1)\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "    return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        assert d_model % num_heads == 0\n",
        "        self.depth = d_model // num_heads\n",
        "\n",
        "        self.query_dense = nn.Linear(d_model, d_model)\n",
        "        self.key_dense = nn.Linear(d_model, d_model)\n",
        "        self.value_dense = nn.Linear(d_model, d_model)\n",
        "        self.out_dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        query = self.split_heads(self.query_dense(query), batch_size)\n",
        "        key = self.split_heads(self.key_dense(key), batch_size)\n",
        "        value = self.split_heads(self.value_dense(value), batch_size)\n",
        "\n",
        "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
        "        scaled_attention = scaled_attention.permute(0, 2, 1, 3).contiguous()\n",
        "        concat_attention = scaled_attention.view(batch_size, -1, self.d_model)\n",
        "        output = self.out_dense(concat_attention)\n",
        "        return output\n",
        "\n",
        "def create_look_ahead_mask(x):\n",
        "    seq_len = x.size(1)\n",
        "    look_ahead_mask = 1 - torch.tril(torch.ones((seq_len, seq_len), device=x.device))\n",
        "    padding_mask = (x == sp.pad_id()).float().unsqueeze(1).unsqueeze(2)\n",
        "    combined_mask = torch.max(look_ahead_mask, padding_mask.squeeze(2))\n",
        "    return combined_mask.unsqueeze(1)\n",
        "\n",
        "class GPT1DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ff_dim, dropout=0.1):\n",
        "        super(GPT1DecoderLayer, self).__init__()\n",
        "        self.self_mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model, eps=1e-6)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, look_ahead_mask):\n",
        "        attn_output = self.self_mha(x, x, x, mask=look_ahead_mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.norm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.norm2(out1 + ffn_output)\n",
        "        return out2\n",
        "\n",
        "class GPT1(nn.Module):\n",
        "    def __init__(self, vocab_size, num_layers, ff_dim, d_model, num_heads, max_len, dropout=0.1):\n",
        "        super(GPT1, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(max_len, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.dec_layers = nn.ModuleList([\n",
        "            GPT1DecoderLayer(d_model, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        look_ahead_mask = create_look_ahead_mask(x)\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.dec_layers:\n",
        "            x = layer(x, look_ahead_mask)\n",
        "        logits = self.final_linear(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxmRzOwbow4C",
        "outputId": "46beb7e5-1d25-4b2d-85c9-f388375af3f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# 6. 모델 및 학습 파라미터 설정\n",
        "\n",
        "# 하이퍼파라미터 (GPT-1 Small과 유사하게 설정)\n",
        "VOCAB_SIZE = 8000\n",
        "NUM_LAYERS = 12\n",
        "D_MODEL = 768\n",
        "NUM_HEADS = 12\n",
        "UNITS = 3072 # Feed-Forward 차원 (d_model * 4)\n",
        "MAX_LEN = 50\n",
        "DROPOUT = 0.1\n",
        "EPOCHS = 5 # 예시로 5 에폭만 설정\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 모델 생성\n",
        "gpt_model = GPT1(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    ff_dim=UNITS,\n",
        "    max_len=MAX_LEN,\n",
        "    dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "# 손실 함수 및 옵티마이저\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=sp.pad_id())\n",
        "optimizer = optim.Adam(gpt_model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "# 데이터로더\n",
        "train_dataloader = DataLoader(gpt_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30FYDYh3ow4C",
        "outputId": "8f55f891-ec4d-439c-ad75-50641330f2f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1/5, Step 0] Loss: 9.2061\n"
          ]
        }
      ],
      "source": [
        "# 7. 학습 루프 정의\n",
        "\n",
        "def accuracy_function(y_pred, y_true, pad_id=0):\n",
        "    preds = y_pred.argmax(dim=-1)\n",
        "    mask = (y_true != pad_id)\n",
        "    correct = (preds == y_true) & mask\n",
        "    acc = correct.float().sum() / mask.float().sum()\n",
        "    return acc\n",
        "\n",
        "def train_model(model, dataloader, optimizer, loss_fn, num_epochs, device):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss, total_acc = 0, 0\n",
        "\n",
        "        for step, (input_ids, target_ids) in enumerate(dataloader):\n",
        "            input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(input_ids)\n",
        "\n",
        "            loss = loss_fn(logits.permute(0, 2, 1), target_ids)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_acc += accuracy_function(logits, target_ids, pad_id=sp.pad_id())\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                print(f\"[Epoch {epoch+1}/{num_epochs}, Step {step}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        avg_acc = total_acc / len(dataloader)\n",
        "        print(f\"\\nEpoch {epoch+1} Completed - Avg Loss: {avg_loss:.4f}, Avg Acc: {avg_acc:.4f}\\n\")\n",
        "\n",
        "# Pre-train을 위한 학습 시작\n",
        "train_model(gpt_model, train_dataloader, optimizer, loss_function, EPOCHS, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uDR_Dg3Low4D"
      },
      "outputs": [],
      "source": [
        "# 8. 문장 생성(추론) 함수 정의\n",
        "\n",
        "def sentence_generation(model, sentence, tokenizer, max_gen_len=50, device='cpu'):\n",
        "    model.eval()\n",
        "\n",
        "    bos_id = tokenizer.bos_id()\n",
        "    eos_id = tokenizer.eos_id()\n",
        "    sep_id = eos_id\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    token_ids = tokenizer.encode(sentence)\n",
        "\n",
        "    # 모델 입력: [BOS] + Q_ids + [SEP]\n",
        "    input_ids = [bos_id] + token_ids + [sep_id]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_gen_len):\n",
        "            input_tensor = torch.tensor([input_ids], dtype=torch.long, device=device)\n",
        "            logits = model(input_tensor)\n",
        "            pred_id = logits.argmax(dim=-1)[0, -1].item()\n",
        "\n",
        "            if pred_id == eos_id:\n",
        "                break\n",
        "\n",
        "            input_ids.append(pred_id)\n",
        "\n",
        "    start_of_answer = len(token_ids) + 2\n",
        "    generated_sentence = tokenizer.decode(input_ids[start_of_answer:])\n",
        "\n",
        "    print(\"입력 :\", sentence)\n",
        "    print(\"출력 :\", generated_sentence)\n",
        "    return generated_sentence\n",
        "\n",
        "# 추론 예시 (실제로는 학습 후에 실행해야 합니다.)\n",
        "# sentence_generation(gpt_model, \"오늘 하루 어땠어?\", sp, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWnYxfIopWtQ"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}