{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd23715a-d076-4d2e-993c-c4ac63e11542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: summa in /opt/conda/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.19 in /opt/conda/lib/python3.12/site-packages (from summa) (1.15.2)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /opt/conda/lib/python3.12/site-packages (from scipy>=0.19->summa) (2.2.6)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade summa\n",
    "!pip install --upgrade nltk #3.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f47f2ece-6328-4501-93ad-e8e526a6f3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1\n",
      "2.7.1+cu118\n",
      "2.3.0\n",
      "1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import nltk\n",
    "import torch\n",
    "import summa\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import urllib.request\n",
    "import warnings\n",
    "\n",
    "print(nltk.__version__)\n",
    "print(torch.__version__)\n",
    "print(pd.__version__)\n",
    "print(version('summa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5787c2b3-281d-4b80-ad27-dcd7feb18fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48218</th>\n",
       "      <td>Technology is changing at the speed of thought...</td>\n",
       "      <td>Speaking at the World Government Summit in Dub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19251</th>\n",
       "      <td>1 more dies at Bihar shelter home where 2 died...</td>\n",
       "      <td>A 27-year-old woman staying at Aasra shelter h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56886</th>\n",
       "      <td>New poster of Kaala released on Rajinikanth's ...</td>\n",
       "      <td>A new poster of Rajinikanth starrer 'Kaala' wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14760</th>\n",
       "      <td>Non-alliance with Mayawati won't have any impa...</td>\n",
       "      <td>Congress' Madhya Pradesh unit President Kamal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36498</th>\n",
       "      <td>Oil spill in Australia would be welcome boost ...</td>\n",
       "      <td>Oil giant BP had claimed that an oil spill off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96457</th>\n",
       "      <td>Kenyan forces kill 31 al-Shabaab militants in ...</td>\n",
       "      <td>Kenyan troops in Somalia reportedly killed 31 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28877</th>\n",
       "      <td>Plane carrying Peru World Cup fans fills with ...</td>\n",
       "      <td>An aircraft carrying around 70 Peruvian FIFA W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50070</th>\n",
       "      <td>Girls in short dress inviting Nirbhaya-type ra...</td>\n",
       "      <td>A teacher in Raipur's Kendriya Vidyalaya alleg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61989</th>\n",
       "      <td>Saudi rejects Royal Court's claim of Prince's ...</td>\n",
       "      <td>Saudi Arabia's Information Ministry has reject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73592</th>\n",
       "      <td>Women's college bans jeans, phones to promote ...</td>\n",
       "      <td>A womenÃ¢ÂÂs college in JharkhandÃ¢ÂÂs Dal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "48218  Technology is changing at the speed of thought...   \n",
       "19251  1 more dies at Bihar shelter home where 2 died...   \n",
       "56886  New poster of Kaala released on Rajinikanth's ...   \n",
       "14760  Non-alliance with Mayawati won't have any impa...   \n",
       "36498  Oil spill in Australia would be welcome boost ...   \n",
       "96457  Kenyan forces kill 31 al-Shabaab militants in ...   \n",
       "28877  Plane carrying Peru World Cup fans fills with ...   \n",
       "50070  Girls in short dress inviting Nirbhaya-type ra...   \n",
       "61989  Saudi rejects Royal Court's claim of Prince's ...   \n",
       "73592  Women's college bans jeans, phones to promote ...   \n",
       "\n",
       "                                                    text  \n",
       "48218  Speaking at the World Government Summit in Dub...  \n",
       "19251  A 27-year-old woman staying at Aasra shelter h...  \n",
       "56886  A new poster of Rajinikanth starrer 'Kaala' wa...  \n",
       "14760  Congress' Madhya Pradesh unit President Kamal ...  \n",
       "36498  Oil giant BP had claimed that an oil spill off...  \n",
       "96457  Kenyan troops in Somalia reportedly killed 31 ...  \n",
       "28877  An aircraft carrying around 70 Peruvian FIFA W...  \n",
       "50070  A teacher in Raipur's Kendriya Vidyalaya alleg...  \n",
       "61989  Saudi Arabia's Information Ministry has reject...  \n",
       "73592  A womenÃ¢ÂÂs college in JharkhandÃ¢ÂÂs Dal...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e87b6-ca85-4756-be85-619a06972aac",
   "metadata": {},
   "source": [
    "데이터 정리\n",
    "1. 중복샘플 및 NULL 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745907a9-c889-4cb4-8269-f89eec478717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(subset = ['text'], inplace=True)\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f7bd11-421c-449a-8369-4e14ed76dde4",
   "metadata": {},
   "source": [
    "2. 정규화 및 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06656cc9-39f2-4c75-b4a0-f58d5e2a4d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "052e7480-5ee4-499f-a2fb-7cc376151130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "\n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72065cc1-3d42-4710-a988-139fa4852bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.12/site-packages (6.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c853886e-5f31-43c4-ab66-72a69e71c78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 전처리 후 결과:  ['saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys almost years work experience program upgrad degree career support helped transition data scientist tech mahindra salary hike upgrad online power learning powered lakh careers', 'kunal shah credit card bill payment platform cred gave users chance win free food swiggy one year pranav kaushik delhi techie bagged reward spending cred coins users get one cred coin per rupee bill paid used avail rewards brands like ixigo bookmyshow ubereats cult fit', 'new zealand defeated india wickets fourth odi hamilton thursday win first match five match odi series india lost international match rohit sharma captaincy consecutive victories dating back march match witnessed india getting seventh lowest total odi cricket history', 'aegon life iterm insurance plan customers enjoy tax benefits premiums paid save taxes plan provides life cover age years also customers options insure critical illnesses disability accidental death benefit rider life cover age years', 'speaking sexual harassment allegations rajkumar hirani sonam kapoor said known hirani many years true metoo movement get derailed metoo movement always believe woman case need reserve judgment added hirani accused assistant worked sanju']\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다.\n",
    "clean_text = []\n",
    "\n",
    "for sentence in data['text']:\n",
    "    clean_text.append(preprocess_sentence(sentence))\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(\"Text 전처리 후 결과: \", clean_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7db44741-2bc6-4b85-9149-c55ea1141eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 전처리 후 결과:  ['upgrad learner switches to career in ml al with salary hike', 'delhi techie wins free food from swiggy for one year on cred', 'new zealand end rohit sharma led india match winning streak', 'aegon life iterm insurance plan helps customers save tax', 'have known hirani for yrs what if metoo claims are not true sonam']\n"
     ]
    }
   ],
   "source": [
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다.\n",
    "clean_headlines = []\n",
    "\n",
    "for sentence in data['headlines']:\n",
    "    clean_headlines.append(preprocess_sentence(sentence, remove_stopwords=False))\n",
    "\n",
    "print(\"Summary 전처리 후 결과: \", clean_headlines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c505811f-43a1-4bf8-b569-6299c052c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['headlines'] = clean_headlines\n",
    "data['text'] = clean_text\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6026f10-00b9-412d-8007-4ff4cfef0d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines    0\n",
      "text         0\n",
      "dtype: int64\n",
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())\n",
    "\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed055da4-3d1b-414f-abca-e301964212e5",
   "metadata": {},
   "source": [
    "중복샘플이 없었고, 정규화를해도 샘플수에 변화가 없는걸로 보아 불용어가 없었다는것을 알 수 있다.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bad4e6fc-6ab3-4d3c-aefa-5a2770c20c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 41 이하인 샘플의 비율: 0.9549613664091094\n",
      "전체 샘플 중 길이가 11 이하인 샘플의 비율: 0.9449877999186661\n"
     ]
    }
   ],
   "source": [
    "text_len = [len(s.split()) for s in data['text']]\n",
    "summary_len = [len(s.split()) for s in data['headlines']]\n",
    "\n",
    "text_max_len = 41\n",
    "summary_max_len = 11\n",
    "#최대길이는 약 95~97%에 맞추는게 좋다고 하여 위 값들로 결정\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
    "\n",
    "below_threshold_len(text_max_len, data['text'])\n",
    "below_threshold_len(summary_max_len, data['headlines'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bbcbd86f-dbcc-471e-a010-d793cf1344a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88749\n"
     ]
    }
   ],
   "source": [
    "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['headlines'].apply(lambda x: len(x.split()) <= summary_max_len)]\n",
    "\n",
    "print('전체 샘플수 :', (len(data)))\n",
    "#길이가 너무 긴 데이터들을 제외함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05aff50b-a45b-4205-84f3-30f96be99d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
    "\n",
    "encoder_input = np.array(data['text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79cd686f-5140-4468-8a23-dbce9f88a161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 71000\n",
      "훈련 레이블의 개수 : 71000\n",
      "테스트 데이터의 개수 : 17749\n",
      "테스트 레이블의 개수 : 17749\n"
     ]
    }
   ],
   "source": [
    "#train, test data split\n",
    "#encoder_input과 크기와 형태만 같은 랜덤 스퀸스 생성\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "#위 시퀸스를 바탕으로 샘플순서를 정의\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "\n",
    "#전체 데이터의 20%크기를 구함\n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "\n",
    "#구한 크기를 바탕으로 전체 데이터 양분\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e261753d-4e03-4ac9-ac17-ea5e6bcaed2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_tokenizer(text): # 토크나이저 정의\n",
    "    text = text.lower()  # 소문자로 변환\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)  # 특수문자 제거\n",
    "    return text.split()  # 공백 기준 토큰화\n",
    "\n",
    "def build_vocab(texts):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # 패딩과 UNK 토큰 추가\n",
    "    word_counter = Counter()\n",
    "\n",
    "    for text in texts:\n",
    "        word_counter.update(src_tokenizer(text))  # 단어 빈도수 계산\n",
    "\n",
    "    # 단어 집합 생성 (빈도가 높은 순서대로)\n",
    "    for word, _ in word_counter.most_common():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "src_vocab = build_vocab(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5db219b5-9429-425d-a058-7f940f61efa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 72382\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 49074\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 23308\n",
      "단어 집합에서 희귀 단어의 비율: 67.79862396728468\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.2189023496622124\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "\n",
    "# 전처리된 데이터 사용\n",
    "text_data = data['text'].tolist()\n",
    "summary_data = data['headlines'].tolist()\n",
    "# 단어 빈도수 계산\n",
    "word_counter = Counter()\n",
    "for text in text_data:\n",
    "    word_counter.update(text.split())\n",
    "\n",
    "total_cnt = len(word_counter)  # 전체 단어 개수\n",
    "total_freq = sum(word_counter.values())  # 전체 단어 등장 횟수\n",
    "rare_cnt = sum(1 for count in word_counter.values() if count < threshold)  # 희귀 단어 개수\n",
    "rare_freq = sum(count for count in word_counter.values() if count < threshold)  # 희귀 단어 등장 횟수\n",
    "\n",
    "# 희귀 단어를 제외한 단어 사전 구축\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # 패딩 및 미등록 단어 추가\n",
    "word_index = {word: idx + 2 for idx, (word, count) in enumerate(word_counter.items()) if count >= threshold}\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf78a6d-fe71-4ca9-a625-9050ce8dc431",
   "metadata": {},
   "source": [
    "---\n",
    "7회 미만 등장하는 단어의 비율은 약 68%이며\n",
    "이마저도 평균 3회 등장\n",
    "즉 없어도 상관없는 단어들이기 때문에 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "33ee4dbe-65b4-491f-b982-00a83996f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 15000\n",
    "\n",
    "def build_limited_vocab(texts, vocab_size):\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}  # 패딩과 UNK 토큰 추가\n",
    "    word_counter = Counter()\n",
    "\n",
    "    for text in texts:\n",
    "        word_counter.update(src_tokenizer(text))  # 단어 빈도수 계산\n",
    "\n",
    "    # 빈도가 높은 상위 vocab_size - 2개 단어만 선택 (PAD, UNK 포함)\n",
    "    for word, _ in word_counter.most_common(vocab_size - 2):\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "src_vocab = build_limited_vocab(encoder_input_train, src_vocab_size)\n",
    "\n",
    "def text_to_sequence(texts, vocab):\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        sequence = [vocab.get(word, vocab[\"<UNK>\"]) for word in src_tokenizer(text)]\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "# 텍스트 데이터 정수 시퀀스로 변환\n",
    "encoder_input_train_seq = text_to_sequence(encoder_input_train, src_vocab)\n",
    "encoder_input_test_seq = text_to_sequence(encoder_input_test, src_vocab)\n",
    "\n",
    "def tar_tokenizer(text):\n",
    "    text = text.lower()  # 소문자로 변환\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)  # 특수문자 제거\n",
    "    return text.split()  # 공백 기준 토큰화\n",
    "\n",
    "tar_vocab = build_vocab(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b62f5db9-3628-436d-94d5-5fa9719b6e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 28912\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 18984\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 9928\n",
      "단어 집합에서 희귀 단어의 비율: 65.66131710016602\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.050808471462564\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "\n",
    "word_counter = Counter()\n",
    "for text in decoder_input_train:\n",
    "    word_counter.update(tar_tokenizer(text))  # 각 문장의 단어 빈도 계산\n",
    "\n",
    "# 전체 단어 개수 및 등장 빈도 계산\n",
    "total_cnt = len(word_counter)  # 전체 단어 개수\n",
    "total_freq = sum(word_counter.values())  # 전체 단어 등장 횟수\n",
    "rare_cnt = sum(1 for count in word_counter.values() if count < threshold)  # 희귀 단어 개수\n",
    "rare_freq = sum(count for count in word_counter.values() if count < threshold)  # 희귀 단어 등장 횟수\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d0ca23a-118f-4e0c-aab9-7431e1d33490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[2, 16, 178, 1180, 69, 156, 313, 1, 711, 156, 5, 97], [2, 338, 128, 177, 11, 1697, 86, 1, 1, 5, 342], [2, 253, 1, 381, 1618, 1, 245, 17, 74, 30, 8, 407], [2, 36, 1, 1, 18, 1, 712, 1, 92, 1415, 5, 58], [2, 243, 261, 107, 42, 1, 1, 7, 824, 1, 1]]\n",
      "target\n",
      "decoder  [[16, 178, 1180, 69, 156, 313, 1, 711, 156, 5, 97, 3], [338, 128, 177, 11, 1697, 86, 1, 1, 5, 342, 3], [253, 1, 381, 1618, 1, 245, 17, 74, 30, 8, 407, 3], [36, 1, 1, 18, 1, 712, 1, 92, 1415, 5, 58, 3], [243, 261, 107, 42, 1, 1, 7, 824, 1, 1, 3]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab_size = 2000\n",
    "tar_vocab = build_limited_vocab(decoder_input_train + decoder_target_train, tar_vocab_size)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train_seq = text_to_sequence(decoder_input_train, tar_vocab)\n",
    "decoder_target_train_seq = text_to_sequence(decoder_target_train, tar_vocab)\n",
    "decoder_input_test_seq = text_to_sequence(decoder_input_test, tar_vocab)\n",
    "decoder_target_test_seq = text_to_sequence(decoder_target_test, tar_vocab)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train_seq[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train_seq[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ca511e2-769a-4384-907f-74b31193c0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 0\n",
      "삭제할 테스트 데이터의 개수 : 0\n",
      "훈련 데이터의 개수 : 71000\n",
      "훈련 레이블의 개수 : 71000\n",
      "테스트 데이터의 개수 : 17749\n",
      "테스트 레이블의 개수 : 17749\n"
     ]
    }
   ],
   "source": [
    "#단어를 강제로 줄여 길이가 0이 된 데이터를 삭제\n",
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0461a7a7-72cb-4606-b6cb-2666ca851b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 텐서 변환 함수 (리스트 → PyTorch 텐서)\n",
    "def convert_to_tensor(sequences):\n",
    "    return [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "\n",
    "# 패딩 적용 함수 (PyTorch `pad_sequence()` 활용)\n",
    "def pad_sequences_pytorch(sequences, maxlen, padding_value=0):\n",
    "    sequences = convert_to_tensor(sequences)  # 리스트를 텐서로 변환\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True, padding_value=padding_value)  # 패딩 적용\n",
    "    return padded_seqs[:, :maxlen]  # maxlen 길이로 자르기 (최대 길이 초과 방지)\n",
    "\n",
    "# 패딩 적용\n",
    "encoder_input_train = pad_sequences_pytorch(encoder_input_train_seq, maxlen=text_max_len)\n",
    "encoder_input_test = pad_sequences_pytorch(encoder_input_test_seq, maxlen=text_max_len)\n",
    "decoder_input_train = pad_sequences_pytorch(decoder_input_train_seq, maxlen=summary_max_len)\n",
    "decoder_target_train = pad_sequences_pytorch(decoder_target_train_seq, maxlen=summary_max_len)\n",
    "decoder_input_test = pad_sequences_pytorch(decoder_input_test_seq, maxlen=summary_max_len)\n",
    "decoder_target_test = pad_sequences_pytorch(decoder_target_test_seq, maxlen=summary_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d658e46a-18c6-400d-8f94-cee87f409404",
   "metadata": {},
   "source": [
    "---\n",
    "추상적 요약\n",
    "attention을 이용한 seq2seq구조를 이용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0ff9761-3eb0-4652-ba2d-3d66de499db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2SeqWithAttention(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(15000, 128)\n",
      "    (lstm): LSTM(128, 256, num_layers=3, batch_first=True, dropout=0.4)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(2000, 128)\n",
      "    (lstm): LSTM(128, 256, num_layers=3, batch_first=True, dropout=0.4)\n",
      "  )\n",
      "  (attention): Attention_dot(\n",
      "    (attn): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (v): Linear(in_features=256, out_features=1, bias=False)\n",
      "  )\n",
      "  (concat): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (output_layer): Linear(in_features=256, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "src_vocab_size = len(src_vocab)  # 단어 집합 크기\n",
    "\n",
    "# 인코더\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=3, dropout=0.4):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers=num_layers,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x): # 인코더의 임베딩 층\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)  # LSTM 실행\n",
    "        return output, hidden, cell\n",
    "\n",
    "# 인코더 모델 생성\n",
    "encoder = Encoder(src_vocab_size, embedding_dim, hidden_size, num_layers=3, dropout=0.4)\n",
    "\n",
    "# 디코더 설계\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, dropout=0.4, num_layers=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers=num_layers, dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden, cell): # 디코더의 임베딩 층\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))  # 초기 상태를 인코더에서 전달받음\n",
    "        return output, hidden, cell\n",
    "\n",
    "# 디코더 모델 생성\n",
    "decoder = Decoder(tar_vocab_size, embedding_dim, hidden_size, num_layers=3, dropout=0.4)\n",
    "\n",
    "\n",
    "class Attention_dot(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention_dot, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_size, hidden_size)  # 어텐션 가중치\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)  # 어텐션 가중치 벡터\n",
    "\n",
    "    def forward(self, decoder_output, encoder_outputs):\n",
    "        attn_weights = torch.bmm(decoder_output, encoder_outputs.transpose(1, 2))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)  # 어텐션 가중치 정규화\n",
    "        attn_out = torch.bmm(attn_weights, encoder_outputs)\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab_size, hidden_size):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.attention = Attention_dot(hidden_size)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)  # 어텐션 결합\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)  # 최종 출력층\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "        encoder_outputs, hidden, cell = self.encoder(encoder_input)\n",
    "        decoder_outputs, _, _ = self.decoder(decoder_input, hidden, cell)\n",
    "\n",
    "        # 어텐션 적용\n",
    "        attn_out = self.attention(decoder_outputs, encoder_outputs)\n",
    "\n",
    "        # 어텐션 결과와 디코더 출력 연결\n",
    "        decoder_concat_output = torch.cat((decoder_outputs, attn_out), dim=-1)\n",
    "\n",
    "        # 어텐션 결합 후 최종 출력\n",
    "        decoder_concat_output = torch.tanh(self.concat(decoder_concat_output))\n",
    "        output = self.output_layer(decoder_concat_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# 모델 생성\n",
    "model = Seq2SeqWithAttention(encoder, decoder, tar_vocab_size, hidden_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1d48158f-543d-465d-aeba-79037327e57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "learning_rate = 0.001\n",
    "patience = 2\n",
    "\n",
    "# 손실 함수 & 옵티마이저\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 패딩 토큰 무시\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# PyTorch DataLoader 설정\n",
    "train_dataset = TensorDataset(encoder_input_train, decoder_input_train, decoder_target_train)\n",
    "test_dataset = TensorDataset(encoder_input_test, decoder_input_test, decoder_target_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 학습 함수\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs, patience):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for encoder_input, decoder_input, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 정수형 변환\n",
    "            encoder_input = encoder_input.to(device).long()\n",
    "            decoder_input = decoder_input.to(device).long()\n",
    "            target = target.to(device).long()\n",
    "\n",
    "            # 모델 실행\n",
    "            output = model(encoder_input, decoder_input)\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            target = target.view(-1)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        # Validation loss 계산\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for encoder_input, decoder_input, target in test_loader:\n",
    "                encoder_input = encoder_input.to(device).long()\n",
    "                decoder_input = decoder_input.to(device).long()\n",
    "                target = target.to(device).long()\n",
    "\n",
    "                output = model(encoder_input, decoder_input)\n",
    "                output = output.view(-1, output.shape[-1])\n",
    "                target = target.view(-1)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping 조건\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2e0496b-b7bb-4798-929d-148dd32afc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 5.0821 | Val Loss: 4.8559\n",
      "Epoch 2/50 | Train Loss: 4.7063 | Val Loss: 4.5445\n",
      "Epoch 3/50 | Train Loss: 4.3604 | Val Loss: 4.1963\n",
      "Epoch 4/50 | Train Loss: 4.0675 | Val Loss: 3.9529\n",
      "Epoch 5/50 | Train Loss: 3.8377 | Val Loss: 3.7716\n",
      "Epoch 6/50 | Train Loss: 3.6569 | Val Loss: 3.6422\n",
      "Epoch 7/50 | Train Loss: 3.5019 | Val Loss: 3.5298\n",
      "Epoch 8/50 | Train Loss: 3.3682 | Val Loss: 3.4462\n",
      "Epoch 9/50 | Train Loss: 3.2505 | Val Loss: 3.3752\n",
      "Epoch 10/50 | Train Loss: 3.1436 | Val Loss: 3.3277\n",
      "Epoch 11/50 | Train Loss: 3.0532 | Val Loss: 3.2847\n",
      "Epoch 12/50 | Train Loss: 2.9684 | Val Loss: 3.2569\n",
      "Epoch 13/50 | Train Loss: 2.8898 | Val Loss: 3.2421\n",
      "Epoch 14/50 | Train Loss: 2.8188 | Val Loss: 3.2250\n",
      "Epoch 15/50 | Train Loss: 2.7529 | Val Loss: 3.2122\n",
      "Epoch 16/50 | Train Loss: 2.6900 | Val Loss: 3.2121\n",
      "Epoch 17/50 | Train Loss: 2.6336 | Val Loss: 3.2105\n",
      "Epoch 18/50 | Train Loss: 2.5757 | Val Loss: 3.2120\n",
      "Epoch 19/50 | Train Loss: 2.5232 | Val Loss: 3.2186\n",
      "Early stopping triggered at epoch 19\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, test_loader, criterion, optimizer, epochs=epochs, patience=patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b1fa6-dd56-45e3-86b6-f1baa5683223",
   "metadata": {},
   "source": [
    "---\n",
    "학습 후 정수행렬로 존재하는 텍스트 데이터를 문자로 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e01c4c39-822f-4f6c-aad0-1ecea6e99b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = {idx: word for word, idx in src_vocab.items()} # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_vocab # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = {idx: word for word, idx in tar_vocab.items()} # 요약 단어 집합에서 정수 -> 단어를 얻음\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "decoder.to(device)\n",
    "\n",
    "# 인코더 설계\n",
    "def encode_input(encoder, input_seq):\n",
    "    encoder_outputs, hidden, cell = encoder(input_seq)\n",
    "    return encoder_outputs, hidden, cell\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "num_layers = 3  # 디코더 LSTM 레이어 개수 (설정에 맞춰 조정)\n",
    "batch_size = 1\n",
    "\n",
    "decoder_state_input_h = torch.zeros((num_layers, batch_size, hidden_size), dtype=torch.float, device=device)\n",
    "decoder_state_input_c = torch.zeros((num_layers, batch_size, hidden_size), dtype=torch.float, device=device)\n",
    "decoder_input = torch.zeros((batch_size, 1), dtype=torch.long, device=device)\n",
    "\n",
    "dec_emb2 = decoder.embedding(decoder_input)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder(decoder_input, decoder_state_input_h, decoder_state_input_c)\n",
    "\n",
    "\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    def __init__(self, decoder, attention, hidden_size, vocab_size):\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.decoder = decoder  # 기존 디코더\n",
    "        self.attention = attention  # 어텐션 레이어\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)  # 어텐션 결합 레이어\n",
    "        self.output_layer = nn.Linear(hidden_size, vocab_size)  # 최종 출력층\n",
    "        self.softmax = nn.Softmax(dim=-1)  # 소프트맥스\n",
    "\n",
    "    def forward(self, decoder_inputs, decoder_hidden_state, decoder_state_h, decoder_state_c):\n",
    "        # 디코더 실행\n",
    "        decoder_outputs, state_h, state_c = self.decoder(decoder_inputs, decoder_state_h, decoder_state_c)\n",
    "\n",
    "        # 어텐션 적용\n",
    "        attn_out = self.attention(decoder_outputs, decoder_hidden_state)\n",
    "\n",
    "        # 어텐션과 디코더 출력 결합\n",
    "        decoder_concat_output = torch.cat((decoder_outputs, attn_out), dim=-1)\n",
    "        decoder_concat_output = torch.tanh(self.concat(decoder_concat_output))\n",
    "\n",
    "        # 최종 출력층 적용\n",
    "        decoder_outputs2 = self.softmax(self.output_layer(decoder_concat_output))\n",
    "\n",
    "        return decoder_outputs2, state_h, state_c\n",
    "\n",
    "# 기존 Attention 클래스 사용\n",
    "attention_layer = Attention_dot(hidden_size)\n",
    "\n",
    "# 디코더 모델 생성\n",
    "decoder_model = DecoderWithAttention(decoder, attention_layer, hidden_size, tar_vocab_size)\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq, encoder, decoder, tar_word_to_index, tar_index_to_word, text_max_len, summary_max_len, device):\n",
    "    # 입력을 PyTorch Tensor로 변환\n",
    "    input_seq = torch.tensor(input_seq, dtype=torch.long, device=device)\n",
    "\n",
    "    # 인코더 실행하여 초기 상태(hidden, cell) 얻기\n",
    "    with torch.no_grad():\n",
    "        e_out, e_h, e_c = encoder(input_seq)\n",
    "\n",
    "    e_out = e_out.repeat(1, text_max_len, 1)  # 차원 조정 (np.tile 대신 repeat 사용)\n",
    "\n",
    "    # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        # 디코더 실행\n",
    "        with torch.no_grad():\n",
    "            output_tokens, h, c = decoder(target_seq, e_h, e_c)\n",
    "\n",
    "        # 가장 높은 확률을 가진 단어 선택\n",
    "        sampled_token_index = torch.argmax(output_tokens[0, -1, :]).item()\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if sampled_token != 'eostoken':\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # 종료 조건: <eos>에 도달하거나 최대 길이를 초과하면 중단\n",
    "        if sampled_token == 'eostoken' or len(decoded_sentence.split()) >= (summary_max_len - 1):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태 업데이트\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ba0d21b1-da7e-46e2-9977-105a62c88eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 요약 : <UNK> mp slams maha govt over <UNK> <UNK> pm modi\n",
      "예측 요약 :  gold gold be be be most death train ever down\n",
      "\n",
      "\n",
      "실제 요약 : worker <UNK> tesla stopped him from <UNK> <UNK>\n",
      "예측 요약 :  gold before before new only people tweets rahul korea post\n",
      "\n",
      "\n",
      "실제 요약 : <UNK> flies by earth <UNK> than moon hrs after <UNK>\n",
      "예측 요약 :  salman be gold gold most most most most most most\n",
      "\n",
      "\n",
      "실제 요약 : korea tells korea to <UNK> deal to end korean war\n",
      "예측 요약 :  korea case be be be be most most days you\n",
      "\n",
      "\n",
      "실제 요약 : aus teen becomes youngest pilot to fly <UNK> around the\n",
      "예측 요약 :  years be be be be be be most death most\n",
      "\n",
      "\n",
      "실제 요약 : aap workers held for <UNK> fake news about haryana cm\n",
      "예측 요약 :  years be be be be be ever first last people\n",
      "\n",
      "\n",
      "실제 요약 : leak discovered in uk <UNK> biggest aircraft <UNK>\n",
      "예측 요약 :  years be be be be be be death most gets\n",
      "\n",
      "\n",
      "실제 요약 : china plans bullet train from kolkata to its <UNK> city\n",
      "예측 요약 :  months be be be be be million you gets gets\n",
      "\n",
      "\n",
      "실제 요약 : hope rajkummar ayushmann are <UNK> about <UNK> <UNK> karan\n",
      "예측 요약 :  months months months months mp mp korea korea post post\n",
      "\n",
      "\n",
      "실제 요약 : <UNK> on <UNK> duty for cow protection in rajasthan\n",
      "예측 요약 :  gold be be be most most death most gets down\n",
      "\n",
      "\n",
      "실제 요약 : mumbai police may close <UNK> pm modi <UNK> case report\n",
      "예측 요약 :  mn gold be new new new new new last people\n",
      "\n",
      "\n",
      "실제 요약 : km high study reveals earth <UNK> would die on mars\n",
      "예측 요약 :  fake bihar gold gold gold most kerala mn most most\n",
      "\n",
      "\n",
      "실제 요약 : of <UNK> <UNK> of <UNK> released police\n",
      "예측 요약 :  pakistan his gold be be ever ever video in death\n",
      "\n",
      "\n",
      "실제 요약 : microsoft <UNK> support for <UNK> old <UNK> <UNK>\n",
      "예측 요약 :  pakistan bjp case be be be be most you most\n",
      "\n",
      "\n",
      "실제 요약 : bjp himachal cm candidate <UNK> kumar <UNK> loses\n",
      "예측 요약 :  people be kerala uber dhoni party ever leader two leader\n",
      "\n",
      "\n",
      "실제 요약 : <UNK> launches st data centre in <UNK> india in <UNK>\n",
      "예측 요약 :  gold be be gold be most death most gets airport\n",
      "\n",
      "\n",
      "실제 요약 : sbi life insurance files for ipo to raise over crore\n",
      "예측 요약 :  bjp his gold new new new last people arrested day\n",
      "\n",
      "\n",
      "실제 요약 : anand <UNK> pm modi for <UNK> <UNK> set to <UNK>\n",
      "예측 요약 :  win gold be new fire apple korea korea india india\n",
      "\n",
      "\n",
      "실제 요약 : sri lanka bans plastic amid <UNK> <UNK> crisis\n",
      "예측 요약 :  years ipl be be be most pakistan day gets post\n",
      "\n",
      "\n",
      "실제 요약 : mumbai citizens <UNK> to take <UNK> after heavy <UNK>\n",
      "예측 요약 :  gold gold be be most most most most most most\n",
      "\n",
      "\n",
      "실제 요약 : <UNK> confirms <UNK> mn now <UNK> at nearly bn\n",
      "예측 요약 :  mn bjp bjp win gold be arrested get months day\n",
      "\n",
      "\n",
      "실제 요약 : <UNK> of <UNK> <UNK> in prasad that killed in taka\n",
      "예측 요약 :  months one be kerala free last women minister ever korea\n",
      "\n",
      "\n",
      "실제 요약 : arjun shares pic with mother on her th death anniversary\n",
      "예측 요약 :  months be be be most million under death you gets\n",
      "\n",
      "\n",
      "실제 요약 : sports minister meets india youngest <UNK> champion\n",
      "예측 요약 :  mn gold be be be be ever ever death apple\n",
      "\n",
      "\n",
      "실제 요약 : <UNK> players <UNK> <UNK> <UNK> <UNK> srk movie <UNK>\n",
      "예측 요약 :  apple gold be new into people ever player ever you\n",
      "\n",
      "\n",
      "실제 요약 : <UNK> mistakenly hits partner with <UNK> during <UNK>\n",
      "예측 요약 :  people be be be be be ever ever death two\n",
      "\n",
      "\n",
      "실제 요약 : bill <UNK> <UNK> to award <UNK> <UNK> <UNK> <UNK> in\n",
      "예측 요약 :  gold gold be be be most most new fire most\n",
      "\n",
      "\n",
      "실제 요약 : twitter tests <UNK> app on android to <UNK> data <UNK>\n",
      "예측 요약 :  bjp be gold be be be most death day day\n",
      "\n",
      "\n",
      "실제 요약 : goa assembly passes bill for <UNK> <UNK> as tree\n",
      "예측 요약 :  gold facebook be kerala rahul men player shares worth <UNK>\n",
      "\n",
      "\n",
      "실제 요약 : indian <UNK> accuses <UNK> <UNK> of sexual <UNK>\n",
      "예측 요약 :  months be be be be million we gets ever ever\n",
      "\n",
      "\n",
      "실제 요약 : get <UNK> to <UNK> <UNK> <UNK> to daughter alia\n",
      "예측 요약 :  months months be be kerala we million we gets kids\n",
      "\n",
      "\n",
      "실제 요약 : it is film not <UNK> <UNK> on row over the\n",
      "예측 요약 :  months months be be kerala <PAD> police police post worth\n",
      "\n",
      "\n",
      "실제 요약 : bn year old <UNK> <UNK> to <UNK> at auction\n",
      "예측 요약 :  korea gold gold be new into new people in can\n",
      "\n",
      "\n",
      "실제 요약 : <UNK> <UNK> gold worth lakh in water <UNK> arrested\n",
      "예측 요약 :  people gold be be ever ever death day player school\n",
      "\n",
      "\n",
      "실제 요약 : navy <UNK> <UNK> yoga on ship <UNK> and in <UNK>\n",
      "예측 요약 :  korea before gold new new people what what before before\n",
      "\n",
      "\n",
      "실제 요약 : world oldest <UNK> <UNK> <UNK> found in <UNK>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_286/194023196.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_seq = torch.tensor(encoder_input_test[i], dtype=torch.long, device=device).unsqueeze(0)\n",
      "/tmp/ipykernel_286/2166841647.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_seq = torch.tensor(input_seq, dtype=torch.long, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 요약 :  gold gold be be be most life day train gets\n",
      "\n",
      "\n",
      "실제 요약 : cr <UNK> from army by doctor for <UNK> his jawan\n",
      "예측 요약 :  people be be be be be be ever ever death\n",
      "\n",
      "\n",
      "실제 요약 : student <UNK> in temple <UNK> while friends <UNK> selfie\n",
      "예측 요약 :  people gold be be be ever ever day leader day\n",
      "\n",
      "\n",
      "실제 요약 : sharif <UNK> <UNK> <UNK> party\n",
      "예측 요약 :  wins months months be be wins months police post worth\n",
      "\n",
      "\n",
      "실제 요약 : not <UNK> if <UNK> get over by <UNK> woman <UNK>\n",
      "예측 요약 :  months be be be be most million <PAD> you gets\n",
      "\n",
      "\n",
      "실제 요약 : <UNK> <UNK> can <UNK> with help of <UNK> nawazuddin\n",
      "예측 요약 :  months be be be most million <PAD> you gets gets\n",
      "\n",
      "\n",
      "실제 요약 : company makes iphone case which also <UNK> as <UNK> <UNK>\n",
      "예측 요약 :  bjp facebook gold be kerala what million before before new\n",
      "\n",
      "\n",
      "실제 요약 : mamata <UNK> has become <UNK> bjp leader in wb\n",
      "예측 요약 :  months months months months mp mp korea korea police police\n",
      "\n",
      "\n",
      "실제 요약 : <UNK> <UNK> <UNK> in tripura meghalaya <UNK> polls\n",
      "예측 요약 :  korea win korea apple back due ever ever gets leader\n",
      "\n",
      "\n",
      "실제 요약 : <UNK> <UNK> <UNK> my role <UNK> sachin son arjun\n",
      "예측 요약 :  months months be be most pakistan under million you gets\n",
      "\n",
      "\n",
      "실제 요약 : first <UNK> evidence of <UNK> in <UNK> age <UNK>\n",
      "예측 요약 :  years st be be be be ever ever death two\n",
      "\n",
      "\n",
      "실제 요약 : nadal completes career matches with win\n",
      "예측 요약 :  people be be be most most new people most new\n",
      "\n",
      "\n",
      "실제 요약 : kohli is the <UNK> batsman to have played odi cricket\n",
      "예측 요약 :  months months months months wins police mp police run man\n",
      "\n",
      "\n",
      "실제 요약 : andhra <UNK> tries to <UNK> senior with <UNK> <UNK> blood\n",
      "예측 요약 :  years st be be be most ever day ever first\n",
      "\n",
      "\n",
      "실제 요약 : <UNK> track st <UNK> <UNK> from <UNK> solar system\n",
      "예측 요약 :  pakistan gold gold gold most kerala mp what most kerala\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq:\n",
    "        key = int(i.item())  # PyTorch Tensor → int 변환\n",
    "        if key != 0:  # 패딩(0) 제외\n",
    "            temp = temp + src_index_to_word.get(key, \"<UNK>\") + ' '  # 안전한 조회\n",
    "    return temp.strip()\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp = ''\n",
    "    for i in input_seq:\n",
    "        key = int(i.item())  # PyTorch Tensor → int 변환\n",
    "        if key != 0 and key != tar_word_to_index['sostoken'] and key != tar_word_to_index['eostoken']:\n",
    "            temp = temp + tar_index_to_word.get(key, \"<UNK>\") + ' '  # 안전한 조회\n",
    "    return temp.strip()  # 양쪽 공백 제거\n",
    "\n",
    "\n",
    "for i in range(50, 100):\n",
    "    #print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    input_seq = torch.tensor(encoder_input_test[i], dtype=torch.long, device=device).unsqueeze(0)\n",
    "    print(\"예측 요약 :\", decode_sequence(input_seq, encoder, decoder, tar_word_to_index, tar_index_to_word, text_max_len, summary_max_len, device))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc1d722-8907-4876-bd91-1a516e9e227c",
   "metadata": {},
   "source": [
    "모델을 수행해보니 같은 단어가 반복적으로 나옴을 확인할 수 있었다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
