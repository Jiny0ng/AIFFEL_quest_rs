{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3b9d7e98-7d2c-46a5-95c3-2baefcb27fe4",
      "metadata": {
        "id": "3b9d7e98-7d2c-46a5-95c3-2baefcb27fe4"
      },
      "source": [
        "## Step 1 — 환경 준비 (라이브러리 설치/임포트)\n",
        "**프로젝트 단계:** 환경 세팅  \n",
        "**목적:** 재현 가능한 실행 환경을 만들고, 필요한 패키지를 설치/임포트한다.  \n",
        "**핵심:** `transformers`, `datasets`, `evaluate`, `torch`, (Colab이면) `tf-keras` 설치."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b02fada-855c-4aff-a791-4145741fba32",
      "metadata": {
        "scrolled": true,
        "collapsed": true,
        "id": "2b02fada-855c-4aff-a791-4145741fba32"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow\n",
        "# !pip install transformers\n",
        "# !pip install datasets\n",
        "# !pip install evaluate\n",
        "# !pip install tf-keras\n",
        "\n",
        "# !pip install transformers[torch]\n",
        "# !pip install torch==2.7.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 — 데이터셋 로드 & 버전 확인\n",
        "**프로젝트 단계:** 데이터 준비  \n",
        "**목적:** NSMC(`Blpeng/nsmc`)를 로드하고, 주요 패키지 버전을 출력해 재현성을 확보한다.  \n",
        "**핵심:** `load_dataset(\"Blpeng/nsmc\")`, `numpy/transformers/datasets/torch` 버전 로그."
      ],
      "metadata": {
        "id": "ZW53tZ60JLjv"
      },
      "id": "ZW53tZ60JLjv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eaec643-8800-4932-b98f-efe268c95957",
      "metadata": {
        "id": "9eaec643-8800-4932-b98f-efe268c95957"
      },
      "outputs": [],
      "source": [
        "#import tensorflow\n",
        "import numpy\n",
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#허깅페이스 Blpeng/NSMC데이터셋을 로드\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"Blpeng/nsmc\")\n",
        "\n",
        "#print(tensorflow.__version__)\n",
        "print(numpy.__version__)\n",
        "print(transformers.__version__)\n",
        "print(datasets.__version__)\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 — 디바이스 확인(GPU/CPU)\n",
        "**프로젝트 단계:** 환경 점검  \n",
        "**목적:** 현재 런타임이 CUDA를 사용할 수 있는지 확인하고, `device`를 설정한다.  \n",
        "**핵심:** `torch.cuda.is_available()`, `device = cuda or cpu`."
      ],
      "metadata": {
        "id": "oTigsz5ZJPe4"
      },
      "id": "oTigsz5ZJPe4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56ea1860-5813-4e14-a600-83a07ec13fd1",
      "metadata": {
        "id": "56ea1860-5813-4e14-a600-83a07ec13fd1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "#device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 — 모델/토크나이저 초기화\n",
        "**프로젝트 단계:** 모델 준비  \n",
        "**목적:** `klue/bert-base` 토크나이저/모델을 이진분류 헤드(`num_labels=2`)로 로드한다.  \n",
        "**핵심:** `AutoTokenizer.from_pretrained`, `AutoModelForSequenceClassification(..., num_labels=2)`."
      ],
      "metadata": {
        "id": "bCkDJ4y8JS_L"
      },
      "id": "bCkDJ4y8JS_L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e661a90-d0b8-4dcb-ac0b-89cb919b9846",
      "metadata": {
        "id": "1e661a90-d0b8-4dcb-ac0b-89cb919b9846"
      },
      "outputs": [],
      "source": [
        "#모델을 직접 로드\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
        "#model = AutoModelForMaskedLM.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "#이진분류를 위한 모델 클래스 변경\n",
        "model_name = \"klue/bert-base\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 — 데이터 스키마/결측 확인\n",
        "**프로젝트 단계:** 데이터 탐색(EDA)  \n",
        "**목적:** 스플릿별 컬럼/결측 여부를 점검하고, 전처리 필요성을 확인한다.  \n",
        "**핵심:** `Dataset.to_pandas().info()`로 결측 존재 확인."
      ],
      "metadata": {
        "id": "QA-hEHRMJbvt"
      },
      "id": "QA-hEHRMJbvt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75bd4f82-d71c-47ec-9ef3-b766755d7265",
      "metadata": {
        "id": "75bd4f82-d71c-47ec-9ef3-b766755d7265"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def inspect_columns_with_info(dataset_dict):\n",
        "    \"\"\"\n",
        "    Hugging Face DatasetDict의 각 스플릿을 Pandas DataFrame으로 변환하고,\n",
        "    .info() 메소드를 사용해 결측값 존재를 추정합니다.\n",
        "    \"\"\"\n",
        "    print(\"Pandas .info()를 사용한 컬럼 정보 확인을 시작합니다.\\n\")\n",
        "\n",
        "    for split_name, dataset in dataset_dict.items():\n",
        "        print(f\"--- {split_name.upper()} 데이터셋 ---\")\n",
        "\n",
        "        # 데이터셋을 Pandas DataFrame으로 변환\n",
        "        df = dataset.to_pandas()\n",
        "\n",
        "        df.info()\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "def filter_nulls(example):\n",
        "    return example['document'] is not None and len(example['document']) > 0\n",
        "\n",
        "inspect_columns_with_info(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f93418ee-0c37-454a-8256-e3aa539db727",
      "metadata": {
        "id": "f93418ee-0c37-454a-8256-e3aa539db727"
      },
      "source": [
        "결측치가 존재함을 확인\n",
        "따라서 전처리를 진행하며 결측치를 제거\n",
        "\n",
        "## Step 6 — 전처리 설계 & 함수 구현\n",
        "**프로젝트 단계:** 데이터 전처리  \n",
        "**목적:** 텍스트를 정규화하여 학습 안정성을 높인다.  \n",
        "**핵심:**  \n",
        "- 결측치 처리(빈 문자열 대체)  \n",
        "- 허용 문자 필터(한글/영문/숫자/기본 구두점)  \n",
        "- 반복 감정표현(`ㅋ/ㅎ/ㅠ/ㅜ`) **길이 2로 제한**  \n",
        "- 공백 정규화  \n",
        "- 토크나이즈(`truncation=True`, `padding=False`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34d9a359-4802-488c-8199-53ad7e007ceb",
      "metadata": {
        "id": "34d9a359-4802-488c-8199-53ad7e007ceb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def preprocess(dataset) :\n",
        "    cleaned_texts = []\n",
        "    for text in dataset['document'] :\n",
        "\n",
        "        #결측치 제거\n",
        "        if text is None :\n",
        "            text = \"\"\n",
        "        #한글, 특수문자, 중복띄어쓰기, 제거\n",
        "        text = re.sub(r'[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9\\s.?!,()]', '', text)\n",
        "        #리뷰 특성상 감정표현을 위해 ㅋ,ㅎ,ㅠ,,ㅜ등이 반복되는 사례가 많음 따라서 2번으로 제한\n",
        "        text = re.sub(r'([ㅋㅎㅜㅠ]){4,}', r'\\1\\1', text)\n",
        "        #공백 정규화\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        cleaned_texts.append(text)\n",
        "\n",
        "    return tokenizer(\n",
        "        cleaned_texts,\n",
        "        truncation=True,\n",
        "        padding=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab0a6e4b-81df-4386-80f5-44b6e2b76d59",
      "metadata": {
        "id": "ab0a6e4b-81df-4386-80f5-44b6e2b76d59"
      },
      "outputs": [],
      "source": [
        "#데이터 전처리\n",
        "ds = ds.filter(filter_nulls)\n",
        "dataset = ds.map(preprocess, batched=True)\n",
        "inspect_columns_with_info(dataset)\n",
        "\n",
        "# train & test\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "#test에서 validation 분할\n",
        "train_val_split = train_dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "#분할된 셋을 각 변수에 할당\n",
        "train_dataset = train_val_split['train']\n",
        "val_dataset = train_val_split['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "332ce8a7-d004-4c87-96a0-4f9b862b612a",
      "metadata": {
        "id": "332ce8a7-d004-4c87-96a0-4f9b862b612a"
      },
      "outputs": [],
      "source": [
        "#data collator\n",
        "\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "# 이전에 생성한 tokenizer를 사용\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "위 문제점을 기반으로 가설 생성\n",
        "## Step 7 — 문제점 요약 & 가설 명시\n",
        "**프로젝트 단계:** 실험 설계  \n",
        "**이슈:**\n",
        "팀원들의 실험결과들을 통해 문제점 예측\n",
        "1. 기존 학습규제들은 빠른 과적합을 불러일으킴\n",
        "2. LoRA를 이용해 과적합을 방지하고 학습속도를 올리려 하였지만, 파라미터 수가 너무 적어 과소적합이 일어남\n",
        "\n",
        "**핵심:** “과적합을 늦추고 일반화 성능을 극대화한다”를 상위 목표로 잡음.\n"
      ],
      "metadata": {
        "id": "Gw6jZX-mCF5j"
      },
      "id": "Gw6jZX-mCF5j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10 — 학습 설정(TrainingArguments)\n",
        "**프로젝트 단계:** 학습 전략 설정  \n",
        "**목적:** 로깅/평가/저장/스케줄/배치/정규화 등 학습 전략을 정의한다.  \n",
        "**핵심:**  \n",
        "- 로깅/평가 간격: **step 단위 2,000** (`logging_steps=2000`, `eval_steps=2000`)  \n",
        "- 체크포인트: `save_total_limit`, (TensorBoard) `report_to=\"tensorboard\"`  \n",
        "- 워밍업/스케줄: `warmup_steps=500`, `warmup_ratio=0.06`  \n",
        "- 최적화: `learning_rate=2e-5`, `weight_decay=0.1`, `num_train_epochs=6`  \n",
        "- 배치: `per_device_train_batch_size=16`, `gradient_accumulation_steps=2`(효과적 배치 32)  \n",
        "- 혼합정밀: `fp16=True`, 재현성: `seed=42`  \n",
        "\n",
        "\n",
        "팀원들의 arg를 바탕으로 종합적인 arg를 셋팅\n"
      ],
      "metadata": {
        "id": "if6QtbZ1KIfh"
      },
      "id": "if6QtbZ1KIfh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c88ecb3-83d3-473d-b118-b25b05d328de",
      "metadata": {
        "id": "2c88ecb3-83d3-473d-b118-b25b05d328de"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "output_dir = '/content/drive/MyDrive/Colab_Results/nsmc_model'\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir,                        # output이 저장될 경로\n",
        "\n",
        "    #시각화를 위한 셋팅\n",
        "    logging_strategy=\"steps\",            # step 단위로 로그 저장\n",
        "    logging_steps=2000,                  # 2000 step마다 로그 저장\n",
        "    eval_strategy=\"steps\",               # step 단위로 검증 진행\n",
        "    eval_steps=2000,                     # 2000 step마다 검증 진행\n",
        "    save_strategy=\"steps\",               # step 단위로 모델 저장\n",
        "    save_steps=2000,                     # 2000 step마다 모델 저장\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    #추가 하이퍼파라미터\n",
        "    lr_scheduler_type='linear',          # 선형 학습률 스케줄러 사용\n",
        "    warmup_steps=500,                    # 훈련 초기 500 step 동안 학습률을 서서히 증가\n",
        "    warmup_ratio=0.06,\n",
        "    metric_for_best_model='f1',          # 훈련 초기 500 step 동안 학습률을 서서히 증가\n",
        "    greater_is_better=True,              # metric_for_best_model의 값이 클수록 좋다고 명시 (필수)\n",
        "    fp16=True,                           # 학습 속도 및 메모리 효율 향상\n",
        "    report_to=\"tensorboard\",             # TensorBoard로 로그 리포팅\n",
        "    seed = 42,\n",
        "\n",
        "\n",
        "    learning_rate = 2e-5,                 #learning_rate\n",
        "    per_device_train_batch_size = 16,      # 각 device 당 batch size\n",
        "    per_device_eval_batch_size = 16,       # evaluation 시에 batch size\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs = 6,                 # train 시킬 총 epochs\n",
        "    weight_decay = 0.1,                  # weight decay\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6260072c-6d04-4e94-89c0-9bb2dd57e570",
      "metadata": {
        "id": "6260072c-6d04-4e94-89c0-9bb2dd57e570"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import evaluate\n",
        "\n",
        "#이진 분류를 위해 지표들을 combine\n",
        "metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions,labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references = labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12 — Trainer 초기화 & 학습 실행\n",
        "**프로젝트 단계:** 학습  \n",
        "**목적:** 설정한 전략과 지표로 훈련을 수행하고, 과적합을 방지한다.  \n",
        "**핵심:**  \n",
        "- `Trainer(model, args, train_dataset, eval_dataset, data_collator, compute_metrics)`  \n",
        "- **EarlyStopping**: `patience=3`, `threshold=0.001`  \n",
        "- 산출: 체크포인트(`checkpoint-XXXX`), TensorBoard 로그(`runs/`)"
      ],
      "metadata": {
        "id": "lwikE_VnKjrj"
      },
      "id": "lwikE_VnKjrj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "185545c0-dadb-4449-8f6c-1b2aa304ef86",
      "metadata": {
        "scrolled": true,
        "id": "185545c0-dadb-4449-8f6c-1b2aa304ef86"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,           # 학습시킬 model\n",
        "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
        "    train_dataset=train_dataset,    # training dataset\n",
        "    eval_dataset=val_dataset,       # evaluation dataset\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)],\n",
        ")\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4jN-tdlcWoDS"
      },
      "id": "4jN-tdlcWoDS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6534d643-d019-46ad-a353-3731e96b9c70",
      "metadata": {
        "id": "6534d643-d019-46ad-a353-3731e96b9c70"
      },
      "outputs": [],
      "source": [
        "import os, glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "\n",
        "def tb_series(runs_dir):\n",
        "    series = {}  # tag -> dict(step->val)\n",
        "    event_files = glob.glob(os.path.join(runs_dir, \"**\", \"events.out.tfevents.*\"), recursive=True)\n",
        "    if not event_files:\n",
        "        raise FileNotFoundError(\"runs/ 아래 TensorBoard 이벤트 파일을 찾지 못했습니다.\")\n",
        "\n",
        "    for f in event_files:\n",
        "        ea = EventAccumulator(f); ea.Reload()\n",
        "        for tag in ea.Tags().get(\"scalars\", []):\n",
        "            if tag not in series:\n",
        "                series[tag] = {}\n",
        "            for e in ea.Scalars(tag):\n",
        "                series[tag][e.step] = float(e.value)  # 중복 step은 마지막 값으로 덮어씀\n",
        "    # 정렬된 리스트로 변환\n",
        "    return {tag: sorted(d.items()) for tag, d in series.items()}\n",
        "\n",
        "def first_available(series, candidates):\n",
        "    for t in candidates:\n",
        "        if t in series and len(series[t])>0:\n",
        "            return t\n",
        "    return None\n",
        "\n",
        "def plot_train_val_loss_from_tb(runs_dir, smooth_k=1, save_path=None):\n",
        "    s = tb_series(runs_dir)\n",
        "    # 태그 후보 (환경/버전에 따라 다를 수 있음)\n",
        "    train_tag = first_available(s, [\"train/loss\",\"loss\",\"training_loss\"])\n",
        "    val_tag   = first_available(s, [\"eval/loss\",\"eval_loss\"])\n",
        "\n",
        "    if not train_tag and not val_tag:\n",
        "        print(\"train/eval loss 태그를 찾지 못했습니다.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(9,5))\n",
        "\n",
        "    if train_tag:\n",
        "        t_steps, t_vals = zip(*s[train_tag])\n",
        "        if smooth_k>1:\n",
        "            # 이동평균(길이 맞춤용 간단 패딩)\n",
        "            t_vals = np.array(t_vals)\n",
        "            k = smooth_k\n",
        "            if len(t_vals) >= k:\n",
        "                t_vals_s = np.convolve(t_vals, np.ones(k)/k, mode=\"valid\")\n",
        "                t_vals_s = np.r_[np.full(k-1, t_vals_s[0]), t_vals_s]\n",
        "            else:\n",
        "                t_vals_s = t_vals\n",
        "            plt.plot(t_steps, t_vals_s, label=f\"Train Loss (k={smooth_k})\")\n",
        "        else:\n",
        "            plt.plot(t_steps, t_vals, label=\"Train Loss\")\n",
        "\n",
        "    if val_tag:\n",
        "        v_steps, v_vals = zip(*s[val_tag])\n",
        "        plt.plot(v_steps, v_vals, label=\"Val Loss\", marker=\"o\", linestyle=\"-\")\n",
        "\n",
        "    # (선택) eval/loss 최고가 아닌 '최저'가 베스트이므로, 다른 지표로 best_step 표시하고 싶으면 여기에 추가\n",
        "    plt.title(\"Train & Validation Loss vs Step (TensorBoard)\")\n",
        "    plt.xlabel(\"Step\"); plt.ylabel(\"Loss\")\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "# 사용 예시:\n",
        "runs_dir = \"/content/drive/MyDrive/Colab_Results/nsmc_model/runs\"\n",
        "plot_train_val_loss_from_tb(runs_dir, smooth_k=5, save_path=\"train_val_loss_tb.png\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 패키지\n",
        "# !pip install tensorboard  # Colab엔 보통 기본 설치\n",
        "\n",
        "import os, glob, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "\n",
        "# 여러분의 경로로 설정\n",
        "runs_dir = \"/content/drive/MyDrive/Colab_Results/nsmc_model/runs\"\n",
        "\n",
        "def load_tb_scalars(runs_dir):\n",
        "    \"\"\"runs/ 아래 모든 tfevents에서 scalar 태그를 수집: tag -> [(step, value), ...]\"\"\"\n",
        "    series = {}\n",
        "    event_files = glob.glob(os.path.join(runs_dir, \"**\", \"events.out.tfevents.*\"), recursive=True)\n",
        "    if not event_files:\n",
        "        raise FileNotFoundError(\"TensorBoard 이벤트 파일을 찾지 못했습니다. report_to='tensorboard'로 학습되었는지 확인하세요.\")\n",
        "    for f in event_files:\n",
        "        ea = EventAccumulator(f); ea.Reload()\n",
        "        for tag in ea.Tags().get(\"scalars\", []):\n",
        "            if tag not in series:\n",
        "                series[tag] = {}\n",
        "            for e in ea.Scalars(tag):\n",
        "                # 같은 step이 여러 파일에 있을 수 있어 마지막 값을 채택\n",
        "                series[tag][e.step] = float(e.value)\n",
        "    # step 오름차순으로 정렬된 리스트로 변환\n",
        "    return {tag: sorted(step2val.items()) for tag, step2val in series.items()}\n",
        "\n",
        "def first_available(series, candidates):\n",
        "    \"\"\"후보 태그들 중 실제로 존재하는 첫 태그 반환\"\"\"\n",
        "    for t in candidates:\n",
        "        if t in series and len(series[t]) > 0:\n",
        "            return t\n",
        "    return None\n",
        "\n",
        "# 1) 로그 적재\n",
        "series = load_tb_scalars(runs_dir)\n",
        "\n",
        "# 2) 검증 지표 태그(환경/버전에 따라 이름이 달라질 수 있어 후보군 준비)\n",
        "candidates = {\n",
        "    \"Accuracy\":  [\"eval/accuracy\", \"eval_accuracy\", \"accuracy\"],\n",
        "    \"F1\":        [\"eval/f1\", \"eval_f1\", \"f1\"],\n",
        "    \"Precision\": [\"eval/precision\", \"eval_precision\"],\n",
        "    \"Recall\":    [\"eval/recall\", \"eval_recall\"],\n",
        "}\n",
        "\n",
        "# 3) 플롯에 쓸 (지표이름 -> (steps, values)) 구성\n",
        "plot_data = {}\n",
        "for name, cand in candidates.items():\n",
        "    tag = first_available(series, cand)\n",
        "    if tag:\n",
        "        xs, ys = zip(*series[tag])\n",
        "        plot_data[name] = (xs, ys)\n",
        "\n",
        "if not plot_data:\n",
        "    raise RuntimeError(\"검증 지표 태그를 찾지 못했습니다. (예: eval/accuracy, eval/f1 등)\")\n",
        "\n",
        "# 4) 베스트 스텝(있으면) 계산: F1 우선, 없으면 Accuracy 기준\n",
        "best_step = None\n",
        "if \"F1\" in plot_data:\n",
        "    xs, ys = plot_data[\"F1\"]\n",
        "    best_step = xs[int(np.argmax(ys))]\n",
        "elif \"Accuracy\" in plot_data:\n",
        "    xs, ys = plot_data[\"Accuracy\"]\n",
        "    best_step = xs[int(np.argmax(ys))]\n",
        "\n",
        "# 5) 한 그래프에 검증 지표 전부 그리기\n",
        "plt.figure(figsize=(9,5))\n",
        "for name, (xs, ys) in plot_data.items():\n",
        "    # 검증 포인트는 간격이 넓은 경우가 많아 마커를 주면 식별이 쉬움\n",
        "    plt.plot(xs, ys, marker=\"o\", linestyle=\"-\", label=name)\n",
        "\n",
        "if best_step is not None:\n",
        "    plt.axvline(best_step, linestyle=\"--\", label=f\"best_step={best_step}\")\n",
        "\n",
        "plt.title(\"Validation Metrics vs Step\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Score\")  # (Accuracy/F1/Precision/Recall은 0~1 범위)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "300nC_CbBJEW"
      },
      "id": "300nC_CbBJEW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "메트릭 변화 추이를 확인한 결과\n",
        "6000step 근방에서 precision이 높고 recall이 낮아\n",
        "일반화의 최고점이라고 볼 수 있다.\n",
        "\n",
        "팀원들의 결과에서는 대부분의 메트릭이 90을 넘지 못하거나\n",
        "넘기더라도 훨씬 많은 스텝을 수행해야 했지만 지금의 설정은 기존보다 적은 학습으로도 높은 퍼포먼스를 보여줄 수 있었다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dr61SoASAklq"
      },
      "id": "dr61SoASAklq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "회고\n",
        "허깅페이스의 모델과 데이터셋을 이용하면 기존에 진행했던 프로젝트 방식들보다 훨씬 편하게 프로젝트를 진행할 수 있는걸 알았다. 기존 프로젝트에서 못했던 레포트화를 하기 위해 노력해봤으며 시각화를 열심히 해봤지만 생각보다 아쉬운 결과를 본것같다."
      ],
      "metadata": {
        "id": "J_MUrko_K1Q4"
      },
      "id": "J_MUrko_K1Q4"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HdwC0egwLRmA"
      },
      "id": "HdwC0egwLRmA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}