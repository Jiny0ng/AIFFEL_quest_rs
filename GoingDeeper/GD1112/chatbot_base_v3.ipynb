{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3d6640-495f-4f7e-b76a-52b9dd0e799c",
   "metadata": {},
   "source": [
    "베이스 코드에 어그멘테이션 진행\n",
    "Lexical Augmentation을 적용하여\n",
    "데이터를 3배정도 증강\n",
    "학습데이터 8000 -> 17000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94c226db-7a3e-4b01-82ab-a71d0fad9c36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Package fonts-nanum is not available, but is referred to by another package.\n",
      "This may mean that the package is missing, has been obsoleted, or\n",
      "is only available from another source\n",
      "\n",
      "E: Package 'fonts-nanum' has no installation candidate\n",
      "Font directories:\n",
      "\t/root/.local/share/fonts\n",
      "\t/usr/local/share/fonts\n",
      "\t/usr/share/fonts\n",
      "\t/root/.fonts\n",
      "\t/usr/share/texmf/fonts/opentype/public/lm\n",
      "\t/usr/share/texmf/fonts/opentype/public/lm-math\n",
      "\t/usr/share/fonts/X11\n",
      "\t/usr/share/fonts/cMap\n",
      "\t/usr/share/fonts/cmap\n",
      "\t/usr/share/fonts/opentype\n",
      "\t/usr/share/fonts/truetype\n",
      "\t/usr/share/fonts/type1\n",
      "\t/usr/share/fonts/X11/Type1\n",
      "\t/usr/share/fonts/X11/encodings\n",
      "\t/usr/share/fonts/X11/util\n",
      "\t/usr/share/fonts/cmap/adobe-cns1\n",
      "\t/usr/share/fonts/cmap/adobe-gb1\n",
      "\t/usr/share/fonts/cmap/adobe-japan1\n",
      "\t/usr/share/fonts/cmap/adobe-japan2\n",
      "\t/usr/share/fonts/cmap/adobe-korea1\n",
      "\t/usr/share/fonts/opentype/urw-base35\n",
      "\t/usr/share/fonts/truetype/liberation\n",
      "\t/usr/share/fonts/type1/texlive-fonts-recommended\n",
      "\t/usr/share/fonts/type1/urw-base35\n",
      "\t/usr/share/fonts/X11/encodings/large\n",
      "/root/.local/share/fonts: skipping, no such directory\n",
      "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts: caching, new cache contents: 0 fonts, 6 dirs\n",
      "/usr/share/fonts/X11: caching, new cache contents: 0 fonts, 3 dirs\n",
      "/usr/share/fonts/X11/Type1: caching, new cache contents: 35 fonts, 0 dirs\n",
      "/usr/share/fonts/X11/encodings: caching, new cache contents: 0 fonts, 1 dirs\n",
      "/usr/share/fonts/X11/encodings/large: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/X11/util: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/cMap: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/cmap: caching, new cache contents: 0 fonts, 5 dirs\n",
      "/usr/share/fonts/cmap/adobe-cns1: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/cmap/adobe-gb1: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/cmap/adobe-japan1: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/cmap/adobe-japan2: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/cmap/adobe-korea1: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/usr/share/fonts/opentype: caching, new cache contents: 0 fonts, 1 dirs\n",
      "/usr/share/fonts/opentype/urw-base35: caching, new cache contents: 35 fonts, 0 dirs\n",
      "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 1 dirs\n",
      "/usr/share/fonts/truetype/liberation: caching, new cache contents: 12 fonts, 0 dirs\n",
      "/usr/share/fonts/type1: caching, new cache contents: 0 fonts, 2 dirs\n",
      "/usr/share/fonts/type1/texlive-fonts-recommended: caching, new cache contents: 12 fonts, 0 dirs\n",
      "/usr/share/fonts/type1/urw-base35: caching, new cache contents: 35 fonts, 0 dirs\n",
      "/root/.fonts: skipping, no such directory\n",
      "/usr/share/texmf/fonts/opentype/public/lm: caching, new cache contents: 72 fonts, 0 dirs\n",
      "/usr/share/texmf/fonts/opentype/public/lm-math: caching, new cache contents: 1 fonts, 0 dirs\n",
      "/usr/share/fonts/X11: skipping, looped directory detected\n",
      "/usr/share/fonts/cMap: skipping, looped directory detected\n",
      "/usr/share/fonts/cmap: skipping, looped directory detected\n",
      "/usr/share/fonts/opentype: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype: skipping, looped directory detected\n",
      "/usr/share/fonts/type1: skipping, looped directory detected\n",
      "/usr/share/fonts/X11/Type1: skipping, looped directory detected\n",
      "/usr/share/fonts/X11/encodings: skipping, looped directory detected\n",
      "/usr/share/fonts/X11/util: skipping, looped directory detected\n",
      "/usr/share/fonts/cmap/adobe-cns1: skipping, looped directory detected\n",
      "/usr/share/fonts/cmap/adobe-gb1: skipping, looped directory detected\n",
      "/usr/share/fonts/cmap/adobe-japan1: skipping, looped directory detected\n",
      "/usr/share/fonts/cmap/adobe-japan2: skipping, looped directory detected\n",
      "/usr/share/fonts/cmap/adobe-korea1: skipping, looped directory detected\n",
      "/usr/share/fonts/opentype/urw-base35: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
      "/usr/share/fonts/type1/texlive-fonts-recommended: skipping, looped directory detected\n",
      "/usr/share/fonts/type1/urw-base35: skipping, looped directory detected\n",
      "/usr/share/fonts/X11/encodings/large: skipping, looped directory detected\n",
      "/var/cache/fontconfig: cleaning cache directory\n",
      "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
      "/root/.fontconfig: not cleaning non-existent cache directory\n",
      "fc-cache: succeeded\n"
     ]
    }
   ],
   "source": [
    "# 1. 나눔폰트 설치\n",
    "!sudo apt-get install -y fonts-nanum\n",
    "\n",
    "# 2. 시스템 폰트 캐시 업데이트\n",
    "!sudo fc-cache -fv\n",
    "\n",
    "# 3. matplotlib 캐시 삭제\n",
    "!rm ~/.cache/matplotlib -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa1da9000d0b3589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: mecab in /opt/conda/lib/python3.12/site-packages (0.996.11)\n",
      "Requirement already satisfied: konlpy in /opt/conda/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: mecab-ko-dic in /opt/conda/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.12/site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/conda/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.12/site-packages (from gensim) (7.3.1)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m22.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m802.0/802.0 kB\u001B[0m \u001B[31m46.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: regex, nltk\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2/2\u001B[0m [nltk][32m1/2\u001B[0m [nltk]\n",
      "\u001B[1A\u001B[2KSuccessfully installed nltk-3.9.1 regex-2025.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "!pip install mecab\n",
    "!pip install konlpy mecab-ko-dic\n",
    "!pip install gensim\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24656f7e59d83be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#라이브러리 임포팅\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "#import mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ec0aaa-8214-4fc3-b1a3-4ee9b49aec72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터 준비\n",
    "#!wget https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
    "train_data = pd.read_csv('ChatbotData.csv') \n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfcf4207-5880-437a-9de1-66378e0fb683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 전처리\n",
    "#한글, 숫자, 특수문자를 제외한 모든 문자 제거 및 소문자화\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 대문자를 소문자로 변환\n",
    "    sentence = re.sub(r\"([?.!,¿\\\\\\\"'():;-])\", r\" \\1 \", sentence)\n",
    "    #  2 특수문자 양쪽에 공백 추가\n",
    "    sentence = re.sub(r' {2,}', ' ', sentence) # 둘 이상의 공백을 하나의 공백으로 치환\n",
    "    sentence = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣0-9?.!,]+\", \" \", sentence) #한글 숫자 특수문자만 남김\n",
    "    sentence = sentence.strip() # 문자열 양 끝 공백 제거\n",
    "    return sentence\n",
    "\n",
    "    \n",
    "questions = []\n",
    "for sentence in train_data['Q']:\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    questions.append(sentence)\n",
    "\n",
    "answers = []\n",
    "for sentence in train_data['A']:\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    answers.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b411687a-498e-48d7-b40c-f3a42591d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(questions))\n",
    "    f.write('\\n'.join(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51871101-7723-419b-873e-33dff4048763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=all.txt --model_prefix=chatbot --vocab_size=8007 --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=[PAD] --unk_id=1 --unk_piece=[UNK] --bos_id=2 --bos_piece=[BOS] --eos_id=3 --eos_piece=[EOS] --user_defined_symbols=[SEP],[CLS],[MASK]\n",
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: all.txt\n",
      "  input_format: \n",
      "  model_prefix: chatbot\n",
      "  model_type: BPE\n",
      "  vocab_size: 8007\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 999999\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: [SEP]\n",
      "  user_defined_symbols: [CLS]\n",
      "  user_defined_symbols: [MASK]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(186) LOG(INFO) Loading corpus: all.txt\n",
      "trainer_interface.cc(411) LOG(INFO) Loaded all 23645 sentences\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(427) LOG(INFO) Adding meta_piece: [MASK]\n",
      "trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(541) LOG(INFO) all chars count=369480\n",
      "trainer_interface.cc(552) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(562) LOG(INFO) Alphabet size=1059\n",
      "trainer_interface.cc(563) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(594) LOG(INFO) Done! preprocessed 23645 sentences.\n",
      "trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 23645\n",
      "trainer_interface.cc(611) LOG(INFO) Done! 20647\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13942 min_freq=42\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1055 size=20 all=16110 active=1765 piece=▁거예요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=776 size=40 all=16826 active=2481 piece=▁여\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=619 size=60 all=17489 active=3144 piece=▁좋아하는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=427 size=80 all=18095 active=3750 piece=▁모\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=358 size=100 all=18825 active=4480 piece=▁전\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=357 min_freq=37\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=307 size=120 all=19259 active=1400 piece=▁뭐\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=262 size=140 all=19852 active=1993 piece=▁소\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=227 size=160 all=20276 active=2417 piece=▁비\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=206 size=180 all=20632 active=2773 piece=▁행\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=188 size=200 all=21004 active=3145 piece=▁관\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=188 min_freq=31\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=172 size=220 all=21358 active=1397 piece=▁재\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=155 size=240 all=21686 active=1725 piece=▁데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=144 size=260 all=22007 active=2046 piece=▁자꾸\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=136 size=280 all=22389 active=2428 piece=▁고백\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=130 size=300 all=22741 active=2780 piece=▁먼저\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=130 min_freq=26\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=122 size=320 all=23074 active=1466 piece=▁헤어진지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=340 all=23404 active=1796 piece=▁버\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=360 all=23620 active=2012 piece=▁돈\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=103 size=380 all=23946 active=2338 piece=▁곳\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=99 size=400 all=24341 active=2733 piece=▁함\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=99 min_freq=22\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=92 size=420 all=24624 active=1499 piece=▁남자친구\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=87 size=440 all=24845 active=1720 piece=다가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=85 size=460 all=25101 active=1976 piece=▁사람은\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=81 size=480 all=25342 active=2217 piece=하네\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=77 size=500 all=25618 active=2493 piece=▁설\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=77 min_freq=20\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=75 size=520 all=25843 active=1494 piece=▁어떨까요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=72 size=540 all=26071 active=1722 piece=▁해도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=68 size=560 all=26346 active=1997 piece=▁그만\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=66 size=580 all=26512 active=2163 piece=▁휴\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=64 size=600 all=26698 active=2349 piece=▁항상\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=64 min_freq=18\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=62 size=620 all=26864 active=1501 piece=▁같은\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=60 size=640 all=27126 active=1763 piece=▁상황\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57 size=660 all=27373 active=2010 piece=▁개\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=56 size=680 all=27579 active=2216 piece=▁가보세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=54 size=700 all=27701 active=2338 piece=▁충분히\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=53 min_freq=16\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=52 size=720 all=27853 active=1538 piece=▁열���히\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50 size=740 all=27993 active=1678 piece=▁종\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49 size=760 all=28155 active=1840 piece=▁만남\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=48 size=780 all=28283 active=1968 piece=으니까요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47 size=800 all=28428 active=2113 piece=▁부모님\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=47 min_freq=15\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=46 size=820 all=28533 active=1517 piece=▁헤어지고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=44 size=840 all=28665 active=1649 piece=더니\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=860 all=28806 active=1790 piece=▁살아\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=42 size=880 all=28900 active=1884 piece=▁잊혀\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41 size=900 all=29019 active=2003 piece=▁재밌\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=41 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=40 size=920 all=29153 active=1570 piece=▁정리가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=940 all=29277 active=1694 piece=▁끊\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38 size=960 all=29469 active=1886 piece=▁잘하는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37 size=980 all=29621 active=2038 piece=▁정도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=36 size=1000 all=29767 active=2184 piece=▁붙잡\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=36 min_freq=13\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35 size=1020 all=29891 active=1601 piece=같아\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=1040 all=30031 active=1741 piece=▁익\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=34 size=1060 all=30166 active=1876 piece=드릴게요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=33 size=1080 all=30315 active=2025 piece=▁힘내세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32 size=1100 all=30420 active=2130 piece=▁아닌데\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=32 min_freq=12\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=31 size=1120 all=30516 active=1617 piece=▁마련\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30 size=1140 all=30614 active=1715 piece=▁거죠\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1160 all=30696 active=1797 piece=생활\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29 size=1180 all=30823 active=1924 piece=▁헷갈\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1200 all=30934 active=2035 piece=지고\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=28 min_freq=11\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28 size=1220 all=30999 active=1598 piece=▁이별후\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1240 all=31091 active=1690 piece=▁생일\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=27 size=1260 all=31123 active=1722 piece=▁가져보세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1280 all=31213 active=1812 piece=▁소식\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=26 size=1300 all=31292 active=1891 piece=▁때까지\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=26 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1320 all=31372 active=1645 piece=아서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1340 all=31500 active=1773 piece=▁알고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25 size=1360 all=31559 active=1832 piece=▁힘들죠\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1380 all=31651 active=1924 piece=▁미치\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24 size=1400 all=31725 active=1998 piece=▁알려줘\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=24 min_freq=10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1420 all=31774 active=1634 piece=▁먹을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=23 size=1440 all=31893 active=1753 piece=▁드디어\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1460 all=31953 active=1813 piece=▁경우\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1480 all=32004 active=1864 piece=▁차분\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22 size=1500 all=32068 active=1928 piece=▁거기까지\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=22 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1520 all=32208 active=1739 piece=▁10\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1540 all=32274 active=1805 piece=답니다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=1560 all=32313 active=1844 piece=▁자연스러운\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1580 all=32417 active=1948 piece=야할\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1600 all=32527 active=2058 piece=▁집중\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=20 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20 size=1620 all=32563 active=1655 piece=▁준비가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1640 all=32595 active=1687 piece=치가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1660 all=32702 active=1794 piece=▁오빠\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19 size=1680 all=32788 active=1880 piece=▁받았어\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1700 all=32804 active=1896 piece=▁권\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=9\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1720 all=32933 active=1766 piece=인거\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1740 all=33010 active=1843 piece=▁얘기\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18 size=1760 all=33069 active=1902 piece=▁안하고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1780 all=33084 active=1917 piece=구나\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1800 all=33223 active=2056 piece=▁동생\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=17 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1820 all=33276 active=1710 piece=▁점심\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17 size=1840 all=33328 active=1762 piece=▁호감이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1860 all=33374 active=1808 piece=냐고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1880 all=33480 active=1914 piece=▁다니\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1900 all=33533 active=1967 piece=▁없애\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1920 all=33611 active=1748 piece=▁가자고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16 size=1940 all=33628 active=1765 piece=▁사랑한다고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1960 all=33684 active=1821 piece=어서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=1980 all=33774 active=1911 piece=▁되면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2000 all=33851 active=1988 piece=▁있지\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=15 min_freq=8\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2020 all=33890 active=1732 piece=▁쉬세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15 size=2040 all=33897 active=1739 piece=▁연락하지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2060 all=33939 active=1781 piece=간이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2080 all=34073 active=1915 piece=▁내는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2100 all=34113 active=1955 piece=▁조급\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=14 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2120 all=34143 active=1731 piece=▁상황이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2140 all=34137 active=1725 piece=▁썸남한테\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14 size=2160 all=34123 active=1711 piece=▁잊어버리세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2180 all=34211 active=1799 piece=어진\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2200 all=34332 active=1920 piece=▁남을\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2220 all=34364 active=1747 piece=▁상태\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2240 all=34439 active=1822 piece=▁전부\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2260 all=34508 active=1891 piece=▁그동안\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2280 all=34512 active=1895 piece=▁여사친\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2300 all=34528 active=1911 piece=였나봐요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=13 min_freq=7\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=2320 all=34528 active=1722 piece=▁않으니까요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2340 all=34595 active=1789 piece=두고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2360 all=34669 active=1863 piece=▁나왔\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2380 all=34701 active=1895 piece=▁수록\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2400 all=34738 active=1932 piece=▁클럽\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2420 all=34772 active=1768 piece=▁드라마\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2440 all=34773 active=1769 piece=▁없으면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2460 all=34756 active=1752 piece=▁생겼는데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12 size=2480 all=34741 active=1737 piece=▁있을거예요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2500 all=34778 active=1774 piece=▁헬\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2520 all=34879 active=1838 piece=전환\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2540 all=34922 active=1881 piece=▁늦게\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2560 all=34952 active=1911 piece=▁상사\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2580 all=34997 active=1956 piece=▁잘생\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2600 all=35058 active=2017 piece=줄게요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2620 all=35080 active=1769 piece=▁뭐하고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2640 all=35072 active=1761 piece=▁지났네\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2660 all=35077 active=1766 piece=▁없나봐요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11 size=2680 all=35065 active=1754 piece=▁힘들었겠어요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2700 all=35121 active=1810 piece=나면\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2720 all=35206 active=1834 piece=지게\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2740 all=35247 active=1875 piece=▁떨리\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2760 all=35284 active=1912 piece=▁보험\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2780 all=35325 active=1953 piece=▁있게\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2800 all=35379 active=2007 piece=장에서\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2820 all=35410 active=1799 piece=▁들어와\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2840 all=35405 active=1794 piece=▁억지로\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2860 all=35393 active=1782 piece=▁혼자만\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2880 all=35404 active=1793 piece=▁싶습니다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10 size=2900 all=35391 active=1780 piece=▁바쁜가봐요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=10 min_freq=6\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2920 all=35425 active=1804 piece=▁찔\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2940 all=35509 active=1888 piece=린다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2960 all=35616 active=1995 piece=▁개강\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=2980 all=35645 active=2024 piece=▁맞추\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3000 all=35672 active=2051 piece=▁성장\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3020 all=35690 active=1796 piece=▁의외\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3040 all=35729 active=1835 piece=갑니다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3060 all=35791 active=1897 piece=▁고민임\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3080 all=35790 active=1896 piece=▁받으면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3100 all=35783 active=1889 piece=▁왔으면\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3120 all=35779 active=1786 piece=▁주말에\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3140 all=35767 active=1774 piece=▁다음부터\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3160 all=35763 active=1770 piece=▁좋아했던\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=3180 all=35756 active=1763 piece=▁드셔보세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3200 all=35748 active=1755 piece=▁봉\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3220 all=35791 active=1830 piece=럽게\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3240 all=35863 active=1902 piece=해진\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3260 all=35893 active=1932 piece=▁다닐\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3280 all=35926 active=1965 piece=▁반응\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3300 all=35963 active=2002 piece=▁���키\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3320 all=36000 active=1832 piece=▁이직\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3340 all=36021 active=1853 piece=▁쳐다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3360 all=36067 active=1899 piece=다는건\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3380 all=36140 active=1972 piece=▁걱정이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3400 all=36132 active=1964 piece=▁단단히\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3420 all=36124 active=1799 piece=▁불편해\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3440 all=36120 active=1795 piece=▁아침이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3460 all=36114 active=1789 piece=▁청첩장\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3480 all=36127 active=1802 piece=▁공부해서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3500 all=36109 active=1784 piece=▁아니지만\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3520 all=36111 active=1808 piece=▁회원정보\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8 size=3540 all=36091 active=1788 piece=▁사람인가봐요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3560 all=36099 active=1796 piece=▁색\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3580 all=36159 active=1856 piece=년을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3600 all=36221 active=1918 piece=순간\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3620 all=36278 active=1862 piece=콜릿\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3640 all=36294 active=1878 piece=▁길어\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3660 all=36307 active=1891 piece=▁동성\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3680 all=36331 active=1915 piece=▁사내\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3700 all=36348 active=1932 piece=▁어필\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3720 all=36361 active=1828 piece=▁종일\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3740 all=36390 active=1857 piece=▁했나\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3760 all=36445 active=1912 piece=할거라\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3780 all=36451 active=1918 piece=▁끝났네\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3800 all=36436 active=1903 piece=▁말하고\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3820 all=36435 active=1821 piece=▁상담을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3840 all=36424 active=1810 piece=▁아픔의\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3860 all=36424 active=1810 piece=▁잊혀질\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3880 all=36421 active=1807 piece=▁하네요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3900 all=36421 active=1807 piece=▁답답하네\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3920 all=36406 active=1807 piece=▁시원하게\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3940 all=36396 active=1797 piece=▁친구라고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3960 all=36385 active=1786 piece=▁떠나보세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7 size=3980 all=36369 active=1770 piece=▁축하합���다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4000 all=36375 active=1776 piece=▁득\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4020 all=36416 active=1859 piece=겨워\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4040 all=36475 active=1918 piece=매일\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4060 all=36528 active=1971 piece=절한\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4080 all=36578 active=2021 piece=화가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4100 all=36597 active=2040 piece=▁관련\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4120 all=36618 active=1848 piece=▁대우\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4140 all=36636 active=1866 piece=▁먹지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4160 all=36652 active=1882 piece=▁성덕\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4180 all=36667 active=1897 piece=▁썸일\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4200 all=36680 active=1910 piece=▁옷이\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4220 all=36693 active=1847 piece=▁자면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4240 all=36709 active=1863 piece=▁찌질\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4260 all=36739 active=1893 piece=▁평가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4280 all=36762 active=1916 piece=교통을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4300 all=36809 active=1963 piece=인건가\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4320 all=36843 active=1869 piece=▁걸까요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4340 all=36831 active=1857 piece=▁길에서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4360 all=36819 active=1845 piece=▁듣는데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4380 all=36809 active=1835 piece=▁미래에\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4400 all=36798 active=1824 piece=▁서로가\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4420 all=36785 active=1827 piece=▁알았어\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4440 all=36771 active=1813 piece=▁이해할\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4460 all=36766 active=1808 piece=▁집착해\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4480 all=36761 active=1803 piece=▁하거나\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4500 all=36766 active=1808 piece=▁고쳐쓰는\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4520 all=36753 active=1826 piece=▁말해봐요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4540 all=36741 active=1814 piece=▁스타트업\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4560 all=36732 active=1805 piece=▁잊는다는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4580 all=36720 active=1793 piece=▁행복하길\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4600 all=36709 active=1782 piece=▁사랑할수록\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4620 all=36690 active=1817 piece=▁줄여보세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6 size=4640 all=36673 active=1800 piece=▁필요한가봐요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4660 all=36690 active=1817 piece=▁씌\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4680 all=36733 active=1860 piece=난거\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4700 all=36767 active=1894 piece=문이\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4720 all=36824 active=1892 piece=오를\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4740 all=36884 active=1952 piece=증을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4760 all=36913 active=1981 piece=▁개념\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4780 all=36924 active=1992 piece=▁끄는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4800 all=36938 active=2006 piece=▁담긴\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4820 all=36929 active=1838 piece=▁명품\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4840 all=36940 active=1849 piece=▁버리\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4860 all=36947 active=1856 piece=▁서툰\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4880 all=36964 active=1873 piece=▁씁쓸\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4900 all=36980 active=1889 piece=▁은행\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=4\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4920 all=36998 active=1867 piece=▁주택\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4940 all=37017 active=1886 piece=▁코가\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4960 all=37037 active=1906 piece=겠다고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=4980 all=37086 active=1955 piece=인데요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5000 all=37105 active=1974 piece=▁고치고\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5020 all=37100 active=1850 piece=▁다가오\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5040 all=37100 active=1850 piece=▁똑같아\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5060 all=37092 active=1842 piece=▁못해서\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5080 all=37080 active=1830 piece=▁볼게요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5100 all=37070 active=1820 piece=▁슬프게\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5120 all=37061 active=1844 piece=▁여자도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5140 all=37056 active=1839 piece=▁재밌는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5160 all=37044 active=1827 piece=▁차이는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5180 all=37033 active=1816 piece=▁하우스\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5200 all=37030 active=1813 piece=지겠네요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5220 all=37036 active=1855 piece=▁달라진게\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5240 all=37022 active=1841 piece=▁믿으세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5260 all=37007 active=1826 piece=▁심호흡을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5280 all=36995 active=1814 piece=▁이상형은\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5300 all=36981 active=1800 piece=▁정리하고\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5320 all=36967 active=1836 piece=▁헤어짐이\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5340 all=36955 active=1824 piece=▁들어주세요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5360 all=36937 active=1806 piece=▁외국인인데\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5 size=5380 all=36923 active=1792 piece=▁있으신가봐요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5400 all=36935 active=1804 piece=▁멘\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5420 all=36948 active=1859 piece=▁팍\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5440 all=36987 active=1898 piece=그만\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5460 all=37021 active=1932 piece=렀어\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5480 all=37050 active=1961 piece=밖은\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5500 all=37091 active=2002 piece=아빠\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5520 all=37126 active=1888 piece=인줄\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5540 all=37167 active=1929 piece=트롤\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5560 all=37192 active=1954 piece=▁같다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5580 all=37192 active=1954 piece=▁글자\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5600 all=37189 active=1951 piece=▁너의\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5620 all=37191 active=1862 piece=▁돌려\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5640 all=37186 active=1857 piece=▁막막\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5660 all=37190 active=1861 piece=▁미루\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5680 all=37204 active=1875 piece=▁변신\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5700 all=37226 active=1897 piece=▁사표\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5720 all=37242 active=1877 piece=▁쌓여\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5740 all=37246 active=1881 piece=▁열흘\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5760 all=37252 active=1887 piece=▁원동\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5780 all=37257 active=1892 piece=▁잊기\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5800 all=37272 active=1907 piece=▁주량\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5820 all=37290 active=1881 piece=▁차인\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5840 all=37293 active=1884 piece=▁태연\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5860 all=37301 active=1892 piece=▁해로\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5880 all=37319 active=1910 piece=드리는\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5900 all=37350 active=1941 piece=지능에\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5920 all=37384 active=1901 piece=▁가려고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5940 all=37369 active=1886 piece=▁결국엔\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5960 all=37354 active=1871 piece=▁기르고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=5980 all=37343 active=1860 piece=▁남이니\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6000 all=37328 active=1845 piece=▁돌리는\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6020 all=37317 active=1856 piece=▁떨어뜨\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6040 all=37309 active=1848 piece=▁맛있어\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6060 all=37297 active=1836 piece=▁뭐하나\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6080 all=37281 active=1820 piece=▁보름달\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6100 all=37271 active=1810 piece=▁산책을\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6120 all=37260 active=1853 piece=▁술마시\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6140 all=37248 active=1841 piece=▁아니네\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6160 all=37241 active=1834 piece=▁애틋한\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6180 all=37229 active=1822 piece=▁염색을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6200 all=37215 active=1808 piece=▁위로를\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6220 all=37207 active=1853 piece=▁일요일\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6240 all=37202 active=1848 piece=▁재밌을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6260 all=37190 active=1836 piece=▁준다면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6280 all=37171 active=1817 piece=▁최악의\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6300 all=37156 active=1802 piece=▁한번더\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6320 all=37144 active=1846 piece=▁희망은\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6340 all=37164 active=1866 piece=해야할지\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6360 all=37160 active=1862 piece=▁고마움을\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6380 all=37142 active=1844 piece=▁남이에요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6400 all=37126 active=1828 piece=▁달라지지\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6420 all=37107 active=1838 piece=▁모아야할\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6440 all=37090 active=1821 piece=▁비교하면\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6460 all=37078 active=1809 piece=▁선생님께\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6480 all=37063 active=1794 piece=▁아름답죠\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6500 all=37046 active=1777 piece=▁여자에게\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6520 all=37031 active=1838 piece=▁이불밖은\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6540 all=37017 active=1824 piece=▁잘지내고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6560 all=37000 active=1807 piece=▁취미네요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6580 all=36984 active=1791 piece=▁함께하고\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6600 all=36967 active=1774 piece=조심하세요\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6620 all=36959 active=1839 piece=▁나쁜거예요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6640 all=36939 active=1819 piece=▁만들었어요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6660 all=36920 active=1800 piece=▁생겼을지도\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6680 all=36901 active=1781 piece=▁없으니까요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6700 all=36882 active=1762 piece=▁잘못입니다\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6720 all=36865 active=1828 piece=▁친해지는게\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6740 all=36847 active=1810 piece=▁낳을뿐이에요\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4 size=6760 all=36827 active=1790 piece=▁챙겨드시거나\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6780 all=36815 active=1778 piece=▁껴\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6800 all=36829 active=1792 piece=▁볶\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=3\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6820 all=36841 active=1852 piece=▁턱\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6840 all=36857 active=1868 piece=갔다\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6860 all=36882 active=1893 piece=꺼풀\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6880 all=36904 active=1915 piece=더군\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6900 all=36931 active=1942 piece=로션\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3 min_freq=2\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6920 all=36960 active=1875 piece=보낼\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=6940 all=37003 active=1918 piece=셨어\n",
      "trainer_interface.cc(689) LOG(INFO) Saving model: chatbot.model\n",
      "trainer_interface.cc(701) LOG(INFO) Saving vocabs: chatbot.vocab\n"
     ]
    }
   ],
   "source": [
    "corpus = \"all.txt\"\n",
    "prefix = \"chatbot\"\n",
    "vocab_size = 8000\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f9be181e6690774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T17:27:59.475291Z",
     "start_time": "2025-09-10T17:27:59.352427Z"
    }
   },
   "outputs": [],
   "source": [
    "#Lexicla Aug를 이용한 데이터 증강\n",
    "import random\n",
    "import gensim\n",
    "\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"ko_c.bin\", binary=True)\n",
    "\n",
    "def lexical_augmentation(sentence_list, wv_model, num_augs=1):\n",
    "    \"\"\"\n",
    "    주어진 문장 리스트에 대해 Lexical Substitution을 적용하여 데이터를 증강하는 함수.\n",
    "\n",
    "    Args:\n",
    "        sentence_list (list): 증강할 원본 문장들의 리스트.\n",
    "        wv_model: Gensim의 KeyedVectors 객체.\n",
    "        num_augs (int): 원본 문장 1개당 생성할 증강 문장의 개수.\n",
    "\n",
    "    Returns:\n",
    "        list: 원본 문장과 증강된 문장이 모두 포함된 리스트.\n",
    "    \"\"\"\n",
    "\n",
    "    # (내부 lexical_sub 함수는 이전과 동일)\n",
    "    def lexical_sub(sentence, wv):\n",
    "        tokens = sentence.split()\n",
    "        #valid_tokens = [tok for tok in tokens if tok in wv.key_to_vec]\n",
    "        # 'wv' 객체에 토큰이 직접 있는지 확인하는 방식으로 변경\n",
    "        valid_tokens = [tok for tok in tokens if tok in wv]\n",
    "        if not valid_tokens:\n",
    "            return None # 변경: 증강 실패 시 None 반환\n",
    "        selected_tok = random.choice(valid_tokens)\n",
    "        try:\n",
    "            similar_word = wv.most_similar(selected_tok)[0][0]\n",
    "        except KeyError:\n",
    "            return None # 변경: 증강 실패 시 None 반환\n",
    "        return \" \".join([similar_word if tok == selected_tok else tok for tok in tokens])\n",
    "\n",
    "\n",
    "    augmented_corpus = []\n",
    "    print(f\"데이터 증강을 시작합니다 (문장당 최대 {num_augs}개 생성)...\")\n",
    "\n",
    "    # 모든 원본 문장을 우선 корпу스에 포함시킵니다.\n",
    "    augmented_corpus.extend(sentence_list)\n",
    "\n",
    "    for sentence in tqdm(sentence_list):\n",
    "        for _ in range(num_augs):\n",
    "            new_sentence = lexical_sub(sentence, wv_model)\n",
    "            if new_sentence: # 증강에 성공한 경우에만 추가\n",
    "                augmented_corpus.append(new_sentence)\n",
    "\n",
    "    # 최종 중복 제거\n",
    "    augmented_corpus = list(dict.fromkeys(augmented_corpus))\n",
    "    print(f\"데이터 증강 완료! (원본: {len(sentence_list)} 문장 -> 증강 후: {len(augmented_corpus)} 문장)\")\n",
    "\n",
    "    return augmented_corpus\n",
    "\n",
    "# --- 사용 예시 ---\n",
    "# que_train_str, ans_train_str가 있다고 가정\n",
    "# 원본의 약 2배로 증강 (원본 1 + 증강 1)\n",
    "# que_train_augmented = lexical_augmentation(que_train_str, wv, num_augs=1)\n",
    "# ans_train_augmented = lexical_augmentation(ans_train_str, wv, num_augs=1)\n",
    "\n",
    "# 원본의 약 3배로 증강 (원본 1 + 증강 2)\n",
    "# que_train_augmented = lexical_augmentation(que_train_str, wv, num_augs=2)\n",
    "# ans_train_augmented = lexical_augmentation(ans_train_str, wv, num_augs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c741a083b6ae3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 증강을 시작합니다 (문장당 최대 2개 생성)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8867/8867 [00:25<00:00, 343.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 증강 완료! (원본: 8867 문장 -> 증강 후: 17810 문장)\n",
      "데이터 증강을 시작합니다 (문장당 최대 2개 생성)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8867/8867 [00:32<00:00, 272.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 증강 완료! (원본: 8867 문장 -> 증강 후: 15440 문장)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#train, test 분리\n",
    "que_train, que_test, ans_train, ans_test = train_test_split(\n",
    "                                                    questions,\n",
    "                                                    answers,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    shuffle = True,\n",
    "                                                    random_state = 21)\n",
    "\n",
    "\n",
    "# 4. 전처리된 train데이터를 Lexical aug를 이용해 증강\n",
    "if 'wv' in globals() and wv is not None:\n",
    "    que_train_augmented = lexical_augmentation(que_train, wv, num_augs=2)\n",
    "    ans_train_augmented = lexical_augmentation(ans_train, wv, num_augs=2)\n",
    "    # 질문(Question) 학습 데이터 증강\n",
    "    #que_train = lexical_augmentation(que_train, wv)\n",
    "\n",
    "    # 답변(Answer) 학습 데이터 증강\n",
    "    #ans_train = lexical_augmentation(ans_train, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "767d64cc-ffc5-40a2-99f4-65be465aa641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3달이 지났네', '어떻하죠 ? 이 상황을', '이제 나 자신으로 돌아와 아름다워지고 싶어 .', '갑자기 선을 긋는 사람', '그렇게 갈 거면서', '인테리어 다시 하고싶어 .', '우리가 좋은데 왜 반대하시지', '미팅한다 !', '소심남 좋아하는 여자도 있나요 ?', '밤이되니깐 감수성 터짐']\n",
      "['물리적인 시간은 잘도 가죠 .', '잠깐 눈을 감고 차분하게 받아들여요 .', '좋은 생각이에요 . 한차례 성숙해진 자신을 발견할 거예요 .', '너무 사적 영역으로 들어왔나봅니다 .', '이야기를 해보세요 .', '나만의 공간으로 꾸며보세요 .', '현실의 벽에 부딪혔나봐요 .', '성공을 기원합니다 .', '사람마다 다를 거예요 .', '새벽은 감수성 더 터지니 얼른 주무세요 .']\n"
     ]
    }
   ],
   "source": [
    "print(que_train_augmented[:10])\n",
    "print(ans_train_augmented[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c639ce3-bff6-4ce4-8e11-57903886c003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 원본 que ---\n",
      "['3달이 지났네', '어떻하죠 ? 이 상황을', '이제 나 자신으로 돌아와 아름다워지고 싶어 .', '갑자기 선을 긋는 사람', '그렇게 갈 거면서']\n",
      "\n",
      "--- 증강된 que ---\n",
      "['어떻하죠 ? 그러 상황을', '그때 나 자신으로 돌아와 아름다워지고 싶어 .', '갑작스레 선을 긋는 사람', '갑자기 선을 긋는 젊은이', '엄청 갈 거면서']\n",
      "\n",
      "--- 원본 ans ---\n",
      "['물리적인 시간은 잘도 가죠 .', '잠깐 눈을 감고 차분하게 받아들여요 .', '좋은 생각이에요 . 한차례 성숙해진 자신을 발견할 거예요 .', '너무 사적 영역으로 들어왔나봅니다 .', '이야기를 해보세요 .']\n",
      "\n",
      "--- 증강된 ans ---\n",
      "['물리적인 시간은 잘도 가죠 는데', '잠시 눈을 감고 차분하게 받아들여요 .', '좋은 생각이에요 는데 한차례 성숙해진 자신을 발견할 거예요 는데', '너무 역사적 영역으로 들어왔나봅니다 .', '이야기를 해보세요 는데']\n"
     ]
    }
   ],
   "source": [
    "#증강된 데이터 탐색\n",
    "original_que_set = set(que_train)\n",
    "original_ans_set = set(ans_train)\n",
    "\n",
    "# 2. augmented_corpus에서 원본에 없는 증강된 문장들만 추출\n",
    "only_augmented_que = [\n",
    "    sentence for sentence in que_train_augmented\n",
    "    if sentence not in original_que_set\n",
    "]\n",
    "\n",
    "only_augmented_ans = [\n",
    "    sentence for sentence in ans_train_augmented\n",
    "    if sentence not in original_ans_set\n",
    "]\n",
    "\n",
    "# 3. 결과 확인\n",
    "print(\"--- 원본 que ---\")\n",
    "print(que_train[:5])\n",
    "\n",
    "print(\"\\n--- 증강된 que ---\")\n",
    "print(only_augmented_que[:5])\n",
    "\n",
    "print(\"\\n--- 원본 ans ---\")\n",
    "print(ans_train[:5])\n",
    "\n",
    "print(\"\\n--- 증강된 ans ---\")\n",
    "print(only_augmented_ans[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b102fc-3b35-4260-953e-9e62e4f66304",
   "metadata": {},
   "outputs": [],
   "source": [
    "'는데'어미로 끝나는 증강이 많이 보이는것을 확인\n",
    "아마 이부분이 오히려 노이즈로 작용해 점수를 낮췄다고 예상할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fca08a674e1db601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이를 40으로 정의\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "START_TOKEN = [2]\n",
    "END_TOKEN = [3]\n",
    "\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(\"chatbot.model\")\n",
    "\n",
    "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    zeros1 = np.zeros(MAX_LENGTH, dtype=int)\n",
    "    zeros2 = np.zeros(MAX_LENGTH, dtype=int)\n",
    "    sentence1 = START_TOKEN + vocab.encode_as_ids(sentence1) + END_TOKEN\n",
    "    zeros1[:len(sentence1)] = sentence1[:MAX_LENGTH]\n",
    "\n",
    "    sentence2 = START_TOKEN + vocab.encode_as_ids(sentence2) + END_TOKEN\n",
    "    zeros2[:len(sentence2)] = sentence2[:MAX_LENGTH]\n",
    "\n",
    "    tokenized_inputs.append(zeros1)\n",
    "    tokenized_outputs.append(zeros2)\n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4763a693627f7a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 5557 6990 3205  111    3    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[   2 5185  217 5924    7    3    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "#인코딩 되어 잘 저장된것을 확인\n",
    "questions_encode, answers_encode = tokenize_and_filter(questions, answers)\n",
    "print(questions_encode[0])\n",
    "print(answers_encode[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24141674af06fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_pad(corpus, vocab, max_len):\n",
    "    encoded_corpus = []\n",
    "    for sentence_tokens in corpus:\n",
    "        # 1. 토큰 리스트를 정수 ID 리스트로 변환\n",
    "        ids = [vocab.piece_to_id(token) for token in sentence_tokens]\n",
    "\n",
    "        # 2. 시작 토큰과 종료 토큰 추가\n",
    "        ids = START_TOKEN + ids + END_TOKEN\n",
    "\n",
    "        # 3. 최대 길이에 맞춰 패딩\n",
    "        padded_ids = np.zeros(max_len, dtype=int)\n",
    "        padded_ids[:len(ids)] = ids[:max_len] # 최대 길이를 넘으면 자르기\n",
    "\n",
    "        encoded_corpus.append(padded_ids)\n",
    "\n",
    "    return np.array(encoded_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e234b65a-b17c-4784-8938-22e18f4cd80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(src_data, tgt_data, tokenizer, max_len=40):\n",
    "    \"\"\"\n",
    "    소스 및 타겟 데이터를 받아 전처리 후 토큰화된 말뭉치를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        src_data (list): 소스 문장 리스트\n",
    "        tgt_data (list): 타겟 문장 리스트\n",
    "        tokenizer (function): 사용할 토크나이저 함수 (e.g., mecab.morphs)\n",
    "        max_len (int): 문장의 최대 토큰 길이\n",
    "\n",
    "    Returns:\n",
    "        tuple: 정제되고 중복이 제거된 (소스 말뭉치, 타겟 말뭉치)\n",
    "    \"\"\"\n",
    "    corpus_pairs = []\n",
    "    for src_sentence, tgt_sentence in zip(src_data, tgt_data):\n",
    "        # 2-1. 문장 정제\n",
    "        src_preprocessed = preprocess_sentence(src_sentence)\n",
    "        tgt_preprocessed = preprocess_sentence(tgt_sentence)\n",
    "\n",
    "        # 2-2. 토큰화\n",
    "        src_tokens = tokenizer.encode_as_pieces(src_preprocessed) # 객체의 메소드를 호출\n",
    "        tgt_tokens = tokenizer.encode_as_pieces(tgt_preprocessed)\n",
    "\n",
    "        # 3. 토큰 개수 필터링\n",
    "        if len(src_tokens) > max_len or len(tgt_tokens) > max_len:\n",
    "            continue\n",
    "\n",
    "        corpus_pairs.append((\" \".join(src_tokens), \" \".join(tgt_tokens)))\n",
    "\n",
    "    # 4. 중복 제거\n",
    "    # 데이터프레임을 사용하여 소스-타겟 쌍을 유지하면서 중복을 안전하게 제거\n",
    "    df = pd.DataFrame(corpus_pairs, columns=['source', 'target'])\n",
    "\n",
    "    # 소스 문장 기준 중복 제거 (첫 번째 등장 쌍 유지)\n",
    "    df = df.drop_duplicates(subset='source', keep='first')\n",
    "\n",
    "    # 타겟 문장 기준 중복 제거 (첫 번째 등장 쌍 유지)\n",
    "    df = df.drop_duplicates(subset='target', keep='first')\n",
    "\n",
    "    # 다시 토큰 리스트 형태로 변환하여 반환\n",
    "    cleaned_src_corpus = [s.split() for s in df['source']]\n",
    "    cleaned_tgt_corpus = [t.split() for t in df['target']]\n",
    "\n",
    "    print(\"데이터 준비 끝!\")\n",
    "    return cleaned_src_corpus, cleaned_tgt_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86e521c802bf34f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 준비 끝!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. 함수를 활용하여 que_corpus, ans_corpus 생성\n",
    "# 기존 CSV 파일의 'Q', 'A' 컬럼을 리스트로 변환하여 사용\n",
    "from sklearn.model_selection import train_test_split\n",
    "questions_list = list(train_data['Q'])\n",
    "answers_list = list(train_data['A'])\n",
    "\n",
    "que_corpus, ans_corpus = build_corpus(questions_list, answers_list, vocab, max_len=40)\n",
    "\n",
    "\n",
    "que_corpus_train, que_corpus_test, ans_corpus_train, ans_corpus_test = train_test_split(que_corpus,\n",
    "                                                    ans_corpus,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    shuffle = True,\n",
    "                                                    random_state = 21)\n",
    "\n",
    "que_corpus_train_encoded = encode_and_pad(que_corpus_train, vocab, MAX_LENGTH)\n",
    "ans_corpus_train_encoded = encode_and_pad(ans_corpus_train, vocab, MAX_LENGTH)\n",
    "\n",
    "que_corpus_test_encoded = encode_and_pad(que_corpus_test, vocab, MAX_LENGTH)\n",
    "ans_corpus_test_encoded = encode_and_pad(ans_corpus_test, vocab, MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03fb2e07781373a",
   "metadata": {},
   "source": [
    "\n",
    "모델링 시작\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdada8046f23003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, questions, answers):\n",
    "        self.inputs = questions\n",
    "        self.dec_inputs = answers[:,:-1]\n",
    "        self.outputs = answers[:,1:]\n",
    "        self.length = len(questions)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return (self.inputs[idx], self.dec_inputs[idx], self.outputs[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "# 3. 전처리된 데이터를 사용하여 DataLoader 생성\n",
    "BATCH_SIZE = 64\n",
    "train_dataset = SequenceDataset(que_corpus_train_encoded, ans_corpus_train_encoded)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "# (선택) 테스트용 DataLoader도 만들 수 있습니다.\n",
    "test_dataset = SequenceDataset(que_corpus_test_encoded, ans_corpus_test_encoded)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"DataLoader 준비 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a89a3064fc3d4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Transformer\n",
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class TFModel(nn.Module):\n",
    "    #ntoken: vocab의 size\n",
    "    #ninp: embedding할 차원의 크기\n",
    "    #nhead: num head\n",
    "    #nhid: feedforward의 차원\n",
    "    #nlayers: layer의 개수\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TFModel, self).__init__()\n",
    "        self.transformer = Transformer(ninp, nhead, dim_feedforward=nhid, num_encoder_layers=nlayers, num_decoder_layers=nlayers,dropout=dropout)\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "\n",
    "        self.pos_encoder_d = PositionalEncoding(ninp, dropout)\n",
    "        self.encoder_d = nn.Embedding(ntoken, ninp)\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.ntoken = ntoken\n",
    "\n",
    "        self.linear = nn.Linear(ninp, ntoken)\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, tgt, srcmask, tgtmask, srcpadmask, tgtpadmask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        tgt = self.encoder_d(tgt) * math.sqrt(self.ninp)\n",
    "        tgt = self.pos_encoder_d(tgt)\n",
    "\n",
    "\n",
    "        output = self.transformer(src.transpose(0,1), tgt.transpose(0,1), srcmask, tgtmask, src_key_padding_mask=srcpadmask, tgt_key_padding_mask=tgtpadmask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def gen_attention_mask(x):\n",
    "    mask = torch.eq(x, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b350c3d-4f29-41ef-b0a3-be5a4c8cdeed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "lr = 1e-4\n",
    "model_aug_v2 = TFModel(vocab_size+7, 256, 8, 512, 2, 0.2).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_aug_v2.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8512895d-f14c-4807-972b-3631e7f5b3c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/91 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[20]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m (inputs, dec_inputs, outputs) \u001B[38;5;129;01min\u001B[39;00m progress:\n\u001B[32m      9\u001B[39m     optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m     src_mask = \u001B[43mmodel\u001B[49m.generate_square_subsequent_mask(MAX_LENGTH).to(device)\n\u001B[32m     11\u001B[39m     src_padding_mask = gen_attention_mask(inputs).to(device)\n\u001B[32m     12\u001B[39m     tgt_mask = model.generate_square_subsequent_mask(MAX_LENGTH-\u001B[32m1\u001B[39m).to(device)\n",
      "\u001B[31mNameError\u001B[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "epoch = 30\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_aug_v2.train()\n",
    "for i in range(epoch):\n",
    "    batchloss = 0.0\n",
    "    progress = tqdm(train_dataloader)\n",
    "    for (inputs, dec_inputs, outputs) in progress:\n",
    "        optimizer.zero_grad()\n",
    "        src_mask = model.generate_square_subsequent_mask(MAX_LENGTH).to(device)\n",
    "        src_padding_mask = gen_attention_mask(inputs).to(device)\n",
    "        tgt_mask = model.generate_square_subsequent_mask(MAX_LENGTH-1).to(device)\n",
    "        tgt_padding_mask = gen_attention_mask(dec_inputs).to(device)\n",
    "\n",
    "        result = model(inputs.to(device), dec_inputs.to(device), src_mask, tgt_mask, src_padding_mask,tgt_padding_mask)\n",
    "        loss = criterion(result.permute(1,2,0), outputs.to(device).long())\n",
    "        progress.set_description(\"{:0.3f}\".format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batchloss += loss\n",
    "    print(\"epoch:\",i+1,\"|\",\"loss:\",batchloss.cpu().item() / len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bdeca2b6-283d-4426-8085-dff382df5ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, version) :\n",
    "    model.train()\n",
    "    epoch = 30\n",
    "    for i in range(epoch):\n",
    "        batchloss = 0.0\n",
    "        progress = tqdm(train_dataloader)\n",
    "        for (inputs, dec_inputs, outputs) in progress:\n",
    "            optimizer.zero_grad()\n",
    "            src_mask = model.generate_square_subsequent_mask(MAX_LENGTH).to(device)\n",
    "            src_padding_mask = gen_attention_mask(inputs).to(device)\n",
    "            tgt_mask = model.generate_square_subsequent_mask(MAX_LENGTH-1).to(device)\n",
    "            tgt_padding_mask = gen_attention_mask(dec_inputs).to(device)\n",
    "    \n",
    "            result = model(inputs.to(device), dec_inputs.to(device), src_mask, tgt_mask, src_padding_mask,tgt_padding_mask)\n",
    "            loss = criterion(result.permute(1,2,0), outputs.to(device).long())\n",
    "            progress.set_description(\"{:0.3f}\".format(loss))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batchloss += loss\n",
    "        print(\"epoch:\",i+1,\"|\",\"loss:\",batchloss.cpu().item() / len(train_dataloader))\n",
    "\n",
    "    #모델 저장 및 로드\n",
    "    PATH = \"chatbot_model_state_aug_v{}.pth\".format(version)\n",
    "    \n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print(f\"모델이 {PATH} 경로에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce0cc7ac-22b1-420f-9094-3ca4788079c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.371: 100%|██████████| 91/91 [00:04<00:00, 20.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 | loss: 2.488067459274124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.082: 100%|██████████| 91/91 [00:04<00:00, 21.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 | loss: 1.3252109904865643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.051: 100%|██████████| 91/91 [00:04<00:00, 21.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 | loss: 1.178278074159727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.035: 100%|██████████| 91/91 [00:04<00:00, 21.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 | loss: 1.1261371151431576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.197: 100%|██████████| 91/91 [00:04<00:00, 21.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 | loss: 1.1023785936963426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.128: 100%|██████████| 91/91 [00:04<00:00, 20.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 | loss: 1.0865057641333276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.144: 100%|██████████| 91/91 [00:04<00:00, 20.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 | loss: 1.0738753434065933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.991: 100%|██████████| 91/91 [00:04<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 | loss: 1.0625557532677283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.273: 100%|██████████| 91/91 [00:04<00:00, 20.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 | loss: 1.0550099467183207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.978: 100%|██████████| 91/91 [00:04<00:00, 20.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 | loss: 1.0453253106756524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.016: 100%|██████████| 91/91 [00:04<00:00, 21.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 | loss: 1.037755861387148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.211: 100%|██████████| 91/91 [00:04<00:00, 21.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 | loss: 1.0306453495235233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.062: 100%|██████████| 91/91 [00:04<00:00, 21.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 | loss: 1.0224971561641483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.986: 100%|██████████| 91/91 [00:04<00:00, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 | loss: 1.014973399403331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.137: 100%|██████████| 91/91 [00:04<00:00, 21.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 | loss: 1.007560310782967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.039: 100%|██████████| 91/91 [00:04<00:00, 21.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 | loss: 0.9992736984085251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.113: 100%|██████████| 91/91 [00:04<00:00, 21.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 | loss: 0.9919329632769575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.123: 100%|██████████| 91/91 [00:04<00:00, 21.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 | loss: 0.9830261062789749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.016: 100%|██████████| 91/91 [00:04<00:00, 21.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 | loss: 0.9746379642696171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.124: 100%|██████████| 91/91 [00:04<00:00, 21.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20 | loss: 0.966238755446214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.889: 100%|██████████| 91/91 [00:04<00:00, 21.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 | loss: 0.9567554180438702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.920: 100%|██████████| 91/91 [00:04<00:00, 21.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22 | loss: 0.948025378552112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.865: 100%|██████████| 91/91 [00:04<00:00, 21.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23 | loss: 0.9384269295157966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.921: 100%|██████████| 91/91 [00:04<00:00, 21.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24 | loss: 0.9295269473568424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.961: 100%|██████████| 91/91 [00:04<00:00, 21.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25 | loss: 0.9211330204219609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.800: 100%|██████████| 91/91 [00:04<00:00, 21.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26 | loss: 0.9090652465820312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.851: 100%|██████████| 91/91 [00:04<00:00, 21.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27 | loss: 0.9005914206033224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.911: 100%|██████████| 91/91 [00:04<00:00, 21.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28 | loss: 0.8902788267030821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.768: 100%|██████████| 91/91 [00:04<00:00, 21.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29 | loss: 0.8795450231531164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.868: 100%|██████████| 91/91 [00:04<00:00, 21.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30 | loss: 0.8689962533804086\n",
      "모델이 chatbot_model_state_aug_v2.pth 경로에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "model_train(model_aug_v2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2008c5-a2c9-40d9-94b7-2aa678753800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#저장된 모델 로드\n",
    "loaded_model = TFModel(vocab_size+7, 256, 8, 512, 2, 0.2).to(device)\n",
    "\n",
    "PATH = \"chatbot_model_v2.pth\"\n",
    "loaded_model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#학습모드가 아닌 eval모드로 전환필수\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"기존 모델 로드 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4b60481-a032-43f3-98d0-85dca5eb6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "def evaluate(model,sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    input = torch.tensor([START_TOKEN + vocab.encode_as_ids(sentence) + END_TOKEN]).to(device)\n",
    "    output = torch.tensor([START_TOKEN]).to(device)\n",
    "\n",
    "    # 디코더의 예측 시작\n",
    "    model.eval()\n",
    "    for i in range(MAX_LENGTH):\n",
    "        src_mask = model.generate_square_subsequent_mask(input.shape[1]).to(device)\n",
    "        tgt_mask = model.generate_square_subsequent_mask(output.shape[1]).to(device)\n",
    "\n",
    "        src_padding_mask = gen_attention_mask(input).to(device)\n",
    "        tgt_padding_mask = gen_attention_mask(output).to(device)\n",
    "\n",
    "        predictions = model(input, output, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask).transpose(0,1)\n",
    "        # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = torch.LongTensor(torch.argmax(predictions.cpu(), axis=-1))\n",
    "\n",
    "\n",
    "        # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "        if torch.equal(predicted_id[0][0], torch.tensor(END_TOKEN[0])):\n",
    "            break\n",
    "\n",
    "        # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
    "        # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
    "        output = torch.cat([output, predicted_id.to(device)], axis=1)\n",
    "\n",
    "    return torch.squeeze(output, axis=0).cpu().numpy()\n",
    "\n",
    "def predict(model, sentence):\n",
    "    prediction = evaluate(model, sentence)\n",
    "    predicted_sentence = vocab.Decode(list(map(int,[i for i in prediction if i < vocab_size+7])))\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24cb88b4-2f0e-462d-9ae0-02b075abb413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 놀고싶다\n",
      "Output: 좋은 사람 만날 수 있을 거예요 .\n",
      "Input: 배고파\n",
      "Output: 이제 다른 곳으로해도 .\n",
      "Input: 고민 상담 해줘\n",
      "Output: 이제 좀 더 많이 지쳤나봐요 .\n"
     ]
    }
   ],
   "source": [
    "result = predict(model_aug_v2,\"놀고싶다\")\n",
    "result = predict(model_aug_v2,\"배고파\")\n",
    "result = predict(model_aug_v2,\"고민 상담 해줘\")\n",
    "#매번 잘 나오지는 않지만 일부 잘 만드는 경우가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9372a0eb-f07f-4342-9d6b-0867da566894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "\n",
    "def predict(sentence, model, vocab):\n",
    "    prediction = evaluate(sentence, model, vocab)\n",
    "    predicted_sentence = vocab.Decode(list(map(int,[i for i in prediction if i < vocab_size+7])))\n",
    "    return predicted_sentence\n",
    "\n",
    "\n",
    "def evaluate(sentence, model, vocab):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    input_tensor = torch.tensor([START_TOKEN + vocab.encode_as_ids(sentence) + END_TOKEN]).to(device)\n",
    "    output_tensor = torch.tensor([START_TOKEN]).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(MAX_LENGTH):\n",
    "            src_mask = model.generate_square_subsequent_mask(input_tensor.shape[1]).to(device)\n",
    "            tgt_mask = model.generate_square_subsequent_mask(output_tensor.shape[1]).to(device)\n",
    "\n",
    "            src_padding_mask = gen_attention_mask(input_tensor).to(device)\n",
    "            tgt_padding_mask = gen_attention_mask(output_tensor).to(device)\n",
    "\n",
    "            predictions = model(input_tensor, output_tensor, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask).transpose(0,1)\n",
    "            predictions = predictions[:, -1:, :]\n",
    "            predicted_id = torch.argmax(predictions, axis=-1)\n",
    "\n",
    "            # if torch.equal(predicted_id[0,0], torch.tensor(END_TOKEN[0])):\n",
    "            #     break\n",
    "            # predicted_id[0,0] 텐서에서 숫자 값을 꺼내 파이썬 정수와 직접 비교\n",
    "            if predicted_id[0,0].item() == END_TOKEN[0]:\n",
    "                break\n",
    "            output_tensor = torch.cat([output_tensor, predicted_id], axis=1)\n",
    "\n",
    "    return torch.squeeze(output_tensor, axis=0).cpu().numpy()\n",
    "\n",
    "\n",
    "# --- [BLEU 점수 계산 함수] ---\n",
    "def calculate_bleu_score(model, questions_corpus, answers_corpus, vocab, weights):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model (torch.nn.Module): 평가할 학습된 모델\n",
    "        questions_corpus (list): 토큰화된 질문 문장 리스트\n",
    "        answers_corpus (list): 토큰화된 정답 문장 리스트\n",
    "        vocab (spm.SentencePieceProcessor): SentencePiece 토크나이저\n",
    "    \"\"\"\n",
    "    print(\"BLEU 점수 계산을 시작합니다...\")\n",
    "    \n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    \n",
    "    # SmoothingFunction은 reference에 없는 단어가 hypothesis에 나올 경우 점수가 0이 되는 것을 방지합니다.\n",
    "    smoothie = SmoothingFunction().method1\n",
    "\n",
    "    for que_tokens, ans_tokens in tqdm(zip(questions_corpus, answers_corpus), total=len(questions_corpus)):\n",
    "        # 정답 문장은 이중 리스트 형태(하나의 질문에 여러 정답이 있을 수 있기 때문)\n",
    "        references.append([ans_tokens])\n",
    "        \n",
    "        # 질문 토큰을 다시 문자열로 합쳐 모델의 입력으로 사용\n",
    "        question_str = vocab.decode(que_tokens)\n",
    "        \n",
    "        # 모델 예측\n",
    "        predicted_sentence = predict(question_str, model, vocab)\n",
    "        \n",
    "        # 예측된 문장을 다시 토큰화\n",
    "        hypothesis_tokens = vocab.encode_as_pieces(predicted_sentence)\n",
    "        hypotheses.append(hypothesis_tokens)\n",
    "\n",
    "    # corpus_bleu를 사용하여 전체 테스트셋에 대한 BLEU 점수 계산\n",
    "    # weights는 BLEU-1, 2, 3, 4의 가중치를 의미하며, (0.25, 0.25, 0.25, 0.25)는 BLEU-4를 의미합니다.\n",
    "    bleu_score = corpus_bleu(references, hypotheses, weights, smoothing_function=smoothie)\n",
    "    \n",
    "    return bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "48704aed-af13-40e8-a3e6-12415d027b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다양한 가중치에 따른 BLEU SCORE 평가\n",
      "--------------------------------------------------\n",
      "BLEU 점수 계산을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1933/1933 [01:06<00:00, 28.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1 (Individual): 35.96%\n",
      "--------------------------------------------------\n",
      "BLEU 점수 계산을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1933/1933 [01:06<00:00, 28.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-2 (Cumulative): 25.93%\n",
      "--------------------------------------------------\n",
      "BLEU 점수 계산을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1933/1933 [01:07<00:00, 28.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-3 (Cumulative): 21.52%\n",
      "--------------------------------------------------\n",
      "BLEU 점수 계산을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1933/1933 [01:06<00:00, 28.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 (Cumulative): 18.58%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#데이터가 짧은 대화 위주다 보니 n이 높은것보단 낮은게 더 평가점수가 높을것이라 예상 \n",
    "#이를 검증하는 간단한 실험 진행\n",
    "# 테스트하고 싶은 weights를 딕셔너리로 정의\n",
    "weights_to_test = {\n",
    "    \"BLEU-1 (Individual)\": (1, 0, 0, 0),\n",
    "    \"BLEU-2 (Cumulative)\": (0.5, 0.5, 0, 0),\n",
    "    \"BLEU-3 (Cumulative)\": (1/3, 1/3, 1/3, 0),\n",
    "    \"BLEU-4 (Cumulative)\": (0.25, 0.25, 0.25, 0.25)\n",
    "}\n",
    "\n",
    "print(\"다양한 가중치에 따른 BLEU SCORE 평가\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, weights in weights_to_test.items():\n",
    "    score = calculate_bleu_score(model, que_corpus_test, ans_corpus_test, vocab, weights)\n",
    "    print(f\"{name}: {score*100:.2f}%\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c5ac86-3644-4924-b343-f67f3ed15314",
   "metadata": {},
   "source": [
    "예상에 맞게 낮은 n에 가중치를 주는편이 그렇지 않은쪽보다 더 좋은 점수를 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "155ee6ff-2b3f-48ce-9e7b-077aeb9275b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다양한 가중치에 따른 BLEU SCORE 평가\n",
      "--------------------------------------------------\n",
      "BLEU 점수 계산을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1933/1933 [01:06<00:00, 28.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1 (Individual): 35.96%\n",
      "--------------------------------------------------\n",
      "BLEU 점수 계산을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1933/1933 [01:06<00:00, 28.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-2 (Cumulative): 18.70%\n",
      "--------------------------------------------------\n",
      "BLEU 점수 계산을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1933/1933 [01:06<00:00, 29.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-3 (Cumulative): 14.81%\n",
      "--------------------------------------------------\n",
      "BLEU 점수 계산을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1933/1933 [01:06<00:00, 29.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 (Cumulative): 11.96%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#단순 그램별 스코어 차이도 확인\n",
    "\n",
    "weights_to_test = {\n",
    "    \"BLEU-1 (Individual)\": (1, 0, 0, 0),\n",
    "    \"BLEU-2 (Cumulative)\": (0, 1, 0, 0),\n",
    "    \"BLEU-3 (Cumulative)\": (0, 0, 1, 0),\n",
    "    \"BLEU-4 (Cumulative)\": (0, 0, 0, 1)\n",
    "}\n",
    "\n",
    "print(\"다양한 가중치에 따른 BLEU SCORE 평가\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, weights in weights_to_test.items():\n",
    "    score = calculate_bleu_score(model, que_corpus_test, ans_corpus_test, vocab, weights)\n",
    "    print(f\"{name}: {score*100:.2f}%\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45980ca2-aee7-41af-8dbc-53b502862a5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다양한 가중치에 따른 BLEU SCORE 평가\n",
      "--------------------------------------------------\n",
      "BLEU 점수 계산을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1933/1933 [01:37<00:00, 19.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1 (Individual): 9.71%\n",
      "--------------------------------------------------\n",
      "BLEU 점수 계산을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1933/1933 [01:36<00:00, 20.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-2 (Cumulative): 2.50%\n",
      "--------------------------------------------------\n",
      "BLEU 점수 계산을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1933/1933 [01:35<00:00, 20.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-3 (Cumulative): 1.00%\n",
      "--------------------------------------------------\n",
      "BLEU 점수 계산을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1933/1933 [01:35<00:00, 20.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4 (Cumulative): 0.53%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#데이터가 짧은 대화 위주다 보니 n이 높은것보단 낮은게 더 평가점수가 높을것이라 예상 \n",
    "#이를 검증하는 간단한 실험 진행\n",
    "# 테스트하고 싶은 weights를 딕셔너리로 정의\n",
    "weights_to_test = {\n",
    "    \"BLEU-1 (Individual)\": (1, 0, 0, 0),\n",
    "    \"BLEU-2 (Cumulative)\": (0.5, 0.5, 0, 0),\n",
    "    \"BLEU-3 (Cumulative)\": (1/3, 1/3, 1/3, 0),\n",
    "    \"BLEU-4 (Cumulative)\": (0.25, 0.25, 0.25, 0.25)\n",
    "}\n",
    "\n",
    "print(\"다양한 가중치에 따른 BLEU SCORE 평가\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, weights in weights_to_test.items():\n",
    "    score = calculate_bleu_score(model_aug_v2, que_corpus_test, ans_corpus_test, vocab, weights)\n",
    "    print(f\"{name}: {score*100:.2f}%\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a9bfa2-1a1b-4b9c-8bd0-99d5cc215c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터가 짧은 대화 위주다 보니 n이 높은것보단 낮은게 더 평가점수가 높을것이라 예상 \n",
    "#이를 검증하는 간단한 실험 진행\n",
    "# 테스트하고 싶은 weights를 딕셔너리로 정의\n",
    "\n",
    "weights_to_test = {\n",
    "    \"BLEU-1 (Individual)\": (1, 0, 0, 0),\n",
    "    \"BLEU-2 (Cumulative)\": (0.5, 0.5, 0, 0),\n",
    "    \"BLEU-3 (Cumulative)\": (1/3, 1/3, 1/3, 0),\n",
    "    \"BLEU-4 (Cumulative)\": (0.25, 0.25, 0.25, 0.25)\n",
    "}\n",
    "\n",
    "print(\"다양한 가중치에 따른 BLEU SCORE 평가\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, weights in weights_to_test.items():\n",
    "    score = calculate_bleu_score(model_aug_v2, que_corpus_test, ans_corpus_test, vocab, weights)\n",
    "    print(f\"{name}: {score*100:.2f}%\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcacc39-d5fb-430f-b52e-2868afbf49f8",
   "metadata": {},
   "source": [
    "회고\n",
    "데이터 증강을 한다고 무조건적으로 성능이 좋아지는건 아니다\n",
    "어제 융님과 대화하던 중 이런 내용이 나오긴 했는데 경험해보니 좀 더 치명적인 느낌이다.\n",
    "증강의 기준을 좀 명확하고 강력하게 잡아야 할듯하다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
