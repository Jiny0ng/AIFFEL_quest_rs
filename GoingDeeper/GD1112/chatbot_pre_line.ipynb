{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-09T06:07:23.812174Z",
     "start_time": "2025-09-09T06:07:23.686325Z"
    }
   },
   "source": "!mkdir -p ~/work/transformer_chatbot/data/spa-eng",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:45:37.968097Z",
     "start_time": "2025-09-10T01:45:32.682625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!python3 -m pip install --upgrade pip\n",
    "!python3 -m pip install konlpy # Python 3.x\n",
    "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh) # MeCab 설치하기\n",
    "!pip install mecab_python3"
   ],
   "id": "90da3dfe5c8d529c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (25.2)\r\n",
      "Requirement already satisfied: konlpy in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (0.6.0)\r\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from konlpy) (1.6.0)\r\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from konlpy) (6.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from konlpy) (2.0.2)\r\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\r\n",
      "mecab-ko is already installed\r\n",
      "Install mecab-ko-dic\r\n",
      "Install mecab-ko-dic\r\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r\n",
      "  6 47.4M    6 3385k    0     0  1281k      0  0:00:37  0:00:02  0:00:35 1830k^C\r\n",
      "Requirement already satisfied: mecab_python3 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (1.0.10)\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:48:23.410524Z",
     "start_time": "2025-09-10T01:45:39.429880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Mecab과 한국어 사전을 설치합니다.\n",
    "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
    "%cd Mecab-ko-for-Google-Colab\n",
    "!bash install_mecab-ko_on_colab_light_220429.sh\n",
    "\n",
    "# 2. konlpy와 mecab-python을 설치합니다.\n",
    "!pip install konlpy\n",
    "!pip install mecab-python"
   ],
   "id": "9cfca8ab773056c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Mecab-ko-for-Google-Colab' already exists and is not an empty directory.\r\n",
      "/Users/park_jinyong/Desktop/GoingDeeper/AIFFEL_quest_rs/GoingDeeper/GD1112/Mecab-ko-for-Google-Colab\n",
      "install_mecab-ko_on_colab_light_220429.sh: line 4: cd: /content: No such file or directory\r\n",
      "Installing konlpy.....\r\n",
      "Requirement already satisfied: konlpy in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (0.6.0)\r\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from konlpy) (1.6.0)\r\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from konlpy) (6.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from konlpy) (2.0.2)\r\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\r\n",
      "Done\r\n",
      "Installing mecab-0.996-ko-0.9.2.tar.gz.....\r\n",
      "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\r\n",
      "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\r\n",
      "install_mecab-ko_on_colab_light_220429.sh: line 15: wget: command not found\r\n",
      "Done\r\n",
      "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\r\n",
      "Done\r\n",
      "Change Directory to mecab-0.996-ko-0.9.2.......\r\n",
      "install_mecab-ko_on_colab_light_220429.sh: line 23: cd: mecab-0.996-ko-0.9.2/: No such file or directory\r\n",
      "installing mecab-0.996-ko-0.9.2.tar.gz........\r\n",
      "configure\r\n",
      "make\r\n",
      "make check\r\n",
      "make install\r\n",
      "ldconfig\r\n",
      "Done\r\n",
      "Change Directory to /content\r\n",
      "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\r\n",
      "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\r\n",
      "install_mecab-ko_on_colab_light_220429.sh: line 44: wget: command not found\r\n",
      "Done\r\n",
      "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\r\n",
      "Done\r\n",
      "Change Directory to mecab-ko-dic-2.1.1-20180720\r\n",
      "install_mecab-ko_on_colab_light_220429.sh: line 52: cd: mecab-ko-dic-2.1.1-20180720/: No such file or directory\r\n",
      "Done\r\n",
      "installing........\r\n",
      "configure\r\n",
      "make\r\n",
      "make install\r\n",
      "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/v0.6.0/scripts/mecab.sh)\r\n",
      "https://github.com/konlpy/konlpy/issues/395#issue-1099168405 - 2022.01.11\r\n",
      "Password:\r\n",
      "Done\r\n",
      "Install mecab-python\r\n",
      "Requirement already satisfied: konlpy in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (0.6.0)\r\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from konlpy) (1.6.0)\r\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from konlpy) (6.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from konlpy) (2.0.2)\r\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\r\n",
      "Requirement already satisfied: mecab-python in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (1.0.0)\r\n",
      "Requirement already satisfied: mecab-python3 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from mecab-python) (1.0.10)\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:49:59.671254Z",
     "start_time": "2025-09-10T01:49:52.766216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentencepiece\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install pandas"
   ],
   "id": "2642d84a15c5c762",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (0.2.1)\r\n",
      "Requirement already satisfied: torch in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from torch) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from torch) (2025.9.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: nltk in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (3.9.1)\r\n",
      "Requirement already satisfied: click in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from nltk) (8.1.8)\r\n",
      "Requirement already satisfied: joblib in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from nltk) (1.5.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from nltk) (2025.9.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from nltk) (4.67.1)\r\n",
      "Collecting pandas\r\n",
      "  Downloading pandas-2.3.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (91 kB)\r\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from pandas) (2.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Collecting pytz>=2020.1 (from pandas)\r\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\r\n",
      "Collecting tzdata>=2022.7 (from pandas)\r\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Downloading pandas-2.3.2-cp39-cp39-macosx_11_0_arm64.whl (10.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.8/10.8 MB\u001B[0m \u001B[31m8.7 MB/s\u001B[0m  \u001B[33m0:00:01\u001B[0mm0:00:01\u001B[0m0:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\r\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\r\n",
      "Installing collected packages: pytz, tzdata, pandas\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3/3\u001B[0m [pandas]2m2/3\u001B[0m [pandas]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed pandas-2.3.2 pytz-2025.2 tzdata-2025.2\r\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:32.620533Z",
     "start_time": "2025-09-10T01:50:27.168011Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install matplotlib",
   "id": "d2442b80e9ef4219",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\r\n",
      "  Downloading matplotlib-3.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (11 kB)\r\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\r\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.4 kB)\r\n",
      "Collecting cycler>=0.10 (from matplotlib)\r\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\r\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\r\n",
      "  Downloading fonttools-4.59.2-cp39-cp39-macosx_10_9_universal2.whl.metadata (109 kB)\r\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\r\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.3 kB)\r\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from matplotlib) (2.0.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from matplotlib) (25.0)\r\n",
      "Collecting pillow>=8 (from matplotlib)\r\n",
      "  Downloading pillow-11.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.0 kB)\r\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\r\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from matplotlib) (2.9.0.post0)\r\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib)\r\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\r\n",
      "Downloading matplotlib-3.9.4-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.8/7.8 MB\u001B[0m \u001B[31m8.8 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\r\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\r\n",
      "Downloading fonttools-4.59.2-cp39-cp39-macosx_10_9_universal2.whl (2.8 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.8/2.8 MB\u001B[0m \u001B[31m10.7 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\r\n",
      "Downloading kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\r\n",
      "Downloading pillow-11.3.0-cp39-cp39-macosx_11_0_arm64.whl (4.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.7/4.7 MB\u001B[0m \u001B[31m9.8 MB/s\u001B[0m  \u001B[33m0:00:00\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\r\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8/8\u001B[0m [matplotlib]8\u001B[0m [matplotlib]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.59.2 importlib-resources-6.5.2 kiwisolver-1.4.7 matplotlib-3.9.4 pillow-11.3.0 pyparsing-3.2.3\r\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:34.932302Z",
     "start_time": "2025-09-10T01:50:34.928980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sentencepiece as spm\n",
    "import torch\n",
    "print(f\"SentencePiece version: {spm.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ],
   "id": "65eda8b0a5d5d9c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece version: 0.2.1\n",
      "PyTorch version: 2.8.0\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:47.920099Z",
     "start_time": "2025-09-10T01:50:35.623127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)"
   ],
   "id": "b846a9bcd9c698ab",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:49.401264Z",
     "start_time": "2025-09-10T01:50:48.817211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "zip_filename = \"spa-eng.zip\"\n",
    "zip_url = \"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "\n",
    "urllib.request.urlretrieve(zip_url, zip_filename)\n",
    "\n",
    "with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(os.path.dirname(zip_filename))\n",
    "\n",
    "print(\"데이터 다운로드 완료\")"
   ],
   "id": "c14f5fd6eb58be97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 다운로드 완료\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:50.121299Z",
     "start_time": "2025-09-10T01:50:50.050499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "extracted_folder = \"./spa-eng\"\n",
    "file_path = os.path.join(extracted_folder, \"spa.txt\")\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    spa_eng_sentences = f.read().splitlines()\n",
    "\n",
    "spa_eng_sentences = list(set(spa_eng_sentences))\n",
    "total_sentence_count = len(spa_eng_sentences)\n",
    "print(\"Example:\", total_sentence_count)\n",
    "\n",
    "for sen in spa_eng_sentences[0:100][::20]:\n",
    "    print(\">>\", sen)\n",
    "\n",
    "print(\"중복 데이터 제거\")"
   ],
   "id": "87efdac2c309a398",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: 118964\n",
      ">> Tell me about your son.\tCuéntame acerca de tu hijo.\n",
      ">> Finish it before you go to bed.\tTermínalo antes de ir a dormir.\n",
      ">> Can you please tell me where the nearest church is?\t¿Sería tan amable de decirme dónde está la iglesia más cercana?\n",
      ">> Save me some ice cream.\tDéjame algo de helado.\n",
      ">> Don't blame the mistake on her.\tNo culpes a ella por el error.\n",
      "중복 데이터 제거\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:50.904940Z",
     "start_time": "2025-09-10T01:50:50.744992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Q. 전처리 함수를 만들어 보세요. 아래 기능을 추가해주세요.\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower() # 대문자를 소문자로 변환\n",
    "    sentence = re.sub(r' {2,}', ' ', sentence) # 둘 이상의 공백을 하나의 공백으로 치환\n",
    "    sentence = sentence.strip() # 문자열 양 끝 공백 제거\n",
    "    return sentence\n",
    "\n",
    "spa_eng_sentences = list(map(preprocess_sentence, spa_eng_sentences))"
   ],
   "id": "2759d75defb43186",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:51.701043Z",
     "start_time": "2025-09-10T01:50:51.695478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_sentence_count = total_sentence_count // 200\n",
    "print(\"Test Size: \", test_sentence_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "train_spa_eng_sentences = spa_eng_sentences[:-test_sentence_count]\n",
    "test_spa_eng_sentences = spa_eng_sentences[-test_sentence_count:]\n",
    "print(\"Train Example:\", len(train_spa_eng_sentences))\n",
    "for sen in train_spa_eng_sentences[0:100][::20]:\n",
    "    print(\">>\", sen)\n",
    "print(\"\\n\")\n",
    "print(\"Test Example:\", len(test_spa_eng_sentences))\n",
    "for sen in test_spa_eng_sentences[0:100][::20]:\n",
    "    print(\">>\", sen)"
   ],
   "id": "c2c3636e84bb5e1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Size:  594\n",
      "\n",
      "\n",
      "Train Example: 118370\n",
      ">> tell me about your son.\tcuéntame acerca de tu hijo.\n",
      ">> finish it before you go to bed.\ttermínalo antes de ir a dormir.\n",
      ">> can you please tell me where the nearest church is?\t¿sería tan amable de decirme dónde está la iglesia más cercana?\n",
      ">> save me some ice cream.\tdéjame algo de helado.\n",
      ">> don't blame the mistake on her.\tno culpes a ella por el error.\n",
      "\n",
      "\n",
      "Test Example: 594\n",
      ">> our school is in this village.\tnuestra escuela está en este pueblo.\n",
      ">> i will explain it to him.\tse lo explicaré a él.\n",
      ">> there's got to be a way.\ttiene que haber un modo.\n",
      ">> tom and mary are sitting on the couch.\ttom y maría están sentados en el sofá.\n",
      ">> their concert was a big hit.\tsu concierto fue un gran éxito.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:52.261786Z",
     "start_time": "2025-09-10T01:50:52.258841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_spa_eng_sentences(spa_eng_sentences):\n",
    "    spa_sentences = []\n",
    "    eng_sentences = []\n",
    "    for spa_eng_sentence in tqdm(spa_eng_sentences):\n",
    "        eng_sentence, spa_sentence = spa_eng_sentence.split('\\t')\n",
    "        spa_sentences.append(spa_sentence)\n",
    "        eng_sentences.append(eng_sentence)\n",
    "    return eng_sentences, spa_sentences\n",
    "\n",
    "print('슝=3')"
   ],
   "id": "ced51de928a5cce9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:52.894690Z",
     "start_time": "2025-09-10T01:50:52.773963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#split test & train data\n",
    "train_eng_sentences, train_spa_sentences = split_spa_eng_sentences(train_spa_eng_sentences)\n",
    "print(len(train_eng_sentences))\n",
    "print(train_eng_sentences[0])\n",
    "print('\\n')\n",
    "print(len(train_spa_sentences))\n",
    "print(train_spa_sentences[0])"
   ],
   "id": "ab42cf48a71008b2",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#split test & train data\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m train_eng_sentences, train_spa_sentences \u001B[38;5;241m=\u001B[39m \u001B[43msplit_spa_eng_sentences\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_spa_eng_sentences\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(train_eng_sentences))\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_eng_sentences[\u001B[38;5;241m0\u001B[39m])\n",
      "Cell \u001B[0;32mIn[21], line 4\u001B[0m, in \u001B[0;36msplit_spa_eng_sentences\u001B[0;34m(spa_eng_sentences)\u001B[0m\n\u001B[1;32m      2\u001B[0m spa_sentences \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      3\u001B[0m eng_sentences \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m spa_eng_sentence \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspa_eng_sentences\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m      5\u001B[0m     eng_sentence, spa_sentence \u001B[38;5;241m=\u001B[39m spa_eng_sentence\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m     spa_sentences\u001B[38;5;241m.\u001B[39mappend(spa_sentence)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages/tqdm/notebook.py:234\u001B[0m, in \u001B[0;36mtqdm_notebook.__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    232\u001B[0m unit_scale \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munit_scale \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munit_scale \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    233\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal \u001B[38;5;241m*\u001B[39m unit_scale \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal\n\u001B[0;32m--> 234\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_printer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mncols\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontainer\u001B[38;5;241m.\u001B[39mpbar \u001B[38;5;241m=\u001B[39m proxy(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplayed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages/tqdm/notebook.py:108\u001B[0m, in \u001B[0;36mtqdm_notebook.status_printer\u001B[0;34m(_, total, desc, ncols)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# Fallback to text bar if there's no total\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m# if not total:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    105\u001B[0m \n\u001B[1;32m    106\u001B[0m \u001B[38;5;66;03m# Prepare IPython progress bar\u001B[39;00m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m IProgress \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# #187 #451 #558 #872\u001B[39;00m\n\u001B[0;32m--> 108\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(WARN_NOIPYW)\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m total:\n\u001B[1;32m    110\u001B[0m     pbar \u001B[38;5;241m=\u001B[39m IProgress(\u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mmax\u001B[39m\u001B[38;5;241m=\u001B[39mtotal)\n",
      "\u001B[0;31mImportError\u001B[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:54.124939Z",
     "start_time": "2025-09-10T01:50:54.097149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_eng_sentences, test_spa_sentences = split_spa_eng_sentences(test_spa_eng_sentences)\n",
    "print(len(test_eng_sentences))\n",
    "print(test_eng_sentences[0])\n",
    "print('\\n')\n",
    "print(len(test_spa_sentences))\n",
    "print(test_spa_sentences[0])"
   ],
   "id": "92bf1860efa3cbe7",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m test_eng_sentences, test_spa_sentences \u001B[38;5;241m=\u001B[39m \u001B[43msplit_spa_eng_sentences\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_spa_eng_sentences\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(test_eng_sentences))\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(test_eng_sentences[\u001B[38;5;241m0\u001B[39m])\n",
      "Cell \u001B[0;32mIn[21], line 4\u001B[0m, in \u001B[0;36msplit_spa_eng_sentences\u001B[0;34m(spa_eng_sentences)\u001B[0m\n\u001B[1;32m      2\u001B[0m spa_sentences \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      3\u001B[0m eng_sentences \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m spa_eng_sentence \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspa_eng_sentences\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m      5\u001B[0m     eng_sentence, spa_sentence \u001B[38;5;241m=\u001B[39m spa_eng_sentence\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m     spa_sentences\u001B[38;5;241m.\u001B[39mappend(spa_sentence)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages/tqdm/notebook.py:234\u001B[0m, in \u001B[0;36mtqdm_notebook.__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    232\u001B[0m unit_scale \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munit_scale \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munit_scale \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    233\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal \u001B[38;5;241m*\u001B[39m unit_scale \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal\n\u001B[0;32m--> 234\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstatus_printer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mncols\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontainer\u001B[38;5;241m.\u001B[39mpbar \u001B[38;5;241m=\u001B[39m proxy(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplayed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages/tqdm/notebook.py:108\u001B[0m, in \u001B[0;36mtqdm_notebook.status_printer\u001B[0;34m(_, total, desc, ncols)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# Fallback to text bar if there's no total\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m# if not total:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    105\u001B[0m \n\u001B[1;32m    106\u001B[0m \u001B[38;5;66;03m# Prepare IPython progress bar\u001B[39;00m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m IProgress \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# #187 #451 #558 #872\u001B[39;00m\n\u001B[0;32m--> 108\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(WARN_NOIPYW)\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m total:\n\u001B[1;32m    110\u001B[0m     pbar \u001B[38;5;241m=\u001B[39m IProgress(\u001B[38;5;28mmin\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mmax\u001B[39m\u001B[38;5;241m=\u001B[39mtotal)\n",
      "\u001B[0;31mImportError\u001B[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:09:41.495982Z",
     "start_time": "2025-09-10T02:09:41.471881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_tokenizer(corpus,\n",
    "                       vocab_size,\n",
    "                       lang=\"spa-eng\",\n",
    "                       pad_id=0,   # pad token의 일련번호\n",
    "                       bos_id=1,  # 문장의 시작을 의미하는 bos token(<s>)의 일련번호\n",
    "                       eos_id=2,  # 문장의 끝을 의미하는 eos token(</s>)의 일련번호\n",
    "                       unk_id=3):   # unk token의 일련번호\n",
    "    file = \"./%s_corpus.txt\" % lang\n",
    "    model = \"%s_spm\" % lang\n",
    "\n",
    "    with open(file, 'w') as f:\n",
    "        for row in corpus: f.write(str(row) + '\\n')\n",
    "\n",
    "    import sentencepiece as spm\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n",
    "        % (file, model, vocab_size) + \\\n",
    "        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n",
    "        % (pad_id, bos_id, eos_id, unk_id)\n",
    "    )\n",
    "\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.Load('%s.model' % model)\n",
    "\n",
    "    return tokenizer\n"
   ],
   "id": "a0140e3b4303e49",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:09:42.556689Z",
     "start_time": "2025-09-10T02:09:42.501500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "VOCAB_SIZE = 20000\n",
    "tokenizer = generate_tokenizer(train_eng_sentences + train_spa_sentences, VOCAB_SIZE, 'spa-eng')\n",
    "tokenizer.set_encode_extra_options(\"bos:eos\")  # 문장 양 끝에 <s> , </s> 추가"
   ],
   "id": "f80a7838c4a7dbda",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_eng_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[63], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m VOCAB_SIZE \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20000\u001B[39m\n\u001B[0;32m----> 2\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m generate_tokenizer(\u001B[43mtrain_eng_sentences\u001B[49m \u001B[38;5;241m+\u001B[39m train_spa_sentences, VOCAB_SIZE, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspa-eng\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mset_encode_extra_options(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbos:eos\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# 문장 양 끝에 <s> , </s> 추가\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_eng_sentences' is not defined"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:57.136288Z",
     "start_time": "2025-09-10T01:50:57.114715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_corpus(sentences, tokenizer):\n",
    "    corpus = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokens = tokenizer.encode_as_ids(sentence)\n",
    "        corpus.append(tokens)\n",
    "    return corpus\n",
    "\n",
    "eng_corpus = make_corpus(train_eng_sentences, tokenizer)\n",
    "spa_corpus = make_corpus(train_spa_sentences, tokenizer)\n",
    "\n",
    "print(train_eng_sentences[0])\n",
    "print(eng_corpus[0])\n",
    "print('\\n')\n",
    "print(train_spa_sentences[0])\n",
    "print(spa_corpus[0])"
   ],
   "id": "318771efac142cbc",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_eng_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m         corpus\u001B[38;5;241m.\u001B[39mappend(tokens)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m corpus\n\u001B[0;32m----> 8\u001B[0m eng_corpus \u001B[38;5;241m=\u001B[39m make_corpus(\u001B[43mtrain_eng_sentences\u001B[49m, tokenizer)\n\u001B[1;32m      9\u001B[0m spa_corpus \u001B[38;5;241m=\u001B[39m make_corpus(train_spa_sentences, tokenizer)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_eng_sentences[\u001B[38;5;241m0\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_eng_sentences' is not defined"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:57.659760Z",
     "start_time": "2025-09-10T01:50:57.635715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_LEN = 50\n",
    "\n",
    "def pad_sequences_custom(sequences, max_len=50, pad_value=0):\n",
    "    \"\"\"\n",
    "    sequences: list of list (각 문장별 토큰 ID 리스트)\n",
    "    max_len: 고정할 최대 시퀀스 길이\n",
    "    pad_value: 패딩에 사용할 값 (일반적으로 0)\n",
    "    \"\"\"\n",
    "    padded_sequences = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        # 초과 길이는 자르고\n",
    "        if len(seq) > max_len:\n",
    "            seq = seq[:max_len]\n",
    "        # 부족한 길이는 pad_value로 채우기\n",
    "        else:\n",
    "            seq = seq + [pad_value] * (max_len - len(seq))\n",
    "\n",
    "        padded_sequences.append(seq)\n",
    "\n",
    "    # 최종적으로 torch.Tensor로 변환 (shape: [batch_size, max_len])\n",
    "    return torch.tensor(padded_sequences, dtype=torch.long)\n",
    "\n",
    "enc_ndarray = pad_sequences_custom(eng_corpus, max_len=MAX_LEN, pad_value=0)\n",
    "dec_ndarray = pad_sequences_custom(spa_corpus, max_len=MAX_LEN, pad_value=0)\n",
    "\n",
    "print(enc_ndarray.shape)  # 예) [batch_size, 50]\n",
    "print(dec_ndarray.shape)  # 예) [batch_size, 50]\n"
   ],
   "id": "16c1c6aabed0fb32",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eng_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 24\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;66;03m# 최종적으로 torch.Tensor로 변환 (shape: [batch_size, max_len])\u001B[39;00m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mtensor(padded_sequences, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[0;32m---> 24\u001B[0m enc_ndarray \u001B[38;5;241m=\u001B[39m pad_sequences_custom(\u001B[43meng_corpus\u001B[49m, max_len\u001B[38;5;241m=\u001B[39mMAX_LEN, pad_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     25\u001B[0m dec_ndarray \u001B[38;5;241m=\u001B[39m pad_sequences_custom(spa_corpus, max_len\u001B[38;5;241m=\u001B[39mMAX_LEN, pad_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28mprint\u001B[39m(enc_ndarray\u001B[38;5;241m.\u001B[39mshape)  \u001B[38;5;66;03m# 예) [batch_size, 50]\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'eng_corpus' is not defined"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:58.051400Z",
     "start_time": "2025-09-10T01:50:58.032573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataset = TensorDataset(enc_ndarray, dec_ndarray)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "id": "10e410747ff34dc9",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_ndarray' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m BATCH_SIZE \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m64\u001B[39m\n\u001B[1;32m      4\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m TensorDataset(\u001B[43menc_ndarray\u001B[49m, dec_ndarray)\n\u001B[1;32m      6\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(train_dataset, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, pin_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m슝=3\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'enc_ndarray' is not defined"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:58.413815Z",
     "start_time": "2025-09-10T01:50:58.401629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "\n",
    "def generate_padding_mask(seq: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    seq: shape [batch_size, seq_len]의 입력 (토큰 ID 텐서)\n",
    "    반환: shape [batch_size, 1, 1, seq_len]의 패딩 마스크\n",
    "         (seq == 0)인 위치가 1, 나머지는 0\n",
    "    \"\"\"\n",
    "    # (seq == 0)은 불리언 텐서를 반환 -> float()로 형변환 -> (1.0 or 0.0)\n",
    "    # 차원 확장: [batch_size, seq_len] → [batch_size, 1, 1, seq_len]\n",
    "    return (seq == 0).unsqueeze(1).unsqueeze(2).float()\n",
    "\n",
    "\n",
    "def generate_lookahead_mask(size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    size: 문장(시퀀스) 길이\n",
    "    반환: shape [size, size],\n",
    "         i < j (대각선 위)에 해당하는 위치가 1, 아닌 곳은 0\n",
    "         (미래 토큰을 가리기 위한 마스크)\n",
    "    \"\"\"\n",
    "    # triu(diagonal=1)은 주대각선 위가 1, 아래가 0인 텐서를 만들어 줌\n",
    "    return torch.triu(torch.ones(size, size), diagonal=1)\n",
    "\n",
    "\n",
    "def generate_masks(src: torch.Tensor, tgt: torch.Tensor):\n",
    "    \"\"\"\n",
    "    src, tgt: shape [batch_size, seq_len]\n",
    "    3가지 마스크를 반환:\n",
    "      - enc_mask: 인코더 입력용 패딩 마스크\n",
    "      - dec_enc_mask: 디코더-인코더 어텐션용 패딩 마스크\n",
    "      - dec_mask: 디코더 자기어텐션용 마스크(룩어헤드 + 패딩)\n",
    "\n",
    "    각각의 shape:\n",
    "      - enc_mask, dec_enc_mask: [batch_size, 1, 1, src_seq_len]\n",
    "      - dec_mask: [batch_size, 1, tgt_seq_len, tgt_seq_len]\n",
    "    \"\"\"\n",
    "    # 1) 인코더 입력용 패딩 마스크\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    # 2) 디코더에서 인코더 값을 볼 때 사용하는 마스크 (src 마스크 재사용)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    # 3) 디코더 자기어텐션 마스크 (미래 토큰 방지 룩어헤드 + tgt 자체 패딩 마스크)\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])  # [tgt_seq_len, tgt_seq_len]\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)           # [batch_size, 1, 1, tgt_seq_len]\n",
    "\n",
    "    # 룩어헤드 마스크를 (batch 차원과 head 차원을 가상으로) 확장\n",
    "    dec_lookahead_mask = dec_lookahead_mask.unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]\n",
    "\n",
    "    # 패딩 + 룩어헤드 마스크 병합\n",
    "    # 브로드캐스팅에 의해 shape [batch_size, 1, tgt_seq_len, tgt_seq_len]이 됨\n",
    "\n",
    "    dec_tgt_padding_mask = dec_tgt_padding_mask.to(device)\n",
    "    dec_lookahead_mask = dec_lookahead_mask.to(device)\n",
    "\n",
    "    dec_mask = torch.max(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n"
   ],
   "id": "ca69d37b32d3c4cb",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:58.706801Z",
     "start_time": "2025-09-10T01:50:58.699208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # d_model을 num_heads로 나눈 만큼이 각 head가 담당할 차원 수\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # Query, Key, Value를 구하는 선형 레이어\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 최종적으로 head들의 출력을 결합해주는 선형 레이어\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Q, K, V:  [batch_size, num_heads, seq_len, depth]\n",
    "        mask:     [batch_size, 1, seq_len, seq_len] 혹은\n",
    "                  [batch_size, num_heads, seq_len, seq_len]\n",
    "                  (어텐션에서 제외할 위치=1, 사용할 위치=0)\n",
    "        \"\"\"\n",
    "        # d_k = depth\n",
    "        d_k = Q.size(-1)  # K.shape[-1]도 동일\n",
    "        # Q와 K의 전치 곱: (batch_size, num_heads, seq_len, seq_len)\n",
    "        QK = torch.matmul(Q, K.transpose(-1, -2))\n",
    "\n",
    "        # 스케일링\n",
    "        scaled_qk = QK / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "        # 마스크가 있는 경우 -1e9(매우 작은 수)를 더하여 softmax 후 확률이 0에 가깝도록 처리\n",
    "        if mask is not None:\n",
    "            scaled_qk = scaled_qk + (mask * -1e9)\n",
    "\n",
    "        attentions = F.softmax(scaled_qk, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        out = torch.matmul(attentions, V)         # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, d_model]\n",
    "        반환: [batch_size, num_heads, seq_len, depth]\n",
    "        \"\"\"\n",
    "        bsz, seq_len, _ = x.size()\n",
    "        # d_model -> (num_heads * depth)이므로 view로 재배치\n",
    "        x = x.view(bsz, seq_len, self.num_heads, self.depth)\n",
    "        # (batch_size, seq_len, num_heads, depth) -> (batch_size, num_heads, seq_len, depth)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        return x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, num_heads, seq_len, depth]\n",
    "        반환: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        bsz, num_heads, seq_len, depth = x.size()\n",
    "        # (batch_size, num_heads, seq_len, depth) -> (batch_size, seq_len, num_heads, depth)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(bsz, seq_len, self.d_model)\n",
    "        return x\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Q, K, V: [batch_size, seq_len, d_model]\n",
    "        mask:    [batch_size, 1, seq_len, seq_len] 혹은\n",
    "                 [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # W_q, W_k, W_v는 각각 (d_model -> d_model) 선형 변환\n",
    "        WQ = self.W_q(Q)  # [batch_size, seq_len, d_model]\n",
    "        WK = self.W_k(K)  # [batch_size, seq_len, d_model]\n",
    "        WV = self.W_v(V)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # 멀티헤드 분할\n",
    "        WQ_splits = self.split_heads(WQ)  # [batch_size, num_heads, seq_len, depth]\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask\n",
    "        )\n",
    "\n",
    "        # head 결과 결합 후 최종 선형\n",
    "        out = self.combine_heads(out)  # [batch_size, seq_len, d_model]\n",
    "        out = self.linear(out)         # [batch_size, seq_len, d_model]\n",
    "\n",
    "        return out, attention_weights\n",
    "\n"
   ],
   "id": "b1f2a9f64beec2b2",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:59.140679Z",
     "start_time": "2025-09-10T01:50:59.137007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))  # 첫 번째 Dense + ReLU\n",
    "        out = self.fc2(out)          # 두 번째 Dense\n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "id": "d37798b20136820d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:50:59.499804Z",
     "start_time": "2025-09-10T01:50:59.494921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        # nn.LayerNorm은 마지막 차원(d_model)을 기준으로 정규화\n",
    "        self.norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multi-Head Attention 단계\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out = out + residual  # residual connection\n",
    "\n",
    "        # Position-Wise Feed Forward 단계\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out = out + residual  # residual connection\n",
    "\n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "id": "143f9181a9661e0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:55:39.053517Z",
     "start_time": "2025-09-10T01:55:39.042903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        # Masked Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, mask=padding_mask)\n",
    "        out = self.do(out)\n",
    "        out = out + residual\n",
    "\n",
    "        # Encoder-Decoder Multi-Head Attention (주의: Q, K, V 순서)\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out = out + residual\n",
    "\n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out = out + residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "id": "f44c1a60a4109bb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:55:35.893830Z",
     "start_time": "2025-09-10T01:55:35.886704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.do = nn.Dropout(dropout)  # 필요 시 입력에 dropout 적용 가능\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        out = x\n",
    "        enc_attns = []\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        return out, enc_attns\n",
    "\n",
    "# 사용 예시: Encoder 인스턴스 생성 후 forward 호출\n",
    "# encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "# out, enc_attns = encoder(x, mask)\n",
    "print(\"슝=3\")"
   ],
   "id": "9dde3e3b1d6b4971",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:51:00.535609Z",
     "start_time": "2025-09-10T01:51:00.528600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "        dec_attns = []\n",
    "        dec_enc_attns = []\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "id": "47d6f047486e10c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:51:00.957124Z",
     "start_time": "2025-09-10T01:51:00.949214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff,\n",
    "                 src_vocab_size, tgt_vocab_size, pos_len,\n",
    "                 dropout=0.2, shared_fc=True, shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        # d_model은 스케일링에 사용되므로 float으로 저장\n",
    "        self.d_model = float(d_model)\n",
    "\n",
    "        # Embedding 레이어: shared_emb True면 동일한 임베딩을 사용합니다.\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding (넘파이 버전 결과를 torch.Tensor로 변환)\n",
    "        pos_encoding_np = positional_encoding(pos_len, d_model)\n",
    "        # 파라미터로 등록하지 않고 고정값이므로 buffer로 등록합니다.\n",
    "        self.register_buffer(\"pos_encoding\", torch.tensor(pos_encoding_np, dtype=torch.float32))\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "        if shared_fc:\n",
    "            # fc 레이어와 디코더 임베딩의 weight를 공유합니다.\n",
    "            self.fc.weight = self.dec_emb.weight\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        emb: 임베딩 레이어\n",
    "        x: [batch_size, seq_len] (토큰 인덱스)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        out = emb(x)  # [batch_size, seq_len, d_model]\n",
    "        if self.shared_fc:\n",
    "            out = out * math.sqrt(self.d_model)\n",
    "        # pos_encoding: [pos_len, d_model] → [1, pos_len, d_model] 후 슬라이싱\n",
    "        out = out + self.pos_encoding[:seq_len, :].unsqueeze(0)\n",
    "        out = self.do(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        \"\"\"\n",
    "        enc_in: [batch_size, src_seq_len]\n",
    "        dec_in: [batch_size, tgt_seq_len]\n",
    "        enc_mask, dec_enc_mask, dec_mask: 마스킹 텐서들\n",
    "        \"\"\"\n",
    "        # Embedding 및 positional encoding 적용\n",
    "        enc_in_emb = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in_emb = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        # Encoder와 Decoder 통과\n",
    "        enc_out, enc_attns = self.encoder(enc_in_emb, enc_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in_emb, enc_out, dec_enc_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "id": "fe2cb387cdfd3db7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:51:52.124720Z",
     "start_time": "2025-09-10T01:51:51.799918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "d_model = 512\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "id": "9ad8c0fc0625c022",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:51:52.329752Z",
     "start_time": "2025-09-10T01:51:52.324946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LearningRateScheduler:\n",
    "    def __init__(self, d_model, warmup_steps=60): # 4000\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # step을 float으로 변환하여 지수 연산이 제대로 수행되도록 함\n",
    "        step = float(step)\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * min(arg1, arg2)\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "id": "d112b6a16c2091a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:51:55.147790Z",
     "start_time": "2025-09-10T01:51:54.418890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Learning Rate 인스턴스 선언\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "# 초기 lr은 스텝 1에 해당하는 값으로 설정합니다.\n",
    "optimizer = torch.optim.Adam(transformer.parameters(),\n",
    "                             lr=learning_rate(1),\n",
    "                             betas=(0.9, 0.98),\n",
    "                             eps=1e-9)\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "id": "9791ae720659a235",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x11183f820>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages/tqdm/notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n",
      "Exception ignored in: <function tqdm.__del__ at 0x11183f820>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages/tqdm/std.py\", line 1148, in __del__\n",
      "    self.close()\n",
      "  File \"/opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages/tqdm/notebook.py\", line 279, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm_notebook' object has no attribute 'disp'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:51:56.974723Z",
     "start_time": "2025-09-10T01:51:56.968494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real: [batch_size, seq_len] (정답 토큰 인덱스)\n",
    "    pred: [batch_size, seq_len, num_classes] (모델의 raw logits)\n",
    "    \"\"\"\n",
    "\n",
    "    real = real.to(device)\n",
    "    pred = pred.to(device)\n",
    "\n",
    "    # 예측 값을 (N, C) 형태로 flatten하고, 정답도 flatten하여 개별 손실 값을 구함\n",
    "    loss_ = F.cross_entropy(pred.contiguous().view(-1, pred.size(-1)), real.contiguous().view(-1), reduction='none')\n",
    "    # 다시 (batch_size, seq_len)로 reshape\n",
    "    loss_ = loss_.view(real.size())\n",
    "\n",
    "    # real이 0이 아닌 위치에 대한 마스크 생성 (0이면 패딩 토큰)\n",
    "    mask = (real != 0).float()\n",
    "    loss_ = loss_ * mask\n",
    "\n",
    "    # 전체 손실 합을 마스크 합으로 나누어 평균 손실 계산\n",
    "    return loss_.sum() / mask.sum()\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "id": "818908c0fd03dda4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:51:57.717932Z",
     "start_time": "2025-09-10T01:51:57.713844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_step(src, tgt, model, optimizer):\n",
    "    model.train()  # 모델을 training 모드로 전환\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # tgt의 오른쪽 시프트: decoder input과 gold target 분리\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 입력\n",
    "    gold = tgt[:, 1:]     # Decoder의 정답(target)\n",
    "\n",
    "    # 마스크 생성 (generate_masks는 PyTorch용으로 변환된 함수여야 합니다)\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    src = src.to(device)\n",
    "    tgt_in = tgt_in.to(device)\n",
    "    enc_mask = enc_mask.to(device)\n",
    "    dec_enc_mask = dec_enc_mask.to(device)\n",
    "    dec_mask = dec_mask.to(device)\n",
    "\n",
    "    # 모델 forward pass\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "\n",
    "    # loss 계산\n",
    "    loss = loss_function(gold, predictions)\n",
    "\n",
    "    # 역전파 수행 및 파라미터 업데이트\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "id": "710a5e920d21c9ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:43:19.769387Z",
     "start_time": "2025-09-09T06:17:56.240735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "torch.set_default_device(\"cpu\")\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    dataset_count = len(train_dataloader)  # train_loader는 PyTorch DataLoader입니다.\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "\n",
    "    for batch, (src, tgt) in enumerate(train_dataloader):\n",
    "        # train_step 함수는 (loss, enc_attns, dec_attns, dec_enc_attns)를 반환합니다.\n",
    "        loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "\n",
    "        total_loss += loss.item()  # PyTorch에서는 loss.numpy() 대신 loss.item() 사용\n",
    "        tqdm_bar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "        tqdm_bar.update(1)\n",
    "\n",
    "    tqdm_bar.close()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / dataset_count:.4f}\")"
   ],
   "id": "1272e7e740e16524",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1850 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fa85cf5fe114fb984b57d837bf58e4b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3993.1490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1850 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "657aa1456ca54ee8bd04c6a1cf9acf34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2609.5598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/1850 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5335ae68b86c411fbf785e76a7761be1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 2089.9432\n",
      "CPU times: user 1h 59min 22s, sys: 51min 43s, total: 2h 51min 6s\n",
      "Wall time: 1h 25min 23s\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:43:29.629931Z",
     "start_time": "2025-09-09T07:43:29.288930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model 저장\n",
    "torch.save(transformer.state_dict(), 'model_weights.pth')"
   ],
   "id": "8e664d855ca29edd",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:03:54.404062Z",
     "start_time": "2025-09-10T02:03:53.657635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 저장할 때 (이미 하셨던 것)\n",
    "torch.save(transformer.state_dict(), \"model_weights.pth\")\n",
    "\n",
    "# 2. 불러올 때\n",
    "# 동일한 구조로 모델을 다시 생성\n",
    "loaded_model = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True\n",
    ").to(device)\n",
    "\n",
    "# 저장된 state_dict 로드\n",
    "state_dict = torch.load(\"model_weights.pth\", map_location=device)\n",
    "loaded_model.load_state_dict(state_dict)\n",
    "\n",
    "# 추론 모드로 전환\n",
    "loaded_model.eval()"
   ],
   "id": "6eeec5f1de3cd87f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (enc_emb): Embedding(20000, 512)\n",
       "  (dec_emb): Embedding(20000, 512)\n",
       "  (do): Dropout(p=0.3, inplace=False)\n",
       "  (encoder): Encoder(\n",
       "    (enc_layers): ModuleList(\n",
       "      (0-1): 2 x EncoderLayer(\n",
       "        (enc_self_attn): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PoswiseFeedForwardNet(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (do): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (do): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dec_layers): ModuleList(\n",
       "      (0-1): 2 x DecoderLayer(\n",
       "        (dec_self_attn): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (enc_dec_attn): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ffn): PoswiseFeedForwardNet(\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (norm_3): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
       "        (do): Dropout(p=0.3, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=20000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:03:57.520550Z",
     "start_time": "2025-09-10T02:03:57.512555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "# 아래 두 문장을 바꿔가며 테스트 해보세요\n",
    "reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n",
    "candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n",
    "\n",
    "print(\"원문:\", reference)\n",
    "print(\"번역문:\", candidate)\n",
    "print(\"BLEU Score:\", sentence_bleu([reference], candidate))"
   ],
   "id": "a374484582d05adc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n",
      "번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n",
      "BLEU Score: 8.190757052088229e-155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:03:59.284073Z",
     "start_time": "2025-09-10T02:03:59.280770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"
   ],
   "id": "237f4881ee9b73b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 0.5\n",
      "2-gram: 0.18181818181818182\n",
      "3-gram: 2.2250738585072626e-308\n",
      "4-gram: 2.2250738585072626e-308\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:04:00.666955Z",
     "start_time": "2025-09-10T02:04:00.661722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n",
    "    return sentence_bleu([reference],\n",
    "                         candidate,\n",
    "                         weights=weights,\n",
    "                         smoothing_function=SmoothingFunction().method1)  # smoothing_function 적용\n",
    "\n",
    "print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n",
    "print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n",
    "print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n",
    "print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n",
    "\n",
    "print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))"
   ],
   "id": "8b5b5b952709aec4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.5\n",
      "BLEU-2: 0.18181818181818182\n",
      "BLEU-3: 0.010000000000000005\n",
      "BLEU-4: 0.011111111111111112\n",
      "\n",
      "BLEU-Total: 0.05637560315259291\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:04:01.965317Z",
     "start_time": "2025-09-10T02:04:01.959394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def translate(tokens, model, src_tokenizer, tgt_tokenizer) :\n",
    "    # tokens: 입력 토큰 리스트\n",
    "    # MAX_LEN: 최대 길이 (전역 변수 혹은 상수)\n",
    "    # device: 모델과 데이터가 위치한 디바이스\n",
    "\n",
    "    if len(tokens) > MAX_LEN :\n",
    "        tokens = tokens[:MAX_LEN]\n",
    "    else :\n",
    "        tokens = tokens + [0] * (MAX_LEN - len(tokens))\n",
    "\n",
    "    padded_tokens = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "\n",
    "    ids = []\n",
    "\n",
    "    output = torch.tensor([[tgt_tokenizer.bos_id()]], dtype=torch.long, device=device)\n",
    "\n",
    "    for i in range(MAX_LEN) :\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask, _ = generate_masks(padded_tokens, output)\n",
    "\n",
    "        predictions, _, _, _ = model(padded_tokens, output, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "\n",
    "        predicted_id = predictions[0, -1].softmax(dim=-1).argmax(dim=-1).item()\n",
    "\n",
    "        if tgt_tokenizer.eos_id() == predicted_id :\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            return result\n",
    "\n",
    "        ids.append(predicted_id)\n",
    "\n",
    "        new_token = torch.tensor([[predicted_id]], dtype=torch.long, device=device)\n",
    "        output = torch.cat([output, new_token], dim=1)\n",
    "\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "    return result\n"
   ],
   "id": "e468cdcb5e3a5303",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:43:49.975657Z",
     "start_time": "2025-09-09T07:43:49.970116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    src_tokens = src_tokenizer.encode_as_ids(src_sentence)\n",
    "    tgt_tokens = tgt_tokenizer.encode_as_ids(tgt_sentence)\n",
    "\n",
    "    if (len(src_tokens) > MAX_LEN): return None\n",
    "    if (len(tgt_tokens) > MAX_LEN): return None\n",
    "\n",
    "    reference = tgt_sentence.split()\n",
    "    candidate = translate(src_tokens, model, src_tokenizer, tgt_tokenizer).split()\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate)\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "\n",
    "    return score\n",
    "\n",
    "print('슝=3')"
   ],
   "id": "d3552f2c019e7a4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T07:45:16.209919Z",
     "start_time": "2025-09-09T07:45:15.975268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Q. 인덱스를 바꿔가며 테스트해 보세요\n",
    "test_idx = 0\n",
    "for test_idx in range(1,15,3) :\n",
    "    eval_bleu_single(transformer,\n",
    "                     test_eng_sentences[test_idx],\n",
    "                     test_spa_sentences[test_idx],\n",
    "                     tokenizer,\n",
    "                     tokenizer)"
   ],
   "id": "5a3c259445f22922",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[43], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m test_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m test_idx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m15\u001B[39m,\u001B[38;5;241m3\u001B[39m) :\n\u001B[0;32m----> 4\u001B[0m     \u001B[43meval_bleu_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mtest_eng_sentences\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtest_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mtest_spa_sentences\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtest_idx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m                     \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[39], line 9\u001B[0m, in \u001B[0;36meval_bleu_single\u001B[0;34m(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mlen\u001B[39m(tgt_tokens) \u001B[38;5;241m>\u001B[39m MAX_LEN): \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m      8\u001B[0m reference \u001B[38;5;241m=\u001B[39m tgt_sentence\u001B[38;5;241m.\u001B[39msplit()\n\u001B[0;32m----> 9\u001B[0m candidate \u001B[38;5;241m=\u001B[39m \u001B[43mtranslate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_tokens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_tokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_tokenizer\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msplit()\n\u001B[1;32m     11\u001B[0m score \u001B[38;5;241m=\u001B[39m sentence_bleu([reference], candidate,\n\u001B[1;32m     12\u001B[0m                       smoothing_function\u001B[38;5;241m=\u001B[39mSmoothingFunction()\u001B[38;5;241m.\u001B[39mmethod1)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n",
      "Cell \u001B[0;32mIn[38], line 21\u001B[0m, in \u001B[0;36mtranslate\u001B[0;34m(tokens, model, src_tokenizer, tgt_tokenizer)\u001B[0m\n\u001B[1;32m     18\u001B[0m output \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([[tgt_tokenizer\u001B[38;5;241m.\u001B[39mbos_id()]], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(MAX_LEN) :\n\u001B[0;32m---> 21\u001B[0m     enc_padding_mask, combined_mask, dec_padding_mask, _ \u001B[38;5;241m=\u001B[39m generate_masks(padded_tokens, output)\n\u001B[1;32m     23\u001B[0m     predictions, _, _, _ \u001B[38;5;241m=\u001B[39m model(padded_tokens, output, enc_padding_mask, combined_mask, dec_padding_mask)\n\u001B[1;32m     25\u001B[0m     predicted_id \u001B[38;5;241m=\u001B[39m predictions[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39msoftmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mitem()\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(src_sentences)\n",
    "\n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx], src_tokenizer, tgt_tokenizer, verbose)\n",
    "        if not score: continue\n",
    "\n",
    "        total_score += score\n",
    "\n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)\n",
    "\n",
    "eval_bleu(transformer, test_eng_sentences, test_spa_sentences, tokenizer, tokenizer, verbose=False)\n"
   ],
   "id": "46ea3a28aee6fe33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def beam_search_decoder(prob, beam_size):\n",
    "    sequences = [[[], 1.0]]  # 생성된 문장과 점수를 저장\n",
    "\n",
    "    for tok in prob:\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in sequences:\n",
    "            for idx, p in enumerate(tok): # 각 단어의 확률을 총점에 누적 곱\n",
    "                candidate = [seq + [idx], score * -math.log(-(p-1))]\n",
    "                all_candidates.append(candidate)\n",
    "\n",
    "        ordered = sorted(all_candidates,\n",
    "                         key=lambda tup:tup[1],\n",
    "                         reverse=True) # 총점 순 정렬\n",
    "        sequences = ordered[:beam_size] # Beam Size에 해당하는 문장만 저장\n",
    "\n",
    "    return sequences\n",
    "\n",
    "print(\"슝=3\")"
   ],
   "id": "d158bb67dfa713a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vocab = {\n",
    "    0: \"<pad>\",\n",
    "    1: \"까요?\",\n",
    "    2: \"커피\",\n",
    "    3: \"마셔\",\n",
    "    4: \"가져\",\n",
    "    5: \"될\",\n",
    "    6: \"를\",\n",
    "    7: \"한\",\n",
    "    8: \"잔\",\n",
    "    9: \"도\",\n",
    "}\n",
    "\n",
    "prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n",
    "            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n",
    "            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n",
    "            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n",
    "            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n",
    "\n",
    "prob_seq = np.array(prob_seq)\n",
    "beam_size = 3\n",
    "\n",
    "result = beam_search_decoder(prob_seq, beam_size)\n",
    "\n",
    "for seq, score in result:\n",
    "    sentence = \"\"\n",
    "\n",
    "    for word in seq:\n",
    "        sentence += vocab[word] + \" \"\n",
    "\n",
    "    print(sentence, \"// Score: %.4f\" % score)"
   ],
   "id": "e0bae4e2a7f66639"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T08:07:08.636460Z",
     "start_time": "2025-09-09T08:06:48.127439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install wget\n",
    "!pip install fasttext"
   ],
   "id": "4e5f0f9aa581fa72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\r\n",
      "  Downloading wget-3.2.zip (10 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hBuilding wheels for collected packages: wget\r\n",
      "\u001B[33m  DEPRECATION: Building 'wget' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wget'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001B[0m\u001B[33m\r\n",
      "\u001B[0m  Building wheel for wget (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=b8b783cc16fbba2e25d97a00b6c66310c348904291a112d774f15b3fffbfdcef\r\n",
      "  Stored in directory: /Users/park_jinyong/Library/Caches/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\r\n",
      "Successfully built wget\r\n",
      "Installing collected packages: wget\r\n",
      "Successfully installed wget-3.2\r\n",
      "Collecting fasttext\r\n",
      "  Downloading fasttext-0.9.3.tar.gz (73 kB)\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting pybind11>=2.2 (from fasttext)\r\n",
      "  Using cached pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\r\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from fasttext) (65.6.3)\r\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/lib/python3.10/site-packages (from fasttext) (1.23.5)\r\n",
      "Using cached pybind11-3.0.1-py3-none-any.whl (293 kB)\r\n",
      "Building wheels for collected packages: fasttext\r\n",
      "  Building wheel for fasttext (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-macosx_15_0_arm64.whl size=297214 sha256=a5996ea5f6b9d0a8d50cf4fb8cba005537d51180b2addf1a95a59d7905d240e0\r\n",
      "  Stored in directory: /Users/park_jinyong/Library/Caches/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\r\n",
      "Successfully built fasttext\r\n",
      "Installing collected packages: pybind11, fasttext\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2/2\u001B[0m [fasttext]\r\n",
      "\u001B[1A\u001B[2KSuccessfully installed fasttext-0.9.3 pybind11-3.0.1\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T00:50:52.111601Z",
     "start_time": "2025-09-10T00:47:24.807799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!rm ko.tar.gz\n",
    "!curl -L \"https://www.dropbox.com/scl/fi/uuhouaynepr9evqfohos3/ko.tar.gz?rlkey=rhie0pp7wmib4sj1xn4twbkw2&e=2&dl=1\" -o ko.tar.gz\n",
    "# 첫 번째 명령어 (파일 크기 및 정보 확인)\n",
    "!ls -lh ko.tar.gz\n",
    "\n",
    "# 두 번째 명령어 (파일의 실제 종류 확인)\n",
    "!file ko.tar.gz"
   ],
   "id": "e85619cffa730e9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100    17  100    17    0     0     24      0 --:--:-- --:--:-- --:--:--    25\r\n",
      "100   475    0   475    0     0    275      0 --:--:--  0:00:01 --:--:--   475\r\n",
      "100 1472M  100 1472M    0     0  7292k      0  0:03:26  0:03:26 --:--:-- 9298k 6531k  0:02:18 7565k 0  7187k      0  0:03:29  0:01:39  0:01:50 7878k29  0:01:41  0:01:48 8392k  0  7320k      0  0:03:25  0:02:26  0:00:59 7590k0:03:25  0:02:27  0:00:58 7782k03:28  0:02:34  0:00:54 6147k9  0:02:38  0:00:51 5377k:02:46  0:00:42 8960k:49  0:00:39 7468k      0  0:03:29  0:02:58  0:00:31 6987k92k      0  0:03:29  0:03:11  0:00:18 6952k:03:24  0:00:03 9017k\r\n",
      "-rw-r--r--@ 1 park_jinyong  staff   1.4G Sep 10 09:50 ko.tar.gz\r\n",
      "ko.tar.gz: gzip compressed data, last modified: Tue Feb 28 00:36:56 2017, from Unix, original size modulo 2^32 1701498880\r\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "!rm ko.tar.gz",
   "id": "e4f0139ff5816c44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T00:57:02.063228Z",
     "start_time": "2025-09-10T00:56:53.834563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 파일 다운로드 (Dropbox 링크이므로 따옴표와 -O 옵션을 사용합니다)\n",
    "#!curl -O \"https://www.dropbox.com/scl/fi/uuhouaynepr9evqfohos3/ko.tar.gz?rlkey=rhie0pp7wmib4sj1xn4twbkw2&e=2&st=ajq6fvhs\"\n",
    "# 2. 압축 해제 (tar 명령어 사용)\n",
    "!tar -xzvf ko.tar.gz\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "kv = KeyedVectors.load_word2vec_format(\"ko.vec\", binary=False)\n",
    "\n",
    "model_path = 'ko.vec'\n",
    "\n",
    "print(\"Word2Vec 모델 로딩 시작...\")\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=False)\n",
    "print(\"모델 로딩 완료!\")\n",
    "\n",
    "# ✅ 모델 테스트\n",
    "try:\n",
    "    vector = word2vec_model['컴퓨터']\n",
    "    print(\"'컴퓨터'의 벡터:\", vector)\n",
    "    print(\"벡터 차원:\", vector.shape) # (200,) 또는 (300,) 모델마다 다름\n",
    "except KeyError:\n",
    "    print(\"'컴퓨터' 단어가 사전에 없습니다.\")"
   ],
   "id": "fa4baf5da0165b03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x ko.bin\r\n",
      "x ko.vec\r\n",
      "Word2Vec 모델 로딩 시작...\n",
      "모델 로딩 완료!\n",
      "'컴퓨터'의 벡터: [ 3.6081e-01 -8.5240e-02  9.2128e-02 -4.4249e-02 -5.1810e-01  2.6427e-01\n",
      "  3.8151e-01  4.1224e-01 -6.1367e-02 -4.2526e-02 -4.7962e-01  3.2127e-01\n",
      " -1.5477e-01  4.1763e-02  2.8822e-01  5.1173e-01 -1.1828e-01  5.2305e-01\n",
      " -1.0910e-01  1.9045e-01 -1.9108e-01 -2.7519e-02 -5.9669e-02 -3.7562e-01\n",
      " -2.8703e-01 -4.0761e-02  7.6759e-02  8.6558e-02 -6.5370e-02  7.1022e-02\n",
      " -2.6064e-01 -1.3056e-01  2.1873e-01 -2.0111e-01 -1.0775e-01 -4.3720e-01\n",
      "  3.0548e-01 -1.8242e-01 -4.0179e-01  1.6505e-01 -4.8148e-02  9.2403e-02\n",
      "  3.0426e-01 -5.6331e-02 -3.3250e-01  2.8570e-01  4.2549e-01  8.9153e-02\n",
      " -2.2464e-01  2.7263e-01 -2.6976e-02 -1.2032e-01  5.1760e-01 -1.0365e-01\n",
      "  1.0206e-01  2.0882e-02  3.8439e-01 -4.7235e-01  3.0485e-01  8.1731e-03\n",
      "  3.4999e-01 -1.2295e-01 -2.3748e-01  1.1078e-01  6.7756e-02  5.0456e-01\n",
      "  1.6364e-01  2.0894e-01  2.4721e-02  3.7645e-01  8.2944e-02 -5.5810e-01\n",
      " -1.6482e-01 -5.0770e-02 -1.8565e-01 -5.3568e-01  1.5775e-01  1.4935e-01\n",
      " -8.5630e-02  2.7038e-01  4.0158e-02 -1.8733e-01  3.7812e-01  3.3115e-01\n",
      " -1.8729e-01 -3.9209e-01 -3.3171e-01  1.5643e-01 -3.5238e-01  3.8611e-01\n",
      "  1.1572e-01  3.1211e-02 -9.5517e-03  1.2448e-01  1.0505e-01  7.1570e-02\n",
      "  9.6124e-03 -2.9649e-01 -2.1548e-01  2.2291e-01 -1.8238e-01 -1.3178e-02\n",
      " -1.6560e-01 -1.4326e-01 -1.6741e-02  4.2288e-01 -1.4512e-01 -3.4336e-02\n",
      "  3.4271e-02  6.3989e-02 -4.7394e-01  2.7599e-02  5.6263e-02  8.5215e-02\n",
      " -1.3336e-01  2.0939e-01  1.7864e-01 -4.3732e-01 -5.9179e-02  1.2843e-01\n",
      " -5.8136e-03 -2.8976e-01 -7.1814e-01  7.7171e-02 -4.1542e-02 -3.6747e-03\n",
      "  1.1310e-01  1.1900e-01  5.4204e-01  2.8027e-01  2.8636e-01 -1.0495e-01\n",
      "  4.6158e-02 -1.8707e-01 -2.7583e-01 -1.1273e-01 -1.9434e-01 -1.4862e-01\n",
      "  2.5747e-01 -2.4066e-01 -1.9151e-01 -2.4363e-01 -6.4466e-02  5.1127e-02\n",
      "  5.3240e-01  3.0133e-01  3.1094e-02  4.5939e-02  3.9414e-01 -1.5729e-01\n",
      "  2.6554e-01  5.6296e-01 -6.7088e-01 -2.1408e-01  5.9323e-02 -7.5162e-02\n",
      "  4.5144e-02  1.2002e-01 -2.9560e-01  2.1724e-01  2.4200e-01  5.0526e-01\n",
      "  2.5691e-01 -4.5188e-01  3.4418e-02  1.2751e-01  5.3614e-01 -2.6381e-01\n",
      "  2.0907e-02  1.8207e-01 -6.2358e-02  1.6773e-01  1.2176e-01 -1.2993e-02\n",
      " -3.5268e-01 -2.7372e-01  4.0025e-01 -1.1860e-01 -3.1789e-01  1.5410e-01\n",
      " -7.5868e-02  3.4835e-02 -2.9939e-01  4.4971e-01  2.3390e-01  8.1813e-02\n",
      " -5.4148e-02  7.5054e-02 -1.6349e-03 -3.5618e-01  2.0823e-01 -2.6477e-01\n",
      " -1.4367e-01  6.4058e-04 -3.2385e-01 -1.1548e-01  1.9808e-01  4.7077e-01\n",
      " -2.4800e-01  4.4282e-01]\n",
      "벡터 차원: (200,)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:01:22.487541Z",
     "start_time": "2025-09-10T01:01:22.419069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word2vec_model.most_similar(\"바나나\")\n",
    "#한국어 모델이기에 바나나와 비슷한 벡터를 가진 단러 출력"
   ],
   "id": "dedf3d92f1202e98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('파인애플', 0.6781472563743591),\n",
       " ('코코넛', 0.632330060005188),\n",
       " ('사탕무', 0.6066221594810486),\n",
       " ('사탕', 0.6055557727813721),\n",
       " ('사탕수수', 0.6022945046424866),\n",
       " ('과일', 0.5955822467803955),\n",
       " ('감자', 0.5950811505317688),\n",
       " ('해바라기', 0.5877541899681091),\n",
       " ('땅콩', 0.5767338871955872),\n",
       " ('코코아', 0.5660088658332825)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:07:31.126314Z",
     "start_time": "2025-09-10T02:07:14.478623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!brew uninstall mecab\n",
    "!brew install mecab-ko mecab-ko-dic\n",
    "!pip install konlpy mecab-ko-dic"
   ],
   "id": "c06ed03526be728f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling /opt/homebrew/Cellar/mecab/0.996... (19 files, 4.1MB)\r\n",
      "\r\n",
      "\u001B[33mWarning:\u001B[0m The following may be mecab configuration files and have not been removed!\r\n",
      "If desired, remove them manually with `rm -rf`:\r\n",
      "  /opt/homebrew/etc/mecabrc\r\n",
      "  /opt/homebrew/etc/mecabrc.default\r\n",
      "\u001B[33mWarning:\u001B[0m mecab-ko 0.996-ko-0.9.2 is already installed and up-to-date.\r\n",
      "To reinstall 0.996-ko-0.9.2, run:\r\n",
      "  brew reinstall mecab-ko\r\n",
      "\u001B[33mWarning:\u001B[0m mecab-ko-dic 2.1.1-20180720 is already installed and up-to-date.\r\n",
      "To reinstall 2.1.1-20180720, run:\r\n",
      "  brew reinstall mecab-ko-dic\r\n",
      "Requirement already satisfied: konlpy in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (0.6.0)\r\n",
      "Collecting mecab-ko-dic\r\n",
      "  Downloading mecab-ko-dic-1.0.0.tar.gz (33.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m33.2/33.2 MB\u001B[0m \u001B[31m6.5 MB/s\u001B[0m  \u001B[33m0:00:05\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: JPype1>=0.7.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from konlpy) (1.6.0)\r\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from konlpy) (6.0.1)\r\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from konlpy) (2.0.2)\r\n",
      "Requirement already satisfied: packaging in /opt/homebrew/anaconda3/envs/chatbot/lib/python3.9/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\r\n",
      "Building wheels for collected packages: mecab-ko-dic\r\n",
      "\u001B[33m  DEPRECATION: Building 'mecab-ko-dic' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'mecab-ko-dic'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001B[0m\u001B[33m\r\n",
      "\u001B[0m  Building wheel for mecab-ko-dic (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for mecab-ko-dic: filename=mecab_ko_dic-1.0.0-py3-none-any.whl size=33424435 sha256=b4d17994688458828d282431a936d8ebddff26a18064ce4ed09bff2a1ff52514\r\n",
      "  Stored in directory: /Users/park_jinyong/Library/Caches/pip/wheels/1e/26/c0/ed4c061096b1480abefe1e2a0356543bf7a38f61c037b69ab6\r\n",
      "Successfully built mecab-ko-dic\r\n",
      "Installing collected packages: mecab-ko-dic\r\n",
      "Successfully installed mecab-ko-dic-1.0.0\r\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:09:05.876722Z",
     "start_time": "2025-09-10T02:09:05.863931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#import mecab\n",
    "\n",
    "sample_sentence = \"나는 사과다.\"\n",
    "#sample_tokens = sample_sentence.split()\n",
    "sample_sentence = make_corpus(sample_sentence, tokenizer)\n",
    "#sample_tokens = mecab.nouns(sample_sentence)\n",
    "\n",
    "selected_tok = random.choice(sample_tokens)\n",
    "result = \"\"\n",
    "for tok in sample_tokens:\n",
    "    if tok is selected_tok:\n",
    "        result += word2vec_model.most_similar(tok)[0][0] + \" \"\n",
    "\n",
    "    else:\n",
    "        result += tok + \" \"\n",
    "\n",
    "print(\"From:\", sample_sentence)\n",
    "print(\"To:\", result)"
   ],
   "id": "7c963d9be8c5e9ed",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[61], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m sample_sentence \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m나는 사과다.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#sample_tokens = sample_sentence.split()\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m sample_sentence \u001B[38;5;241m=\u001B[39m make_corpus(sample_sentence, \u001B[43mtokenizer\u001B[49m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#sample_tokens = mecab.nouns(sample_sentence)\u001B[39;00m\n\u001B[1;32m      8\u001B[0m selected_tok \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mchoice(sample_tokens)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T02:05:44.035622Z",
     "start_time": "2025-09-10T02:05:44.033227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "korean_tokenizer = generate_tokenizer(\n",
    "    corpus=korean_sentences,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    lang=\"kor\"  # 파일 이름은 \"kor_spm.model\" 등으로 생성됨\n",
    ")"
   ],
   "id": "baccf561cd448ec4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/chatbot/bin/python\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T01:31:29.794067Z",
     "start_time": "2025-09-10T01:31:29.786178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Q. Lexical Substitution 을 구현해봅시다.\n",
    "def lexical_sub(sentence, wv):\n",
    "    # 문장을 토큰화\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    # 유효한 단어 필터링 (임베딩에 존재하는 단어만 고려)\n",
    "    valid_tokens = [tok for tok in tokens if tok in wv]\n",
    "\n",
    "    # 대체할 단어 선택 (임베딩 내 존재하는 단어 중 하나)\n",
    "    if not valid_tokens:\n",
    "        return sentence  # 모든 단어가 임베딩 내에 없으면 원래 문장 반환\n",
    "\n",
    "    selected_tok = random.choice(valid_tokens)\n",
    "\n",
    "    # 가장 유사한 단어 찾기\n",
    "    similar_word = wv.most_similar(selected_tok)[0][0]\n",
    "\n",
    "    # 변환된 문장 생성\n",
    "    new_sentence = \" \".join([similar_word if tok == selected_tok else tok for tok in tokens])\n",
    "\n",
    "    return new_sentence"
   ],
   "id": "fd6c1ea0d96742b8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "647d776cc5e15c14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
