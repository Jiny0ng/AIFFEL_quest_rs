{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Step 0. í™˜ê²½ì¤€ë¹„/ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
      ],
      "metadata": {
        "id": "-VtKxELDpkex"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NgoI1RZbpVgY"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "# !pip install loralib\n",
        "# !pip install trl\n",
        "# !pip install accelerate\n",
        "# !pip install transformers\n",
        "# !pip install evaluate\n",
        "#!pip install sacrebleu rouge-score\n",
        "# #ì‹¤í–‰ í›„ ëŸ°íƒ€ì„ ì¬ì‹œì‘"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mxk296_GtpLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 0. í™˜ê²½ ì„¤ì • / ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
      ],
      "metadata": {
        "id": "RzLa_okSlERw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/airobotlab/KoChatGPT\n",
        "!cp -r KoChatGPT/colossalai_ChatGPT_230319/chatgpt chatgpt\n",
        "\n",
        "import os\n",
        "\n",
        "modifications = [\n",
        "    {\n",
        "        \"file\": \"chatgpt/trainer/callbacks/save_checkpoint.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 3, \"old\": \"from chatgpt.trainer.strategies import ColossalAIStrategy, Strategy\",\n",
        "             \"new\": \"from chatgpt.trainer.strategies import Strategy\"},\n",
        "            {\"line\": 71, \"old\": \"only_rank0 = not isinstance(self.strategy, ColossalAIStrategy)\",\n",
        "             \"new\": \"            only_rank0 = not isinstance(self.strategy)\"},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 1, \"old\": \"from .colossalai import ColossalAIStrategy\", \"new\": \"\"},  # ì‚­ì œ\n",
        "            {\"line\": 5, \"old\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy', 'ColossalAIStrategy']\",\n",
        "             \"new\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy']\"},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 3, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"chatgpt/trainer/base.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"chatgpt/trainer/rm.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "#íŒŒì¼ì—ì„œ ì§€ì •ëœ ì¤„ì„ ì°¾ì•„ ë‚´ìš©ì„ ìˆ˜ì •\n",
        "def modify_file(file_path, changes):\n",
        "\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}\")\n",
        "        return\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    modified = False\n",
        "\n",
        "    for change in changes:\n",
        "        line_index = change[\"line\"]\n",
        "        if 0 <= line_index < len(lines):\n",
        "            if lines[line_index].strip() == change[\"old\"]:\n",
        "                lines[line_index] = change[\"new\"] + \"\\n\"\n",
        "                modified = True\n",
        "            else:\n",
        "                print(f\"âš ï¸ {file_path} íŒŒì¼ì˜ {change['line']}ë²ˆì§¸ ì¤„ì´ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤.\")\n",
        "                print(f\"   ì˜ˆìƒ: {change['old']}\")\n",
        "                print(f\"   ì‹¤ì œ: {lines[line_index].strip()}\")\n",
        "\n",
        "    if modified:\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.writelines(lines)\n",
        "        print(f\"âœ… ìˆ˜ì • ì™„ë£Œ: {file_path}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ {file_path} ìˆ˜ì •í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "for mod in modifications:\n",
        "    modify_file(mod[\"file\"], mod[\"changes\"])"
      ],
      "metadata": {
        "id": "rVUN4FKep34F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í™•ì¸ ë° dirve ì—°ê²°\n",
        "í•™ìŠµì— í•„ìš”í•œ autotokenizer, autoModelForCauseLMë“±ì„ import\n",
        "\n",
        "ì¶”í›„ ì›í™œí•œ ìˆ˜í–‰ì„ ìœ„í•´ driveì™€ ì—°ê²°"
      ],
      "metadata": {
        "id": "c8kbgeeNleE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import numpy\n",
        "\n",
        "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
        "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
        "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
        "print(\"GPU ì‚¬ìš© ê°€ëŠ¥ì—¬ë¶€: {}\".format(torch.cuda.is_available()))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ë§Œì¼ ì•„ë˜ ëª¨ë“ˆì´ ë¶ˆëŸ¬ì™€ì§€ì§€ ì•ŠëŠ”ë‹¤ë©´ Clone ë° ìˆ˜ì •ì„ ì˜ ì§„í–‰í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\n",
        "from chatgpt.trainer.strategies import NaiveStrategy"
      ],
      "metadata": {
        "id": "YlAfGYnIp-kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"skt/kogpt2-base-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "jNf3h6nHqC1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1. ë°ì´í„° ë° ë””ì½”ë”© ì„±ëŠ¥ í™•ì¸"
      ],
      "metadata": {
        "id": "f_fLyQI5tlY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_txt =  \"ë°”ëŒë„ ì—†ëŠ” ê³µì¤‘ì— ìˆ˜ì§ì˜ íŒŒë¬¸ì„ ë‚´ì´ë©° ê³ ìš”íˆ ë–¨ì–´ì§€ëŠ” ì˜¤ë™ìì€ ëˆ„êµ¬ì˜ ë°œìì·¨ ì…ë‹ˆê¹Œ.\"\n",
        "\n",
        "tokens = tokenizer(input_txt).tokens()\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()\n",
        "\n",
        "pd.options.display.max_columns = 40\n",
        "pd.options.display.max_rows = 60\n",
        "df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n",
        "df"
      ],
      "metadata": {
        "id": "Rs_G9IkwtlDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë””ì½”ë”© ì„±ëŠ¥ í™•ì¸\n",
        "max_length=128\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
        "print(tokenizer.decode(output_greedy[0]))"
      ],
      "metadata": {
        "id": "tG08wi-UtxXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì²˜ìŒ ìƒì„±í•œ ë‹µë³€ì„ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ëª¨ìŠµì´ ë³´ì¸ë‹¤.\n",
        "ì•ì„œ ìƒì„±í•œ í† í°ì˜ ì˜í–¥ì„ ë§ì´ ë°›ìŒ"
      ],
      "metadata": {
        "id": "oI6K-Sz43N8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
        "                             do_sample=True, top_p=0.90)\n",
        "print(tokenizer.decode(output_beam[0]))"
      ],
      "metadata": {
        "id": "yTgN3OeHt-cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë‹¨ìˆœíˆ ë””ì½”ë”©ì—ì„œ beam searchë¥¼ ì ìš©í•´ì£¼ëŠ”ê²ƒë§Œìœ¼ë¡œ ì–´ëŠì •ë„ì˜ í’ˆì§ˆì€ ê¸°ëŒ€í•  ìˆ˜ ìˆì§€ë§Œ\n",
        "ëª¨ë¸ ìì²´ì˜ ì„±ëŠ¥ì„ ëŒì–´ì˜¬ë¦°ë‹¤ë©´ ë” ê²½ì œì ìœ¼ë¡œ ê³ í’ˆì§ˆì˜ ì¶œë ¥ì„ ë§Œë“¤ ìˆ˜ ìˆì„ê²ƒì´ë¼ ìƒê°í•¨.\n"
      ],
      "metadata": {
        "id": "9Gv4FrsxuImx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2. Supervised Fine-Tuning\n",
        "\n",
        "ë² ì´ìŠ¤ ëª¨ë¸ : skt/kogpt2-base-v2\n",
        "\n",
        "ë°ì´í„° : {prompt, completion} ë°ì´í„°ìŒ\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BZANQvrnusk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Dict, Sequence\n",
        "from torch.utils.data import Dataset\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "import copy\n",
        "import json"
      ],
      "metadata": {
        "id": "7BceByiuuCAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸, í† í¬ë‚˜ì´ì € í˜¸ì¶œ\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
        "    padding_side=\"right\",\n",
        "    model_max_length=512,\n",
        ")\n",
        "\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "trfs2OXUu0qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  prompt ë”•ì…”ë„ˆë¦¬ í…œí”Œë¦¿ê³¼ SFT ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¥¼ ì •ì˜\n",
        "\n",
        "class SFT_dataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
        "        super(SFT_dataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "\n",
        "        pattern_instruction = 'prompt'  # instruction\n",
        "        pattern_output = 'completion'  # response\n",
        "\n",
        "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
        "            list_data_dict = json.load(json_file)\n",
        "\n",
        "        PROMPT_DICT = {\n",
        "            \"prompt_input\": (\n",
        "                \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
        "            )\n",
        "        }\n",
        "\n",
        "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
        "\n",
        "        sources = []\n",
        "        for example in list_data_dict:\n",
        "            tmp = prompt_input.format_map(example)\n",
        "            sources.append(tmp)\n",
        "\n",
        "        targets = []\n",
        "        for example in list_data_dict:\n",
        "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
        "        examples = [s + t for s, t in zip(sources, targets)]\n",
        "\n",
        "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
        "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
        "\n",
        "        input_ids = examples_tokenized[\"input_ids\"]\n",
        "        labels = copy.deepcopy(input_ids)\n",
        "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "            label[:source_len] = -100\n",
        "\n",
        "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
        "\n",
        "\n",
        "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "        tokenized_list = [\n",
        "            tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"longest\",\n",
        "                max_length=tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "            )\n",
        "            for text in strings\n",
        "        ]\n",
        "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "        input_ids_lens = labels_lens = [\n",
        "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "        ]\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            input_ids_lens=input_ids_lens,\n",
        "            labels_lens=labels_lens,\n",
        "        )\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
      ],
      "metadata": {
        "id": "NyDxWdhGu3wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )"
      ],
      "metadata": {
        "id": "F0xfJLtCvuka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SFT_dataset í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ í›ˆë ¨ì…‹ì„ ë§Œë“¤ê³  data collator ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
        "\n",
        "train_dataset = SFT_dataset(data_path_1_SFT='KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "\n",
        "print('input : %s'%train_dataset.input_ids[0])\n",
        "print('output: %s'%train_dataset.labels[0])"
      ],
      "metadata": {
        "id": "nigQ2D2WvwhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë””ì½”ë”© í•¨ìˆ˜ ì‘ì„±\n",
        "def decode_tokens(tokenizer, input_ids, label_ids):\n",
        "  \"\"\"\n",
        "  ì£¼ì–´ì§„ input_idsì™€ label_idsë¥¼ ë””ì½”ë”©í•˜ì—¬ ì›ë³¸ textë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜.\n",
        "  label_idsì— í¬í•¨ëœ -100 ê°’ì€ ë””ì½”ë”©ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.\n",
        "  \"\"\"\n",
        "  # input_ids ë””ì½”ë”©\n",
        "  # skip_special_tokens=True ì˜µì…˜ìœ¼ë¡œ </s>ì™€ ê°™ì€ íŠ¹ìˆ˜ í† í°ì„ ì œì™¸í•˜ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "  decoded_input = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "\n",
        "  # label_idsì—ì„œ -100ì„ ì œì™¸í•œ í† í°ë§Œ í•„í„°ë§\n",
        "  filtered_label_ids = [token_id for token_id in label_ids if token_id != -100]\n",
        "\n",
        "  # í•„í„°ë§ëœ label_ids ë””ì½”ë”©\n",
        "  decoded_label = tokenizer.decode(filtered_label_ids, skip_special_tokens=True)\n",
        "\n",
        "  print(\"--- [ë””ì½”ë”© ê²°ê³¼] ---\")\n",
        "  print(f\"â¡ï¸ Input (ì „ì²´ ì›ë³¸ ë¬¸ì¥):\\n{decoded_input}\\n\")\n",
        "  print(f\"âœ… Label (ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” ì •ë‹µ ë¬¸ì¥):\\n{decoded_label}\")\n",
        "\n",
        "# í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ ë°ì´í„° í™•ì¸\n",
        "decode_tokens(\n",
        "    tokenizer,\n",
        "    train_dataset.input_ids[0],\n",
        "    train_dataset.labels[0]\n",
        ")"
      ],
      "metadata": {
        "id": "mkeq8p5MvyPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training argumentsë¥¼ ì‚¬ìš©í•´ trainer í´ë˜ìŠ¤ë¥¼ ì •ì˜\n",
        "\n",
        "# training_args = transformers.TrainingArguments(\n",
        "#     output_dir=\"test\",\n",
        "#     overwrite_output_dir=True,\n",
        "#     num_train_epochs=1,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     per_device_eval_batch_size=8,\n",
        "#     warmup_steps=5,\n",
        "#     prediction_loss_only=True,\n",
        "#     fp16 = True\n",
        "#     )\n",
        "training_args = transformers.TrainingArguments(\n",
        "        output_dir=\"models/improved_sft_v2\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,  # ë³€ê²½: 1 -> 3\n",
        "        per_device_train_batch_size=4,  # ë³€ê²½: 8 -> 4 (ë©”ëª¨ë¦¬ ì ˆì•½)\n",
        "        gradient_accumulation_steps=4,  # ì¶”ê°€: ê°€ìƒ ë°°ì¹˜ 16\n",
        "        learning_rate=2e-5,  # ë³€ê²½: ê¸°ë³¸ê°’ -> ë‚®ì¶¤\n",
        "        warmup_steps=100,  # ì¶”ê°€: ì›Œë°ì—…\n",
        "        prediction_loss_only=True,\n",
        "        fp16=True\n",
        "        )\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "njAfLJo1wYMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SFT í›ˆë ¨ì„ ì§„í–‰\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained('/content/drive/MyDrive/model_output/output_1_SFT')"
      ],
      "metadata": {
        "id": "jXGDyUSzwNag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"
      ],
      "metadata": {
        "id": "0MOTQL2_1pGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ë¬¸ì¥ ìƒì„± ëŠ¥ë ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ë¹ ë¥´ê²Œ í—ˆê¹…í˜ì´ìŠ¤ì˜ pipleline í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ generator êµ¬í˜„\n",
        "generator = transformers.pipeline('text-generation', model='models/output_1_SFT', tokenizer=tokenizer)\n",
        "\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    eos_token_id=375, # \\n\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
        "    )\n",
        "}\n",
        "\n",
        "list_prompt = ['ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
        "               'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
        "               'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´?',\n",
        "               'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
        "\n",
        "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
        "\n",
        "list_result = generator(list_prompt, **generation_args)\n",
        "for prompt, result in zip(list_prompt, list_result):\n",
        "    print()\n",
        "    print((result[0]['generated_text']))"
      ],
      "metadata": {
        "id": "bEZz3ni4wRL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì €ëŠ” ì¸ê³µì§€ëŠ¥ ì–´ì‹œìŠ¤í„´íŠ¸ì´ê¸° ë•Œë¬¸ì— \"ì…ë ¥\"ì— ëŒ€í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤\n",
        "\n",
        "ë¼ëŠ” í”„ë¡¬í”„íŠ¸ê°€ ì¡´ì¬í•˜ëŠ”ë“¯í•¨"
      ],
      "metadata": {
        "id": "eg8-Dts92I9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2. RM í•™ìŠµ\n",
        "Ranknigì…‹ìœ¼ë¡œ ì¤€ë¹„ëœ ë°ì´í„°ë¥¼\n",
        "\n",
        "pairwiseí˜•ì‹ìœ¼ë¡œ ì „í™˜ í›„ í•™ìŠµì‹œë„\n",
        "\n"
      ],
      "metadata": {
        "id": "cq_Ur-S5cFvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "irkAmJqVSeKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RM í•™ìŠµì— í•„ìš”í•œ í´ë˜ìŠ¤ ì„í¬íŠ¸\n",
        "from chatgpt.dataset import RewardDataset\n",
        "from chatgpt.models.base import RewardModel\n",
        "from chatgpt.trainer.strategies import NaiveStrategy\n",
        "from chatgpt.trainer.rm import RewardModelTrainer\n",
        "\n",
        "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
        "\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# SFTì™€ ë™ì¼í•œ í† í¬ë‚˜ì´ì € ì„¤ì • ì‚¬ìš©\n",
        "rm_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
        "    padding_side=\"right\",\n",
        "    model_max_length=512,\n",
        ")"
      ],
      "metadata": {
        "id": "zsIoA5nBcQaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTRM_custom(RewardModel):\n",
        "    \"\"\"\n",
        "    GPT-2ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” Custom Reward Model.\n",
        "    ì…ë ¥ëœ í…ìŠ¤íŠ¸ì˜ ì¢‹ê³  ë‚˜ì¨ì„ íŒë‹¨í•˜ì—¬ ë‹¨ì¼ ì ìˆ˜(reward)ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 pretrained: Optional[str] = None,\n",
        "                 config: Optional[GPT2Config] = None,\n",
        "                 checkpoint: bool = False,\n",
        "                 lora_rank: int = 0,\n",
        "                 lora_train_bias: str = 'none',\n",
        "                 tokenizer=None) -> None:\n",
        "\n",
        "        if pretrained is not None:\n",
        "            # ì‚¬ì „ í•™ìŠµëœ GPT2 ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "            model = GPT2Model.from_pretrained(pretrained)\n",
        "            # Special tokenì´ ì¶”ê°€ëœ í† í¬ë‚˜ì´ì €ì— ë§ê²Œ ì„ë² ë”© í¬ê¸°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.\n",
        "            model.resize_token_embeddings(len(tokenizer))\n",
        "        elif config is not None:\n",
        "            model = GPT2Model(config)\n",
        "        else:\n",
        "            model = GPT2Model(GPT2Config())\n",
        "\n",
        "        if checkpoint:\n",
        "            model.gradient_checkpointing_enable()\n",
        "\n",
        "        # ëª¨ë¸ì˜ ë§ˆì§€ë§‰ hidden stateë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‹¨ì¼ ì ìˆ˜ë¥¼ ì¶œë ¥í•˜ëŠ” value_headë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "        value_head = nn.Linear(model.config.n_embd, 1)\n",
        "\n",
        "        # ë¶€ëª¨ í´ë˜ìŠ¤ì¸ RewardModelì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
        "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
        "\n",
        "        if pretrained is not None:\n",
        "            self.model = model\n",
        "            self.pretrained = pretrained\n",
        "\n",
        "    def save_pretrained(self, dir):\n",
        "        if self.pretrained is not None:\n",
        "            self.model.save_pretrained(dir)\n",
        "\n",
        "# NaiveStrategy ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ RM ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
        "with NaiveStrategy().model_init_context():\n",
        "    rm_model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=rm_tokenizer).cuda()"
      ],
      "metadata": {
        "id": "t_zqgljXe6Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì¤€ë¹„ëœ RM ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "with open('/content/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "\n",
        "# ë­í‚¹ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ pairwise(chosen, rejected) ìŒ ë§Œë“¤ê¸°\n",
        "total_data_ranking2chosen = []\n",
        "for tmp in list_data_dict:\n",
        "    one_data_ranking2chosen = []\n",
        "\n",
        "    # 3ê°œì˜ ë‹µë³€ ì¤‘ 2ê°œë¥¼ ë½‘ëŠ” ëª¨ë“  ì¡°í•©(3ê°€ì§€)ì— ëŒ€í•´ ìŒì„ ìƒì„±\n",
        "    # completion_0 vs completion_1\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # completion_0 vs completion_2\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # completion_1 vs completion_2\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
        "\n",
        "print('before data num: %d'%(len(list_data_dict)))\n",
        "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
        "print('data example: \\n%s'%total_data_ranking2chosen[45])"
      ],
      "metadata": {
        "id": "nz8g2uKTf09U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„° ì…”í”Œ ë° ë¶„í• \n",
        "random.seed(230319)\n",
        "random.shuffle(total_data_ranking2chosen)\n",
        "\n",
        "train_data = total_data_ranking2chosen[:1000]\n",
        "eval_data = total_data_ranking2chosen[1000:1200]\n",
        "\n",
        "print(f\"Train data size: {len(train_data)}\")\n",
        "print(f\"Eval data size: {len(eval_data)}\")\n",
        "\n",
        "# RewardDataset ê°ì²´ ìƒì„±\n",
        "train_dataset = RewardDataset(train_data, rm_tokenizer, 512)\n",
        "eval_dataset = RewardDataset(eval_data, rm_tokenizer, 512)\n",
        "\n",
        "# ë°ì´í„° ìƒ˜í”Œ í™•ì¸\n",
        "idx = 1\n",
        "print('#'*70)\n",
        "print('## prompt ##')\n",
        "print(train_data[idx]['prompt'])\n",
        "print('#'*70)\n",
        "print('## chosen ##')\n",
        "print(train_data[idx]['chosen'])\n",
        "print('#'*70)\n",
        "print('## rejected ##')\n",
        "print(train_data[idx]['rejected'])"
      ],
      "metadata": {
        "id": "LXspb2rCf7fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RM íŠ¸ë ˆì´ë„ˆ ì„¤ì •\n",
        "rm_trainer = RewardModelTrainer(model=rm_model,\n",
        "                             strategy=NaiveStrategy(),\n",
        "                             optim=torch.optim.Adam(rm_model.parameters(), lr=5e-5),\n",
        "                             train_dataset=train_dataset,\n",
        "                             eval_dataset=eval_dataset,\n",
        "                             batch_size=4,\n",
        "                             max_epochs=1)\n",
        "\n",
        "# RM í•™ìŠµ ì‹œì‘\n",
        "rm_trainer.fit(use_lora=0)\n",
        "\n",
        "# í•™ìŠµëœ RM ëª¨ë¸ ì €ì¥\n",
        "rm_model.save_pretrained('/content/drive/MyDrive/model_output/output_2_RM')"
      ],
      "metadata": {
        "id": "p5LGYyfSk5A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RM ì¶”ë¡  í•¨ìˆ˜ ì •ì˜\n",
        "def inference_RM(input_text):\n",
        "    input_ids = rm_tokenizer.encode(input_text, return_tensors='pt').to(\n",
        "        torch.cuda.current_device()\n",
        "    )\n",
        "    output = rm_model(input_ids)\n",
        "    output_reward = output.cpu().detach().numpy()[0]\n",
        "\n",
        "    print('input: %s\\nreward score: %.1f' % (input_text, output_reward))\n",
        "    return output_reward\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ 1: ë¶€ì •ì ì¸ ë¬¸ì¥\n",
        "input_text = 'ì¸ê³µì§€ëŠ¥ì€ ë˜¥ë©ì²­ì´ ì…ë‹ˆë‹¤'\n",
        "output_reward = inference_RM(input_text=input_text)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ 2: ê¸ì •ì ì´ê³  ì§§ì€ ë¬¸ì¥\n",
        "input_text = 'ì¸ê³µì§€ëŠ¥(AI)ì€ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.'\n",
        "output_reward = inference_RM(input_text=input_text)\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ 3: ê¸ì •ì ì´ê³  ìƒì„¸í•œ ë¬¸ì¥\n",
        "input_text = \"ì¸ê³µì§€ëŠ¥ì€ ì¼ë°˜ì ìœ¼ë¡œ ì¸ê°„ì˜ ì§€ëŠ¥ì´ í•„ìš”í•˜ê±°ë‚˜ ì¸ê°„ì´ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ê²ƒë³´ë‹¤ ê·œëª¨ê°€ í° ë°ì´í„°ë¥¼ í¬í•¨í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¶”ë¡ , í•™ìŠµ ë° í–‰ë™í•  ìˆ˜ ìˆëŠ” ì»´í“¨í„° ë° ê¸°ê³„ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒê³¼ ê´€ë ¨ëœ ê³¼í•™ ë¶„ì•¼ì…ë‹ˆë‹¤.\"\n",
        "output_reward = inference_RM(input_text=input_text)\n",
        "\n",
        "# GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8N022e-gk88L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 4. PPO ì ìš©\n",
        "\n",
        "SFT, RMëª¨ë¸ì„ ì´ìš©í•´ PPOë¥¼ ìˆ˜í–‰\n"
      ],
      "metadata": {
        "id": "ce7OUNBEmmEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
        "from chatgpt.trainer import PPOTrainer\n",
        "\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "mlbLLjvwmcZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NaiveStrategy ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ PPO í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  ëª¨ë¸ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
        "with NaiveStrategy().model_init_context():\n",
        "    # Actor: SFT ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "    actor = GPTActor(pretrained='/content/drive/MyDrive/model_output/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
        "\n",
        "    # Critic: RM ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
        "    critic = GPTCritic(pretrained='/content/drive/MyDrive/model_output/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
        "\n",
        "    # Tokenizer: ì´ì „ê³¼ ë™ì¼í•œ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
        "        padding_side=\"right\",\n",
        "        model_max_length=512\n",
        "    )\n",
        "\n",
        "    # Initial Model: SFT ëª¨ë¸ì„ ë³µì‚¬í•˜ì—¬ KL í˜ë„í‹° ê³„ì‚°ì— ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "    initial_model = deepcopy(actor)\n",
        "\n",
        "    # Reward Model: Critic ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë³´ìƒ ê³„ì‚°ì— ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())\n",
        "\n",
        "# Actorì™€ Criticì„ ìœ„í•œ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "actor_optim = torch.optim.Adam(actor.parameters(), lr=5e-6)\n",
        "critic_optim = torch.optim.Adam(critic.parameters(), lr=5e-6)\n",
        "\n",
        "# Strategyë¥¼ í†µí•´ ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì €ë¥¼ ë˜í•‘í•©ë‹ˆë‹¤.\n",
        "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
        "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
      ],
      "metadata": {
        "id": "-xBee6Cnm0IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PPO í•™ìŠµì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë°ì´í„° ë¡œë“œ\n",
        "with open('//content/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
        "\n",
        "# PPO Trainer ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•  í† í¬ë‚˜ì´ì € í•¨ìˆ˜ ì •ì˜\n",
        "def tokenize_fn(texts):\n",
        "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
        "    return {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "print(f\"PPO í•™ìŠµì— ì‚¬ìš©ë  í”„ë¡¬í”„íŠ¸ ê°œìˆ˜: {len(list_prompt)}\")"
      ],
      "metadata": {
        "id": "R-W-ui73m1Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PPOTrainer ì´ˆê¸°í™”\n",
        "trainer = PPOTrainer(NaiveStrategy(),\n",
        "                     actor,           #í•™ìŠµì´ ì§„í–‰ë  ëª¨ë¸\n",
        "                     critic,          #RM\n",
        "                     reward_model,    #RM\n",
        "                     initial_model,   #SFTì ìš©ëª¨ë¸\n",
        "                     actor_optim,\n",
        "                     critic_optim,\n",
        "                     max_epochs=1,\n",
        "                     train_batch_size=8,\n",
        "                     tokenizer=tokenize_fn,\n",
        "                     max_length=128,\n",
        "                     do_sample=True,\n",
        "                     temperature=1.0,\n",
        "                     top_k=50,\n",
        "                     pad_token_id=tokenizer.pad_token_id,\n",
        "                     eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# PPO í•™ìŠµ ì‹œì‘\n",
        "trainer.fit(list_prompt,\n",
        "            num_episodes=10,\n",
        "            max_timesteps=3,\n",
        "            update_timesteps=3)\n",
        "\n",
        "# ìµœì¢… PPO ëª¨ë¸ ì €ì¥\n",
        "actor.model.save_pretrained('/content/drive/MyDrive/model_output/output_3_PPO')"
      ],
      "metadata": {
        "id": "xE2fEs3Loi3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìµœì¢… PPO ëª¨ë¸ë¡œ ë‹µë³€ ìƒì„±\n",
        "def generation(input_text, model, tokenizer):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
        "        torch.cuda.current_device())\n",
        "\n",
        "    # PPO ëª¨ë¸ì€ Actor í´ë˜ìŠ¤ë¡œ ë˜í•‘ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ë‚´ë¶€ ëª¨ë¸ì„ ì§ì ‘ ì‚¬ìš©\n",
        "    outputs = model.model.generate(input_ids,\n",
        "                             max_length=250,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "\n",
        "    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    print(output_text)\n",
        "    return output_text\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ìš© í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸\n",
        "list_prompt = [\n",
        "    'ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?',\n",
        "    'ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?',\n",
        "    'ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´',\n",
        "    'ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?']\n",
        "\n",
        "# SFT/PPO ëª¨ë¸ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\"\n",
        "    )\n",
        "}\n",
        "list_formatted_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
        "\n",
        "print(\"--- PPO Model Generation Results ---\")\n",
        "for input_text in list_formatted_prompt:\n",
        "    generation(input_text, actor, tokenizer)\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "WXllsoUXokgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ì‹¤í—˜ ê²°ê³¼ í™•ì¸\n",
        "pipelineì„ ì´ìš©í•´ ì¶œë ¥ì„ ìƒì„±"
      ],
      "metadata": {
        "id": "HfOFch9OplFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "import torch, re\n",
        "\n",
        "generators = {\n",
        "    name: pipeline(\n",
        "        \"text-generation\",\n",
        "        model=path,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device\n",
        "    )\n",
        "    for name, path in MODEL_PATHS.items()\n",
        "}\n",
        "print(\"ready:\", list(generators.keys()))\n",
        "\n",
        "#í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ + ì¢…ë£Œ ì²˜ë¦¬\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": \"### Instruction(ëª…ë ¹ì–´):\\n{prompt}\\n\\n### Response(ì‘ë‹µ):\",\n",
        "}\n",
        "def build_prompt(p: str) -> str:\n",
        "    return PROMPT_DICT[\"prompt_input\"].format_map({\"prompt\": p})\n",
        "\n",
        "# ì¢…ë£Œ ê·œì¹™: ì‘ë‹µ ì´í›„ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¼ì£¼ëŠ” ê°„ë‹¨ í›„ì²˜ë¦¬\n",
        "def postprocess(prompt_text: str, generated_text: str) -> str:\n",
        "    # 1) í”„ë¡¬í”„íŠ¸ ì—ì½” ì œê±°\n",
        "    body = generated_text[len(prompt_text):] if generated_text.startswith(prompt_text) else generated_text\n",
        "\n",
        "    # 2) \"ë‹¤ìŒ ì„¹ì…˜\"ì„ ì•”ì‹œí•˜ëŠ” ë§ˆì»¤ê°€ ë³´ì´ë©´ ê±°ê¸°ì„œ ìë¥´ê¸° (ì˜µì…˜)\n",
        "    cut_points = [\n",
        "        \"\\n\\n###\",             # ë‹¤ìŒ ì„¹ì…˜ ë§ˆì»¤\n",
        "        \"\\n\\nInstruction\",     # ì˜ëª» ì—ì½” ì‹œ\n",
        "        \"\\n\\nëª…ë ¹ì–´\",           # í•œê¸€ ì—ì½” ì‹œ\n",
        "    ]\n",
        "    for cp in cut_points:\n",
        "        if cp in body:\n",
        "            body = body.split(cp)[0]\n",
        "            break\n",
        "\n",
        "    # 3) ê³¼ë„í•œ ê³µë°± ì •ë¦¬\n",
        "    body = body.strip()\n",
        "    return body\n",
        "\n",
        "# ì¢…ë£Œ í† í°: ê°œí–‰ì„ EOSë¡œ ì“°ê³  ì‹¶ë‹¤ë©´ í† í¬ë‚˜ì´ì €ì—ì„œ id ì°¾ê¸°\n",
        "EOS_NL = tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "t13-l_5o1sk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GEN_ARGS_GREEDY = dict(\n",
        "    do_sample=False,\n",
        "    num_beams=4,\n",
        "    max_new_tokens=96,\n",
        "    no_repeat_ngram_size=4,\n",
        "    repetition_penalty=1.2,\n",
        "    eos_token_id=EOS_NL,                  # ì‘ë‹µ ëë‚˜ë©´ ê°œí–‰ì—ì„œ ëŠê¸°\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    early_stopping=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "c5DMKvsQ2DY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_prompt = [\n",
        "    \"ë¶ˆê³ ê¸°ìš© ê³ ê¸° í•œìš°ì—ìš”?\",\n",
        "    \"ë¦¬ì²˜ë“œ ë‹‰ìŠ¨ì´ 43ëŒ€ ë¶€í†µë ¹ì§ì„ ìˆ˜í–‰í•œ ë…„ë„ëŠ”?\",\n",
        "    \"ì‹œì¹´ê³  ì˜¤í—¤ì–´ êµ­ì œê³µí•­ì€ ì–´ë””ì— ìˆì–´?\",\n",
        "    \"ì˜¤ëŠ˜ ë¯¸ì„¸ë¨¼ì§€ ì–´ë•Œ?\"\n",
        "]\n",
        "batched_inputs = [build_prompt(p) for p in list_prompt]\n",
        "\n",
        "def run_and_show(generators, batched_inputs, gen_args):\n",
        "    # ê° ëª¨ë¸ì„ ìˆœíšŒí•˜ë©° ë°°ì¹˜ ìƒì„±\n",
        "    all_outputs = {name: gen(batched_inputs, **gen_args) for name, gen in generators.items()}\n",
        "\n",
        "    # í”„ë¡¬í”„íŠ¸ë³„ë¡œ ë¬¶ì–´ì„œ ì¶œë ¥\n",
        "    for i, raw_prompt in enumerate(list_prompt):\n",
        "        full_prompt = batched_inputs[i]\n",
        "        print(\"=\"*100)\n",
        "        print(f\"PROMPT {i+1}: {raw_prompt}\")\n",
        "        for name in [\"base\", \"sft\", \"ppo\"]:\n",
        "            res_text = all_outputs[name][i][0][\"generated_text\"]\n",
        "            ans = postprocess(full_prompt, res_text)\n",
        "            print(f\"\\n[{name.upper()}]\\n{ans}\\n\")\n",
        "\n",
        "#  ê·¸ë¦¬ë”” ë¹„êµ\n",
        "run_and_show(generators, batched_inputs, GEN_ARGS_GREEDY)"
      ],
      "metadata": {
        "id": "-X7rFdMy2Frk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë² ì´ìŠ¤ ëª¨ë¸ì—ì„œ sft, PPOë¥¼ ê±°ì¹ ìˆ˜ë¡ ì ì  ë‹µë³€ì´ ê¹¨ë—í•´ì§€ëŠ”ê±¸ ë³¼ ìˆ˜ ìˆë‹¤.\n"
      ],
      "metadata": {
        "id": "BTvO1HME2Qto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------\n",
        "--------------------------------------\n",
        "\n",
        "#Beam search ì ìš©\n",
        "--------------------------------------\n",
        "--------------------------------------\n"
      ],
      "metadata": {
        "id": "jBtvPXHa4H8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê°œí–‰ EOSëŠ” ì´ì „ì— ë§Œë“  EOS_NL ì‚¬ìš©(ì—†ìœ¼ë©´ ì•„ë˜ ì¤„ë¡œ êµ¬í•˜ì„¸ìš”)\n",
        "# EOS_NL = tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
        "\n",
        "GEN_ARGS_BEAM = dict(\n",
        "    do_sample=True,          # ğŸ”’ ìˆœìˆ˜ ë¹”ì„œì¹˜ (ëœë¤ì„± X)\n",
        "    num_beams=7,\n",
        "    max_new_tokens=96,\n",
        "    no_repeat_ngram_size=4,\n",
        "    repetition_penalty=1.2,\n",
        "    eos_token_id=EOS_NL,      # ì‘ë‹µ ëì„ ê°œí–‰ìœ¼ë¡œ ìœ ë„\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# ì‹¤í–‰\n",
        "run_and_show(generators, batched_inputs, GEN_ARGS_BEAM)"
      ],
      "metadata": {
        "id": "XqTjG5J04NvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beam Searchë¥¼ ì ìš©í•œ ëª¨ë¸ì˜ ë‹µë³€ì˜ ì–´íœ˜ê°€ ê¸°ì¡´ì— ë¹„í•´ í¸í•´ì¡Œë‹¤.\n",
        "ì ìš©í•˜ì§€ ì•Šì€ ì¶œë ¥ì—ì„œëŠ” ê¸°ì¡´ì— ì£¼ì–´ì§„ ë‹¨ì–´ë“¤ì„ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒí•´ ì–´íˆ¬ê°€ ë”±ë”±í•œ ëŠë‚Œì´ ë“  ë°˜ë©´\n",
        "Beam Searchë¥¼ ì ìš©í•œ ì¶œë ¥ì—ì„œëŠ” ì¢€ ë” ë‹¤ì–‘í•œ ì–´íœ˜ë¥¼ ë³´ì´ë©° ìì—°ìŠ¤ëŸ¬ìš´ ì¶œë ¥ì„ ë³´ì˜€ë‹¤."
      ],
      "metadata": {
        "id": "IU86ofUs5zRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EX 01. ì¶”ê°€ ë°ì´í„° ì •ì œ\n",
        "\n",
        "ì£¼ë§ì‚¬ì´ì— ì‹œë„í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "lM5IN2L5kkSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ExFM7_PCwg6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pmnEocV_zyvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkhWsYln484p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdNOUrx3G2vA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}