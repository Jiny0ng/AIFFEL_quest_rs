{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Step 0. 환경준비/ 라이브러리 설치"
      ],
      "metadata": {
        "id": "-VtKxELDpkex"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NgoI1RZbpVgY"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "# !pip install loralib\n",
        "# !pip install trl\n",
        "# !pip install accelerate\n",
        "# !pip install transformers\n",
        "# !pip install evaluate\n",
        "#!pip install sacrebleu rouge-score\n",
        "# #실행 후 런타임 재시작"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mxk296_GtpLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 0. 환경 설정 / 라이브러리 임포트"
      ],
      "metadata": {
        "id": "RzLa_okSlERw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/airobotlab/KoChatGPT\n",
        "!cp -r KoChatGPT/colossalai_ChatGPT_230319/chatgpt chatgpt\n",
        "\n",
        "import os\n",
        "\n",
        "modifications = [\n",
        "    {\n",
        "        \"file\": \"chatgpt/trainer/callbacks/save_checkpoint.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 3, \"old\": \"from chatgpt.trainer.strategies import ColossalAIStrategy, Strategy\",\n",
        "             \"new\": \"from chatgpt.trainer.strategies import Strategy\"},\n",
        "            {\"line\": 71, \"old\": \"only_rank0 = not isinstance(self.strategy, ColossalAIStrategy)\",\n",
        "             \"new\": \"            only_rank0 = not isinstance(self.strategy)\"},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"chatgpt/trainer/strategies/__init__.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 1, \"old\": \"from .colossalai import ColossalAIStrategy\", \"new\": \"\"},  # 삭제\n",
        "            {\"line\": 5, \"old\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy', 'ColossalAIStrategy']\",\n",
        "             \"new\": \"__all__ = ['Strategy', 'NaiveStrategy', 'DDPStrategy']\"},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"chatgpt/dataset/reward_dataset.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 3, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"chatgpt/trainer/base.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"file\": \"chatgpt/trainer/rm.py\",\n",
        "        \"changes\": [\n",
        "            {\"line\": 8, \"old\": \"from tqdm import tqdm\", \"new\": \"from tqdm.notebook import tqdm\"},\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "#파일에서 지정된 줄을 찾아 내용을 수정\n",
        "def modify_file(file_path, changes):\n",
        "\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"⚠️ 파일이 존재하지 않습니다: {file_path}\")\n",
        "        return\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    modified = False\n",
        "\n",
        "    for change in changes:\n",
        "        line_index = change[\"line\"]\n",
        "        if 0 <= line_index < len(lines):\n",
        "            if lines[line_index].strip() == change[\"old\"]:\n",
        "                lines[line_index] = change[\"new\"] + \"\\n\"\n",
        "                modified = True\n",
        "            else:\n",
        "                print(f\"⚠️ {file_path} 파일의 {change['line']}번째 줄이 예상과 다릅니다.\")\n",
        "                print(f\"   예상: {change['old']}\")\n",
        "                print(f\"   실제: {lines[line_index].strip()}\")\n",
        "\n",
        "    if modified:\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.writelines(lines)\n",
        "        print(f\"✅ 수정 완료: {file_path}\")\n",
        "    else:\n",
        "        print(f\"⚠️ {file_path} 수정할 내용이 없습니다.\")\n",
        "\n",
        "for mod in modifications:\n",
        "    modify_file(mod[\"file\"], mod[\"changes\"])"
      ],
      "metadata": {
        "id": "rVUN4FKep34F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##라이브러리 버전 확인 및 dirve 연결\n",
        "학습에 필요한 autotokenizer, autoModelForCauseLM등을 import\n",
        "\n",
        "추후 원활한 수행을 위해 drive와 연결"
      ],
      "metadata": {
        "id": "c8kbgeeNleE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import numpy\n",
        "\n",
        "print(\"Torch version:{}\".format(torch.__version__)) # Torch version:1.12.1\n",
        "print(\"Cuda version: {}\".format(torch.version.cuda)) # Cuda version: 11.3\n",
        "print(\"transformers version: {}\".format(transformers.__version__)) # transformers 4.28.0\n",
        "print(\"GPU 사용 가능여부: {}\".format(torch.cuda.is_available()))\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 만일 아래 모듈이 불러와지지 않는다면 Clone 및 수정을 잘 진행했는지 확인해주세요.\n",
        "from chatgpt.trainer.strategies import NaiveStrategy"
      ],
      "metadata": {
        "id": "YlAfGYnIp-kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"skt/kogpt2-base-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "jNf3h6nHqC1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 1. 데이터 및 디코딩 성능 확인"
      ],
      "metadata": {
        "id": "f_fLyQI5tlY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_txt =  \"바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\"\n",
        "\n",
        "tokens = tokenizer(input_txt).tokens()\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()\n",
        "\n",
        "pd.options.display.max_columns = 40\n",
        "pd.options.display.max_rows = 60\n",
        "df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n",
        "df"
      ],
      "metadata": {
        "id": "Rs_G9IkwtlDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코딩 성능 확인\n",
        "max_length=128\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
        "print(tokenizer.decode(output_greedy[0]))"
      ],
      "metadata": {
        "id": "tG08wi-UtxXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "처음 생성한 답변을 반복적으로 수행하는 모습이 보인다.\n",
        "앞서 생성한 토큰의 영향을 많이 받음"
      ],
      "metadata": {
        "id": "oI6K-Sz43N8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
        "                             do_sample=True, top_p=0.90)\n",
        "print(tokenizer.decode(output_beam[0]))"
      ],
      "metadata": {
        "id": "yTgN3OeHt-cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "단순히 디코딩에서 beam search를 적용해주는것만으로 어느정도의 품질은 기대할 수 있지만\n",
        "모델 자체의 성능을 끌어올린다면 더 경제적으로 고품질의 출력을 만들 수 있을것이라 생각함.\n"
      ],
      "metadata": {
        "id": "9Gv4FrsxuImx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2. Supervised Fine-Tuning\n",
        "\n",
        "베이스 모델 : skt/kogpt2-base-v2\n",
        "\n",
        "데이터 : {prompt, completion} 데이터쌍\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BZANQvrnusk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Dict, Sequence\n",
        "from torch.utils.data import Dataset\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "import copy\n",
        "import json"
      ],
      "metadata": {
        "id": "7BceByiuuCAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델, 토크나이저 호출\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
        "    padding_side=\"right\",\n",
        "    model_max_length=512,\n",
        ")\n",
        "\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "trfs2OXUu0qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 인퍼런스 단계에서 사용할 prompt 딕셔너리 템플릿과 SFT 데이터셋 클래스를 정의\n",
        "\n",
        "class SFT_dataset(Dataset):\n",
        "\n",
        "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
        "        super(SFT_dataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "\n",
        "        pattern_instruction = 'prompt'  # instruction\n",
        "        pattern_output = 'completion'  # response\n",
        "\n",
        "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
        "            list_data_dict = json.load(json_file)\n",
        "\n",
        "        PROMPT_DICT = {\n",
        "            \"prompt_input\": (\n",
        "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
        "            )\n",
        "        }\n",
        "\n",
        "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
        "\n",
        "        sources = []\n",
        "        for example in list_data_dict:\n",
        "            tmp = prompt_input.format_map(example)\n",
        "            sources.append(tmp)\n",
        "\n",
        "        targets = []\n",
        "        for example in list_data_dict:\n",
        "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
        "        examples = [s + t for s, t in zip(sources, targets)]\n",
        "\n",
        "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
        "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
        "\n",
        "        input_ids = examples_tokenized[\"input_ids\"]\n",
        "        labels = copy.deepcopy(input_ids)\n",
        "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "            label[:source_len] = -100\n",
        "\n",
        "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
        "\n",
        "\n",
        "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "        tokenized_list = [\n",
        "            tokenizer(\n",
        "                text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"longest\",\n",
        "                max_length=tokenizer.model_max_length,\n",
        "                truncation=True,\n",
        "            )\n",
        "            for text in strings\n",
        "        ]\n",
        "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "        input_ids_lens = labels_lens = [\n",
        "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "        ]\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            input_ids_lens=input_ids_lens,\n",
        "            labels_lens=labels_lens,\n",
        "        )\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
      ],
      "metadata": {
        "id": "NyDxWdhGu3wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )"
      ],
      "metadata": {
        "id": "F0xfJLtCvuka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SFT_dataset 클래스를 사용해 훈련셋을 만들고 data collator 인스턴스 생성\n",
        "\n",
        "train_dataset = SFT_dataset(data_path_1_SFT='KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        "\n",
        "print('input : %s'%train_dataset.input_ids[0])\n",
        "print('output: %s'%train_dataset.labels[0])"
      ],
      "metadata": {
        "id": "nigQ2D2WvwhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코딩 함수 작성\n",
        "def decode_tokens(tokenizer, input_ids, label_ids):\n",
        "  \"\"\"\n",
        "  주어진 input_ids와 label_ids를 디코딩하여 원본 text를 출력하는 함수.\n",
        "  label_ids에 포함된 -100 값은 디코딩에서 제외합니다.\n",
        "  \"\"\"\n",
        "  # input_ids 디코딩\n",
        "  # skip_special_tokens=True 옵션으로 </s>와 같은 특수 토큰을 제외하고 볼 수 있습니다.\n",
        "  decoded_input = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "\n",
        "  # label_ids에서 -100을 제외한 토큰만 필터링\n",
        "  filtered_label_ids = [token_id for token_id in label_ids if token_id != -100]\n",
        "\n",
        "  # 필터링된 label_ids 디코딩\n",
        "  decoded_label = tokenizer.decode(filtered_label_ids, skip_special_tokens=True)\n",
        "\n",
        "  print(\"--- [디코딩 결과] ---\")\n",
        "  print(f\"➡️ Input (전체 원본 문장):\\n{decoded_input}\\n\")\n",
        "  print(f\"✅ Label (모델이 학습하는 정답 문장):\\n{decoded_label}\")\n",
        "\n",
        "# 함수를 사용하여 첫 번째 데이터 확인\n",
        "decode_tokens(\n",
        "    tokenizer,\n",
        "    train_dataset.input_ids[0],\n",
        "    train_dataset.labels[0]\n",
        ")"
      ],
      "metadata": {
        "id": "mkeq8p5MvyPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments를 사용해 trainer 클래스를 정의\n",
        "\n",
        "# training_args = transformers.TrainingArguments(\n",
        "#     output_dir=\"test\",\n",
        "#     overwrite_output_dir=True,\n",
        "#     num_train_epochs=1,\n",
        "#     per_device_train_batch_size=8,\n",
        "#     per_device_eval_batch_size=8,\n",
        "#     warmup_steps=5,\n",
        "#     prediction_loss_only=True,\n",
        "#     fp16 = True\n",
        "#     )\n",
        "training_args = transformers.TrainingArguments(\n",
        "        output_dir=\"models/improved_sft_v2\",\n",
        "        overwrite_output_dir=True,\n",
        "        num_train_epochs=3,  # 변경: 1 -> 3\n",
        "        per_device_train_batch_size=4,  # 변경: 8 -> 4 (메모리 절약)\n",
        "        gradient_accumulation_steps=4,  # 추가: 가상 배치 16\n",
        "        learning_rate=2e-5,  # 변경: 기본값 -> 낮춤\n",
        "        warmup_steps=100,  # 추가: 워밍업\n",
        "        prediction_loss_only=True,\n",
        "        fp16=True\n",
        "        )\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "njAfLJo1wYMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SFT 훈련을 진행\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained('/content/drive/MyDrive/model_output/output_1_SFT')"
      ],
      "metadata": {
        "id": "jXGDyUSzwNag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###모델 성능 평가"
      ],
      "metadata": {
        "id": "0MOTQL2_1pGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장 생성 능력을 확인하기 위해 빠르게 허깅페이스의 pipleline 클래스를 사용하여 generator 구현\n",
        "generator = transformers.pipeline('text-generation', model='models/output_1_SFT', tokenizer=tokenizer)\n",
        "\n",
        "generation_args = dict(\n",
        "    num_beams=4,\n",
        "    repetition_penalty=2.0,\n",
        "    no_repeat_ngram_size=4,\n",
        "    eos_token_id=375, # \\n\n",
        "    max_new_tokens=64,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
        "    )\n",
        "}\n",
        "\n",
        "list_prompt = ['불고기용 고기 한우에요?',\n",
        "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
        "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
        "               '오늘 미세먼지 어때?']\n",
        "\n",
        "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
        "\n",
        "list_result = generator(list_prompt, **generation_args)\n",
        "for prompt, result in zip(list_prompt, list_result):\n",
        "    print()\n",
        "    print((result[0]['generated_text']))"
      ],
      "metadata": {
        "id": "bEZz3ni4wRL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "저는 인공지능 어시스턴트이기 때문에 \"입력\"에 대한 정보를 가지고 있지 않습니다\n",
        "\n",
        "라는 프롬프트가 존재하는듯함"
      ],
      "metadata": {
        "id": "eg8-Dts92I9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 2. RM 학습\n",
        "Ranknig셋으로 준비된 데이터를\n",
        "\n",
        "pairwise형식으로 전환 후 학습시도\n",
        "\n"
      ],
      "metadata": {
        "id": "cq_Ur-S5cFvF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "irkAmJqVSeKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RM 학습에 필요한 클래스 임포트\n",
        "from chatgpt.dataset import RewardDataset\n",
        "from chatgpt.models.base import RewardModel\n",
        "from chatgpt.trainer.strategies import NaiveStrategy\n",
        "from chatgpt.trainer.rm import RewardModelTrainer\n",
        "\n",
        "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
        "\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "# SFT와 동일한 토크나이저 설정 사용\n",
        "rm_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
        "    padding_side=\"right\",\n",
        "    model_max_length=512,\n",
        ")"
      ],
      "metadata": {
        "id": "zsIoA5nBcQaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTRM_custom(RewardModel):\n",
        "    \"\"\"\n",
        "    GPT-2를 기반으로 하는 Custom Reward Model.\n",
        "    입력된 텍스트의 좋고 나쁨을 판단하여 단일 점수(reward)를 출력합니다.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 pretrained: Optional[str] = None,\n",
        "                 config: Optional[GPT2Config] = None,\n",
        "                 checkpoint: bool = False,\n",
        "                 lora_rank: int = 0,\n",
        "                 lora_train_bias: str = 'none',\n",
        "                 tokenizer=None) -> None:\n",
        "\n",
        "        if pretrained is not None:\n",
        "            # 사전 학습된 GPT2 모델을 불러옵니다.\n",
        "            model = GPT2Model.from_pretrained(pretrained)\n",
        "            # Special token이 추가된 토크나이저에 맞게 임베딩 크기를 조정합니다.\n",
        "            model.resize_token_embeddings(len(tokenizer))\n",
        "        elif config is not None:\n",
        "            model = GPT2Model(config)\n",
        "        else:\n",
        "            model = GPT2Model(GPT2Config())\n",
        "\n",
        "        if checkpoint:\n",
        "            model.gradient_checkpointing_enable()\n",
        "\n",
        "        # 모델의 마지막 hidden state를 입력으로 받아 단일 점수를 출력하는 value_head를 정의합니다.\n",
        "        value_head = nn.Linear(model.config.n_embd, 1)\n",
        "\n",
        "        # 부모 클래스인 RewardModel을 초기화합니다.\n",
        "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
        "\n",
        "        if pretrained is not None:\n",
        "            self.model = model\n",
        "            self.pretrained = pretrained\n",
        "\n",
        "    def save_pretrained(self, dir):\n",
        "        if self.pretrained is not None:\n",
        "            self.model.save_pretrained(dir)\n",
        "\n",
        "# NaiveStrategy 컨텍스트 내에서 RM 모델을 초기화합니다.\n",
        "with NaiveStrategy().model_init_context():\n",
        "    rm_model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=rm_tokenizer).cuda()"
      ],
      "metadata": {
        "id": "t_zqgljXe6Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 준비된 RM 데이터셋 로드\n",
        "with open('/content/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "\n",
        "# 랭킹 정보를 바탕으로 pairwise(chosen, rejected) 쌍 만들기\n",
        "total_data_ranking2chosen = []\n",
        "for tmp in list_data_dict:\n",
        "    one_data_ranking2chosen = []\n",
        "\n",
        "    # 3개의 답변 중 2개를 뽑는 모든 조합(3가지)에 대해 쌍을 생성\n",
        "    # completion_0 vs completion_1\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # completion_0 vs completion_2\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_0']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_0']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    # completion_1 vs completion_2\n",
        "    data = {}\n",
        "    data['prompt'] = tmp['prompt']\n",
        "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
        "        data['chosen'] = tmp['completion_1']\n",
        "        data['rejected'] = tmp['completion_2']\n",
        "    else:\n",
        "        data['chosen'] = tmp['completion_2']\n",
        "        data['rejected'] = tmp['completion_1']\n",
        "    one_data_ranking2chosen.append(data)\n",
        "\n",
        "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
        "\n",
        "print('before data num: %d'%(len(list_data_dict)))\n",
        "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
        "print('data example: \\n%s'%total_data_ranking2chosen[45])"
      ],
      "metadata": {
        "id": "nz8g2uKTf09U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 셔플 및 분할\n",
        "random.seed(230319)\n",
        "random.shuffle(total_data_ranking2chosen)\n",
        "\n",
        "train_data = total_data_ranking2chosen[:1000]\n",
        "eval_data = total_data_ranking2chosen[1000:1200]\n",
        "\n",
        "print(f\"Train data size: {len(train_data)}\")\n",
        "print(f\"Eval data size: {len(eval_data)}\")\n",
        "\n",
        "# RewardDataset 객체 생성\n",
        "train_dataset = RewardDataset(train_data, rm_tokenizer, 512)\n",
        "eval_dataset = RewardDataset(eval_data, rm_tokenizer, 512)\n",
        "\n",
        "# 데이터 샘플 확인\n",
        "idx = 1\n",
        "print('#'*70)\n",
        "print('## prompt ##')\n",
        "print(train_data[idx]['prompt'])\n",
        "print('#'*70)\n",
        "print('## chosen ##')\n",
        "print(train_data[idx]['chosen'])\n",
        "print('#'*70)\n",
        "print('## rejected ##')\n",
        "print(train_data[idx]['rejected'])"
      ],
      "metadata": {
        "id": "LXspb2rCf7fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RM 트레이너 설정\n",
        "rm_trainer = RewardModelTrainer(model=rm_model,\n",
        "                             strategy=NaiveStrategy(),\n",
        "                             optim=torch.optim.Adam(rm_model.parameters(), lr=5e-5),\n",
        "                             train_dataset=train_dataset,\n",
        "                             eval_dataset=eval_dataset,\n",
        "                             batch_size=4,\n",
        "                             max_epochs=1)\n",
        "\n",
        "# RM 학습 시작\n",
        "rm_trainer.fit(use_lora=0)\n",
        "\n",
        "# 학습된 RM 모델 저장\n",
        "rm_model.save_pretrained('/content/drive/MyDrive/model_output/output_2_RM')"
      ],
      "metadata": {
        "id": "p5LGYyfSk5A8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RM 추론 함수 정의\n",
        "def inference_RM(input_text):\n",
        "    input_ids = rm_tokenizer.encode(input_text, return_tensors='pt').to(\n",
        "        torch.cuda.current_device()\n",
        "    )\n",
        "    output = rm_model(input_ids)\n",
        "    output_reward = output.cpu().detach().numpy()[0]\n",
        "\n",
        "    print('input: %s\\nreward score: %.1f' % (input_text, output_reward))\n",
        "    return output_reward\n",
        "\n",
        "# 테스트 1: 부정적인 문장\n",
        "input_text = '인공지능은 똥멍청이 입니다'\n",
        "output_reward = inference_RM(input_text=input_text)\n",
        "\n",
        "# 테스트 2: 긍정적이고 짧은 문장\n",
        "input_text = '인공지능(AI)은 매우 유용합니다.'\n",
        "output_reward = inference_RM(input_text=input_text)\n",
        "\n",
        "# 테스트 3: 긍정적이고 상세한 문장\n",
        "input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다.\"\n",
        "output_reward = inference_RM(input_text=input_text)\n",
        "\n",
        "# GPU 메모리 정리\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8N022e-gk88L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Step 4. PPO 적용\n",
        "\n",
        "SFT, RM모델을 이용해 PPO를 수행\n"
      ],
      "metadata": {
        "id": "ce7OUNBEmmEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
        "from chatgpt.trainer import PPOTrainer\n",
        "\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "mlbLLjvwmcZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NaiveStrategy 컨텍스트 내에서 PPO 학습에 필요한 모든 모델을 준비합니다.\n",
        "with NaiveStrategy().model_init_context():\n",
        "    # Actor: SFT 모델을 불러옵니다.\n",
        "    actor = GPTActor(pretrained='/content/drive/MyDrive/model_output/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
        "\n",
        "    # Critic: RM 모델을 불러옵니다.\n",
        "    critic = GPTCritic(pretrained='/content/drive/MyDrive/model_output/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
        "\n",
        "    # Tokenizer: 이전과 동일한 설정을 사용합니다.\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
        "        padding_side=\"right\",\n",
        "        model_max_length=512\n",
        "    )\n",
        "\n",
        "    # Initial Model: SFT 모델을 복사하여 KL 페널티 계산에 사용합니다.\n",
        "    initial_model = deepcopy(actor)\n",
        "\n",
        "    # Reward Model: Critic 모델을 기반으로 보상 계산에 사용합니다.\n",
        "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())\n",
        "\n",
        "# Actor와 Critic을 위한 옵티마이저를 설정합니다.\n",
        "actor_optim = torch.optim.Adam(actor.parameters(), lr=5e-6)\n",
        "critic_optim = torch.optim.Adam(critic.parameters(), lr=5e-6)\n",
        "\n",
        "# Strategy를 통해 모델과 옵티마이저를 래핑합니다.\n",
        "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
        "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
      ],
      "metadata": {
        "id": "-xBee6Cnm0IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PPO 학습을 위한 프롬프트 데이터 로드\n",
        "with open('//content/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
        "    list_data_dict = json.load(json_file)\n",
        "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
        "\n",
        "# PPO Trainer 내부에서 사용할 토크나이저 함수 정의\n",
        "def tokenize_fn(texts):\n",
        "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
        "    return {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "print(f\"PPO 학습에 사용될 프롬프트 개수: {len(list_prompt)}\")"
      ],
      "metadata": {
        "id": "R-W-ui73m1Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PPOTrainer 초기화\n",
        "trainer = PPOTrainer(NaiveStrategy(),\n",
        "                     actor,           #학습이 진행될 모델\n",
        "                     critic,          #RM\n",
        "                     reward_model,    #RM\n",
        "                     initial_model,   #SFT적용모델\n",
        "                     actor_optim,\n",
        "                     critic_optim,\n",
        "                     max_epochs=1,\n",
        "                     train_batch_size=8,\n",
        "                     tokenizer=tokenize_fn,\n",
        "                     max_length=128,\n",
        "                     do_sample=True,\n",
        "                     temperature=1.0,\n",
        "                     top_k=50,\n",
        "                     pad_token_id=tokenizer.pad_token_id,\n",
        "                     eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# PPO 학습 시작\n",
        "trainer.fit(list_prompt,\n",
        "            num_episodes=10,\n",
        "            max_timesteps=3,\n",
        "            update_timesteps=3)\n",
        "\n",
        "# 최종 PPO 모델 저장\n",
        "actor.model.save_pretrained('/content/drive/MyDrive/model_output/output_3_PPO')"
      ],
      "metadata": {
        "id": "xE2fEs3Loi3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 PPO 모델로 답변 생성\n",
        "def generation(input_text, model, tokenizer):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
        "        torch.cuda.current_device())\n",
        "\n",
        "    # PPO 모델은 Actor 클래스로 래핑되어 있으므로, 내부 모델을 직접 사용\n",
        "    outputs = model.model.generate(input_ids,\n",
        "                             max_length=250,\n",
        "                             do_sample=True,\n",
        "                             top_k=50,\n",
        "                             top_p=0.95,\n",
        "                             num_return_sequences=1)\n",
        "\n",
        "    output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "    print(output_text)\n",
        "    return output_text\n",
        "\n",
        "# 테스트용 프롬프트 리스트\n",
        "list_prompt = [\n",
        "    '불고기용 고기 한우에요?',\n",
        "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
        "    '시카고 오헤어 국제공항은 어디에 있어',\n",
        "    '오늘 미세먼지 어때?']\n",
        "\n",
        "# SFT/PPO 모델에 맞는 프롬프트 형식으로 변환\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
        "    )\n",
        "}\n",
        "list_formatted_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
        "\n",
        "print(\"--- PPO Model Generation Results ---\")\n",
        "for input_text in list_formatted_prompt:\n",
        "    generation(input_text, actor, tokenizer)\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "WXllsoUXokgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#실험 결과 확인\n",
        "pipeline을 이용해 출력을 생성"
      ],
      "metadata": {
        "id": "HfOFch9OplFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer\n",
        "import torch, re\n",
        "\n",
        "generators = {\n",
        "    name: pipeline(\n",
        "        \"text-generation\",\n",
        "        model=path,\n",
        "        tokenizer=tokenizer,\n",
        "        device=device\n",
        "    )\n",
        "    for name, path in MODEL_PATHS.items()\n",
        "}\n",
        "print(\"ready:\", list(generators.keys()))\n",
        "\n",
        "#프롬프트 템플릿 + 종료 처리\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\",\n",
        "}\n",
        "def build_prompt(p: str) -> str:\n",
        "    return PROMPT_DICT[\"prompt_input\"].format_map({\"prompt\": p})\n",
        "\n",
        "# 종료 규칙: 응답 이후 불필요한 텍스트를 잘라주는 간단 후처리\n",
        "def postprocess(prompt_text: str, generated_text: str) -> str:\n",
        "    # 1) 프롬프트 에코 제거\n",
        "    body = generated_text[len(prompt_text):] if generated_text.startswith(prompt_text) else generated_text\n",
        "\n",
        "    # 2) \"다음 섹션\"을 암시하는 마커가 보이면 거기서 자르기 (옵션)\n",
        "    cut_points = [\n",
        "        \"\\n\\n###\",             # 다음 섹션 마커\n",
        "        \"\\n\\nInstruction\",     # 잘못 에코 시\n",
        "        \"\\n\\n명령어\",           # 한글 에코 시\n",
        "    ]\n",
        "    for cp in cut_points:\n",
        "        if cp in body:\n",
        "            body = body.split(cp)[0]\n",
        "            break\n",
        "\n",
        "    # 3) 과도한 공백 정리\n",
        "    body = body.strip()\n",
        "    return body\n",
        "\n",
        "# 종료 토큰: 개행을 EOS로 쓰고 싶다면 토크나이저에서 id 찾기\n",
        "EOS_NL = tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "t13-l_5o1sk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GEN_ARGS_GREEDY = dict(\n",
        "    do_sample=False,\n",
        "    num_beams=4,\n",
        "    max_new_tokens=96,\n",
        "    no_repeat_ngram_size=4,\n",
        "    repetition_penalty=1.2,\n",
        "    eos_token_id=EOS_NL,                  # 응답 끝나면 개행에서 끊기\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    early_stopping=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "c5DMKvsQ2DY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_prompt = [\n",
        "    \"불고기용 고기 한우에요?\",\n",
        "    \"리처드 닉슨이 43대 부통령직을 수행한 년도는?\",\n",
        "    \"시카고 오헤어 국제공항은 어디에 있어?\",\n",
        "    \"오늘 미세먼지 어때?\"\n",
        "]\n",
        "batched_inputs = [build_prompt(p) for p in list_prompt]\n",
        "\n",
        "def run_and_show(generators, batched_inputs, gen_args):\n",
        "    # 각 모델을 순회하며 배치 생성\n",
        "    all_outputs = {name: gen(batched_inputs, **gen_args) for name, gen in generators.items()}\n",
        "\n",
        "    # 프롬프트별로 묶어서 출력\n",
        "    for i, raw_prompt in enumerate(list_prompt):\n",
        "        full_prompt = batched_inputs[i]\n",
        "        print(\"=\"*100)\n",
        "        print(f\"PROMPT {i+1}: {raw_prompt}\")\n",
        "        for name in [\"base\", \"sft\", \"ppo\"]:\n",
        "            res_text = all_outputs[name][i][0][\"generated_text\"]\n",
        "            ans = postprocess(full_prompt, res_text)\n",
        "            print(f\"\\n[{name.upper()}]\\n{ans}\\n\")\n",
        "\n",
        "#  그리디 비교\n",
        "run_and_show(generators, batched_inputs, GEN_ARGS_GREEDY)"
      ],
      "metadata": {
        "id": "-X7rFdMy2Frk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "베이스 모델에서 sft, PPO를 거칠수록 점점 답변이 깨끗해지는걸 볼 수 있다.\n"
      ],
      "metadata": {
        "id": "BTvO1HME2Qto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------\n",
        "--------------------------------------\n",
        "\n",
        "#Beam search 적용\n",
        "--------------------------------------\n",
        "--------------------------------------\n"
      ],
      "metadata": {
        "id": "jBtvPXHa4H8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 개행 EOS는 이전에 만든 EOS_NL 사용(없으면 아래 줄로 구하세요)\n",
        "# EOS_NL = tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
        "\n",
        "GEN_ARGS_BEAM = dict(\n",
        "    do_sample=True,          # 🔒 순수 빔서치 (랜덤성 X)\n",
        "    num_beams=7,\n",
        "    max_new_tokens=96,\n",
        "    no_repeat_ngram_size=4,\n",
        "    repetition_penalty=1.2,\n",
        "    eos_token_id=EOS_NL,      # 응답 끝을 개행으로 유도\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    early_stopping=True,\n",
        ")\n",
        "\n",
        "# 실행\n",
        "run_and_show(generators, batched_inputs, GEN_ARGS_BEAM)"
      ],
      "metadata": {
        "id": "XqTjG5J04NvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beam Search를 적용한 모델의 답변의 어휘가 기존에 비해 편해졌다.\n",
        "적용하지 않은 출력에서는 기존에 주어진 단어들을 우선적으로 선택해 어투가 딱딱한 느낌이 든 반면\n",
        "Beam Search를 적용한 출력에서는 좀 더 다양한 어휘를 보이며 자연스러운 출력을 보였다."
      ],
      "metadata": {
        "id": "IU86ofUs5zRX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###EX 01. 추가 데이터 정제\n",
        "\n",
        "주말사이에 시도해보겠습니다."
      ],
      "metadata": {
        "id": "lM5IN2L5kkSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ExFM7_PCwg6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pmnEocV_zyvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkhWsYln484p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UdNOUrx3G2vA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}