{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28cb821e-b2c1-4dbe-8b28-99428d5b2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac33336c-5a0e-4a4f-bad0-fff4b961dd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/home/jovyan/work/weat/data/GoogleNews-vectors-negative300.bin': File exists\n",
      "ln: failed to create symbolic link '/home/jovyan/work/weat/data/GoogleNews-vectors-negative300.bin.gz': File exists\n",
      "ln: failed to create symbolic link '/home/jovyan/work/weat/data/synopsis.zip': File exists\n",
      "Archive:  /home/jovyan/work/weat/data/synopsis.zip\n",
      "replace /home/jovyan/work/weat/data/synopsis.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ~/work/weat/data\n",
    "!ln -s ~/data/* ~/work/weat/data/\n",
    "!unzip ~/work/weat/data/synopsis.zip -d ~/work/weat/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033b76f1-cb1d-4d7c-b803-23832ecbcbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==4.3.2 in /opt/conda/lib/python3.12/site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.12/site-packages (from gensim==4.3.2) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.12/site-packages (from gensim==4.3.2) (1.12.0)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /opt/conda/lib/python3.12/site-packages (from gensim==4.3.2) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.12/site-packages (from smart_open>=1.8.1->gensim==4.3.2) (1.17.3)\n",
      "Requirement already satisfied: scipy==1.12.0 in /opt/conda/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy==1.26.2 in /opt/conda/lib/python3.12/site-packages (1.26.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==4.3.2\n",
    "!pip install scipy==1.12.0 numpy==1.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b7d31e-c8a2-4d6f-aa59-c808384dfc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /opt/conda/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.12/site-packages (from konlpy) (1.26.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu noble InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu noble-security InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu noble-updates InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu noble-backports InRelease\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "openjdk-17-jdk is already the newest version (17.0.16+8~us1-0ubuntu1~24.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
      "Requirement already satisfied: konlpy in /opt/conda/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /opt/conda/lib/python3.12/site-packages (from konlpy) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.6 in /opt/conda/lib/python3.12/site-packages (from konlpy) (1.26.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy\n",
    "!sudo apt update\n",
    "!sudo apt install openjdk-17-jdk -y\n",
    "!echo 'export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))' >> ~/.bashrc\n",
    "!source ~/.bashrc\n",
    "\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8c255c-4fb2-45e4-8b95-723c1198dc3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš´ë“œ ì—”ì§€ë‹ˆì–´ ìƒìš°(ìœ ì§€íƒœ ë¶„)ëŠ” ì¹˜ë§¤ì— ê±¸ë¦° í• ë¨¸ë‹ˆ(ë°±ì„±í¬ ë¶„)ì™€\n",
      " ì Šì€ ì‹œì ˆ ìƒì²˜í•œ í•œ ì•„ë²„ì§€(ë°•ì¸í™˜ ë¶„), ê³ ëª¨(ì‹ ì‹ ì•  ë¶„)ì™€ í•¨ê»˜ ì‚´ê³  ìˆë‹¤.\n",
      " ì–´ëŠ ê²¨ìš¸ ê·¸ëŠ” ì§€ë°© ë°©ì†¡êµ­ ë¼ë””ì˜¤ PD ì€ìˆ˜(ì´ì˜ì•  ë¶„)ë¥¼ ë§Œë‚œë‹¤.\n",
      " ìì—°ì˜ ì†Œë¦¬ë¥¼ ì±„ì§‘í•´ í‹€ì–´ì£¼ëŠ” ë¼ë””ì˜¤ í”„ë¡œê·¸ë¨ì„ ì¤€ë¹„í•˜ëŠ” ì€ìˆ˜ëŠ” ìƒìš°ì™€ ë…¹ìŒ ì—¬í–‰ì„ ë– ë‚œë‹¤.\n",
      " ìì—°ìŠ¤ë ˆ ê°€ê¹Œì›Œì§€ëŠ” ë‘ ì‚¬ëŒì€ ì–´ëŠ ë‚ , ì€ìˆ˜ì˜ ì•„íŒŒíŠ¸ì—ì„œ ë°¤ì„ ë³´ë‚¸ë‹¤.\n",
      " ë„ˆë¬´ ì‰½ê²Œ ì‚¬ë‘ì— ë¹ ì§„ ë‘ ì‚¬ëŒ... ìƒìš°ëŠ” ì£¼ì²´í•  ìˆ˜ ì—†ì„ ì •ë„ë¡œ ê·¸ë…€ì—ê²Œ ë¹¨ë ¤ë“ ë‹¤.\n",
      " ê·¸ëŸ¬ë‚˜ ê²¨ìš¸ì— ë§Œë‚œ ë‘ ì‚¬ëŒì˜ ê´€ê³„ëŠ” ë´„ì„ ì§€ë‚˜ ì—¬ë¦„ì„ ë§ì´í•˜ë©´ì„œ ì‚ê±±ê±°ë¦°ë‹¤.\n",
      " ì´í˜¼ ê²½í—˜ì´ ìˆëŠ” ì€ìˆ˜ëŠ” ìƒìš°ì—ê²Œ ê²°í˜¼í•  ìƒê°ì´ ì—†ë‹¤ë©° ë¶€ë‹´ìŠ¤ëŸ¬ìš´ í‘œì •ì„ ë‚´ë¹„ì¹œë‹¤.\n",
      " \"ì–´ë–»ê²Œ ì‚¬ë‘ì´ ë³€í•˜ë‹ˆ?...\"ë¼ê³  ë¬»ëŠ” ìƒìš°ì—ê²Œ ì€ìˆ˜ëŠ” ê·¸ì € \"í—¤ì–´ì ¸\" ë¼ê³  ë‹¨í˜¸í•˜ê²Œ ë§í•œë‹¤.\n",
      " ì˜ì›íˆ ë³€í•  ê²ƒ ê°™ì§€ ì•Šë˜ ì‚¬ë‘ì´ ë³€í•˜ê³ , ê·¸ ì‚¬ì‹¤ì„ ë°›ì•„ë“¤ì´ì§€ ëª»í•˜ëŠ” ìƒìš°ëŠ” ì–´ì°Œ í•  ë°”ë¥¼ ëª¨ë¥¸ë‹¤.\n",
      " ì€ìˆ˜ë¥¼ ìŠì§€ ëª»í•˜ëŠ” ìƒìš°ëŠ” ë¯¸ë ¨ê³¼ ì§‘ì°©ì˜ ê°ì •ì„ ì´ê¸°ì§€ ëª»í•˜ê³  ì„œìš¸ê³¼ ê°•ë¦‰ì„ ì˜¤ê°„ë‹¤.\n",
      "ìœ ì‚¬ ì´ë˜ ì—°ë ¹, ì„±ë³„, ë¹ˆë¶€ì˜ ì°¨ì´ì™€ ì •ì¹˜ì ì¸ ì…ì¥ì„ ë¶ˆë¬¸í•˜ê³  ì¼ê±°ì— êµ­ë¯¼ì„ í†µí•©í•´ ì˜¨ 'ì• êµ­ì‹¬'ì´ë¼ëŠ” ì„±ì—­ì— ì¼ì¹¨ì„ ê°€í•˜ëŠ” ë‹¤íë©˜í„°ë¦¬. ì¬ì‘ë…„ ì „êµ­ ë¯¼ì¡±ë¯¼ì£¼ ìœ ê°€ì¡±í˜‘ì˜íšŒì˜ ì¥ê¸°ë†ì„±ì„ ë‹¤ë£¬ ì¸ìƒì ì¸ ë‹¤íë©˜í„°ë¦¬ <ë¯¼ë“¤ë ˆ>ë¥¼ ë§Œë“¤ì—ˆë˜ ë…ë¦½ì˜í™”ì§‘ë‹¨ 'ë¹¨ê°„ ëˆˆì‚¬ëŒ'ì´ ìš°ë¦¬ ì‚¬íšŒ êµ¬ì„êµ¬ì„ì„ ë°œë¹ ë¥´ê²Œ ëŒì•„ë‹¤ë‹ˆë©° ì• êµ­ì‹¬ê³¼ ë¯¼ì¡±ì£¼ì˜ê°€ ê°•ìš”ë˜ëŠ” í˜„ì¥ì„ ë°œêµ´í•˜ì—¬ ì¹´ë©”ë¼ì— ë‹´ì•˜ë‹¤. ë°•í™ ì„œê°•ëŒ€ ëª…ì˜ˆì´ì¥, ì´ë„í˜• 'í•œêµ­ë…¼ë‹¨' ë°œí–‰ì¸, ì¶•êµ¬í•´ì„¤ì ì‹ ë¬¸ì„ , í™ì„¸í™”, ë°•ë…¸í•´ ë“± ì‚¬íšŒ ê°ê³„ì˜ 'ìŠ¤íƒ€'ë“¤ì´ ë“±ì¥í•´ ì €ë§ˆë‹¤ì˜ í™•ê³ í•œ ì‹ ë…ì„ ì„±í† í•œë‹¤. ê°ë… ì´ê²½ìˆœê³¼ ìµœí•˜ë™í•˜ëŠ” ì´ ì‘í’ˆì„ ìœ„í•´ 3ë…„ê°„ ë°±ì—¬ ëª…ì„ ì¸í„°ë·°í–ˆë‹¤ê³  í•œë‹¤. 2001 ì˜¬í•´ì˜ ë…ë¦½ì˜í™”ìƒ ìˆ˜ìƒ.\n",
      " ë¯¼ì¡±ê³¼ êµ­ê°€ë€ ê³µë™ì²´ì—ì„œ ë¶€ë‹¨íˆ ê¶Œë ¥ê³¼ ë¶€ë¥¼ ì–»ëŠ” ì, ë‚˜ì•„ê°€ ë¯¼ì¡±ê³¼ êµ­ê°€ë€ ê³µë™ì²´ì—ì„œ ì–»ì€ ì‹ ë¶„ê³¼ ë¶€ê·€ë¥¼ ì˜ì›íˆ ê·¸ì˜ ìì†ì—ê²Œ ëŒ€ë¬¼ë¦¼í•˜ë ¤ëŠ” ì, ê·¸ë˜ì„œ ë¯¼ì¡±ê³¼ êµ­ê°€ë€ ê³µë™ì²´ë¥¼ ë¶€ë‹¨íˆ ìœ ì§€í•´ì•¼ë§Œ í•˜ëŠ” ì, ë”°ë¼ì„œ ë¯¼ì¡±ê³¼ êµ­ê°€ë€ ê³µë™ì²´ì˜ ë‹¹ìœ„ì„±ê³¼ ê°œì¸ì˜ ê°€ì¹˜ë¥¼ ì´ˆì›”í•˜ëŠ” ê·¸ ì¡´ì—„ì„±ì„ ëë„ ì—†ì´ ì°½ì¡°í•˜ê³  ë˜ë‡Œì–´ì•¼ í•˜ëŠ” ì, ì¢…êµ­ì—ëŠ” ë¯¼ì¡±ê³¼ êµ­ê°€ë€ ê³µë™ì²´ì— ì†í•´ ìˆë‹¤ê³  íƒœë‚´ì—ì„œë¶€í„° ì„¸ë‡Œëœ ëª¨ë“  ì´ë“¤ì˜ ì‚¶ê³¼ í–‰ë™ì—ì„œ ì˜ì›íˆ ìê¸°ë³µì œë˜ëŠ” ìˆœí™˜ì˜ ê³ ë¦¬, ì˜ìƒí•˜ëŠ” ì• êµ­ì˜ ì›ë™ë ¥ì€ ê·¸ ìˆœí™˜ì˜ ê³¨ì—ì„œ ì˜¨ë‹¤.\n",
      "ì—½ê¸°ì ì¸ ì‚´ì¸ì‚¬ê±´ì´ ë°œìƒí•œ ì¥ì†Œë¥¼ ê´€ê´‘í•˜ëŠ” íˆ¬ì–´íŒ€. ê·¸ íŒ€ì—ì„œ ê´€ê´‘ê°ë“¤ì€ ì‚´ì¸ì‚¬ê±´ê³¼ ê´€ë ¨í•˜ì—¬ íˆìŠ¤í…Œë¦¬ì»¬í•œ ë°˜ì‘ì„ ë³´ì´ëŠ”ë° ê³¼ì—° ì´ë“¤ì˜ ì •ì²´ëŠ”? (Tourists see whrer a murder take place. They respond hysterically to the murderâ€¦what are they?)\n",
      " ì œ46íšŒ ë°œë¼ëŒë¦¬ë“œ êµ­ì œì˜í™”ì œ (2001, ìŠ¤í˜ì¸)\n",
      "ì°©í•˜ì§€ë§Œ ì—‰ëš±í•œ íƒœí¬(ë°°ë‘ë‚˜ ë¶„), ì˜ˆìœ ê¹ìŸì´ í˜œì£¼(ì´ìš”ì› ë¶„), ê·¸ë¦¼ì„ ì˜ ê·¸ë¦¬ëŠ” ì§€ì˜(ì˜¥ì§€ì˜ ë¶„), ëª…ë‘í•œ ìŒë‘¥ì´ ë¹„ë¥˜(ì´ì€ì‹¤ ë¶„)ì™€ ì˜¨ì¡°(ì´ì€ì£¼ ë¶„)ëŠ” ë‹¨ì§ì¹œêµ¬ë“¤. ëŠ˜ í•¨ê»˜ì˜€ë˜ ê·¸ë“¤ì´ì§€ë§Œ ìŠ¤ë¬´ ì‚´ì´ ë˜ë©´ì„œ ê¸¸ì´ ë‹¬ë¼ì§„ë‹¤. ì¦ê¶ŒíšŒì‚¬ì— ì…ì‚¬í•œ í˜œì£¼ëŠ” ì„±ê³µí•œ ì»¤ë¦¬ì–´ìš°ë¨¼ì˜ ì•¼ì‹¬ì„ í‚¤ìš°ê³  ë¯¸ìˆ ì— ì¬ëŠ¥ì´ ìˆëŠ” ì§€ì˜ì€ ìœ í•™ì„ ê¿ˆê¾¼ë‹¤. í•œí¸ íƒœí¬ëŠ” ë´‰ì‚¬í™œë™ì—ì„œ ì•Œê²Œ ëœ ë‡Œì„±ë§ˆë¹„ ì‹œì¸ì„ ì¢‹ì•„í•˜ëŠ”ë°...\n",
      "  ì–´ëŠ ë‚  ì§€ì˜ì´ ê¸¸ ìƒì€ ìƒˆë¼ ê³ ì–‘ì´ í‹°í‹°ë¥¼ ë§Œë‚¨ë©´ì„œ ìŠ¤ë¬´ ì‚´ ê·¸ë…€ë“¤ì˜ ì‚¶ì— ê³ ì–‘ì´ í•œ ë§ˆë¦¬ê°€ ë¼ì–´ë“¤ê²Œ ëœë‹¤. í˜¼ì ìˆê¸¸ ì¢‹ì•„í•˜ê³ , ì‰½ê²Œ ë§ˆìŒì„ ì—´ì§€ ì•ŠëŠ” ì‹ ë¹„ë¡œìš´ ë™ë¬¼ ê³ ì–‘ì´. ê³ ì–‘ì´ë¥¼ ë‹®ì€ ìŠ¤ë¬´ ì‚´ ê·¸ë…€ë“¤. ê³ ì–‘ì´ í‹°í‹°ì™€ í•¨ê»˜ í•œ ì‹œê°„ë™ì•ˆ ì‚¶ì€ ì˜ˆìƒëª»í•œ ë°©í–¥ìœ¼ë¡œ í˜ëŸ¬ê°€ì§€ë§Œ ë§ˆì¹¨ë‚´ ê·¸ë…€ë“¤ë§Œì˜ í•´ê²°ì±…ì„ ì°¾ê²Œ ë˜ëŠ”ë°... ì‚¬ë‘ìŠ¤ëŸ° ëª½ìƒê°€ íƒœí¬, ì•„ë¦„ë‹¤ìš´ ì•¼ì‹¬ê°€ í˜œì£¼, ì‹ ë¹„ë¡œìš´ ì•„ì›ƒì‚¬ì´ë” ì§€ì˜. ë§ˆì§€ë§‰ìœ¼ë¡œ ê³ ì–‘ì´ë¥¼ ë¶€íƒë°›ì€ ì‚¬ëŒì€ ëˆ„êµ¬ì¼ê¹Œ?\n",
      "ì¸ë„ ë“± ì•„ì‹œì•„ ì‹ë¯¼ì§€ì— ì²˜ìŒ ë°œì„ ë””ë”˜ ë’¤ ì—¬í–‰í•˜ê³  â€œê²½ì˜â€í•œ ì´ë“¤ì€ ê³¼ì—° ëˆ„êµ¬ì˜€ì„ê¹Œ? ê³¼ê±°ì˜ ì´ë¯¸ì§€ë“¤ì€, ì´ë¯¼ê³¼ ì¸ì¢… ë¬¸ì œ, â€˜ì˜¤ë¦¬ì—”íƒˆë¦¬ì¦˜â€™ì´ ê²©ë ¬íˆ ì¶©ëŒí•˜ê³  ìˆëŠ” í˜„ì¬ì™€ ê°•ë ¥í•˜ê²Œ ê³µëª…í•œë‹¤.\n",
      " [ì œ19íšŒ ì¸ë””ë‹¤íí˜ìŠ¤í‹°ë°œ]\n",
      "í™€ë¡œ ì‚´ì•„ê°€ëŠ” ë¯¸êµ­ í• ë¨¸ë‹ˆì™€ í•œêµ­ í• ë¨¸ë‹ˆì˜ ì´ì•¼ê¸°. ê³µì›ì—ì„œ ê°€ë” ë§ˆì£¼ì¹˜ê²Œ ë˜ëŠ” ê·¸ë“¤ì€ ë¹„ë¡ ì–¸ì–´ ì†Œí†µì˜ ì–´ë ¤ì›€ì„ ê²ªì§€ë§Œ ì‹œê°„ì´ í˜ëŸ¬ê°ì— ë”°ë¼ ì„œë¡œ ê°€ê¹Œì›Œì ¸ ê·¸ë“¤ì˜ ì™¸ë¡œì›€ê³¼ ìš°ì •ì„ ê³µìœ í•˜ê²Œ ëœë‹¤. ê²¨ìš¸ì´ ì§€ë‚˜ê³  ë´„ì´ ì™”ì„ ë•Œ ê¸¸ê°€ì˜ ë¯¼ë“¤ë ˆ í™€ì”¨ëŠ” ì‚¶ì˜ ì´ì¹˜ë¥¼ ë§í•´ì£¼ë“¯ í•œ í• ë¨¸ë‹ˆì˜ ì£¼ìœ„ë¥¼ ë§´ëˆë‹¤. (Two elderly widows, an American and a Korean, frequent the same park in Philadelphia and attempt a friendship, though the Korean widow speaks no English. Driven by loneliness and a spark of hope, they persevere within the limits of body language, and the outcome poses a question of life as fundamental as a flower.)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.path.join(os.getenv(\"HOME\"), \"work/weat/data\")\n",
    "file_name = os.path.join(data_dir, \"synopsis.txt\")\n",
    "\n",
    "with open(file_name, 'r') as file:\n",
    "    for i in range(20):\n",
    "        print(file.readline(), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f426e839-2d96-4be4-826a-6b03ba5b87c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'ê°€ì¡±' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m w2v = KeyedVectors.load_word2vec_format(model_dir, binary=\u001b[38;5;28;01mTrue\u001b[39;00m, limit=\u001b[32m500000\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m genre \u001b[38;5;129;01min\u001b[39;00m genre_name :\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43mw2v\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpositive\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgenre\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/gensim/models/keyedvectors.py:841\u001b[39m, in \u001b[36mKeyedVectors.most_similar\u001b[39m\u001b[34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[39m\n\u001b[32m    838\u001b[39m         weight[idx] = item[\u001b[32m1\u001b[39m]\n\u001b[32m    840\u001b[39m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m mean = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m all_keys = [\n\u001b[32m    843\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_index_for(key)\n\u001b[32m    844\u001b[39m ]\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/gensim/models/keyedvectors.py:518\u001b[39m, in \u001b[36mKeyedVectors.get_mean_vector\u001b[39m\u001b[34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[39m\n\u001b[32m    516\u001b[39m         total_weight += \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_missing:\n\u001b[32m--> \u001b[39m\u001b[32m518\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKey \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not present in vocabulary\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total_weight > \u001b[32m0\u001b[39m:\n\u001b[32m    521\u001b[39m     mean = mean / total_weight\n",
      "\u001b[31mKeyError\u001b[39m: \"Key 'ê°€ì¡±' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "data_dir = os.path.join(os.getenv(\"HOME\"), 'work/weat/data')\n",
    "model_dir = os.path.join(data_dir, 'GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# 50ë§Œê°œì˜ ë‹¨ì–´ë§Œ í™œìš©í•©ë‹ˆë‹¤. ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë‹¤ë©´ limit íŒŒë¼ë¯¸í„°ê°’ì„ ìƒëµí•˜ì—¬ 300ë§Œê°œë¥¼ ëª¨ë‘ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "w2v = KeyedVectors.load_word2vec_format(model_dir, binary=True, limit=500000)\n",
    "genre_name = ['SF', 'ê°€ì¡±', 'ê³µì—°', 'ê³µí¬', 'ê¸°íƒ€', 'ë‹¤íë©˜í„°ë¦¬', 'ë“œë¼ë§ˆ', 'ë©œë¡œë¡œë§¨ìŠ¤', 'ë®¤ì§€ì»¬', 'ë¯¸ìŠ¤í„°ë¦¬', 'ë²”ì£„', 'ì‚¬ê·¹', 'ì„œë¶€ê·¹',\n",
    "         'ì„±ì¸ë¬¼', 'ìŠ¤ë¦´ëŸ¬', 'ì• ë‹ˆë©”ì´ì…˜', 'ì•¡ì…˜', 'ì–´ë“œë²¤ì²˜', 'ì „ìŸ', 'ì½”ë¯¸ë””', 'íŒíƒ€ì§€']\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(model_dir, binary=True, limit=500000)\n",
    "\n",
    "for genre in genre_name :\n",
    "    w2v.most_similar(positive=[genre])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932c4fe3-9501-4463-9ca4-4d4e38fd1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "tokenized = []\n",
    "\n",
    "with open(file_name, 'r') as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line: break\n",
    "        words = okt.pos(line, stem=True, norm=True)\n",
    "        res = []\n",
    "        for w in words:\n",
    "            if w[1] in [\"Noun\"]:      # \"Adjective\", \"Verb\" ë“±ì„ í¬í•¨í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "                res.append(w[0])    # ëª…ì‚¬ì¼ ë•Œë§Œ tokenized ì— ì €ì¥í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "        tokenized.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89988735-57fd-4006-946f-a7146e6e12d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ì‘í’ˆ', 0.8795446753501892),\n",
       " ('ë‹¤íë©˜í„°ë¦¬', 0.841648519039154),\n",
       " ('ë“œë¼ë§ˆ', 0.8251305818557739),\n",
       " ('ì˜í™”ë¡œ', 0.8014270663261414),\n",
       " ('í˜•ì‹', 0.7982307076454163),\n",
       " ('ì£¼ì œ', 0.785247802734375),\n",
       " ('ì½”ë¯¸ë””', 0.7803109288215637),\n",
       " ('ê°ë™', 0.7775558233261108),\n",
       " ('ìŠ¤í† ë¦¬', 0.7725319266319275),\n",
       " ('ì˜ìƒ', 0.7705848813056946)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(tokenized, vector_size=100, window=5, min_count=3, sg=0)\n",
    "model.wv.most_similar(positive=['ì˜í™”'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96848e2d-a048-4762-a4f5-b2cf45b34ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    "\n",
    "#ëª…ì‚¬ë§Œ ì¶”ì¶œí•´ ë‹¨ì–´ì¥ ì œì‘\n",
    "def read_token(file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open(data_dir + '/' + file_name, 'r') as fread:\n",
    "        print(file_name, 'íŒŒì¼ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤.')\n",
    "        while True:\n",
    "            line = fread.readline()\n",
    "            if not line: break\n",
    "            tokenlist = okt.pos(line, stem=True, norm=True)\n",
    "            for word in tokenlist:\n",
    "                if word[1] in [\"Noun\"]:#, \"Adjective\", \"Verb\"]:\n",
    "                    result.append((word[0]))\n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db92c8-d8ea-4e2d-a59d-c57188cc1cc5",
   "metadata": {},
   "source": [
    "ì „ì²˜ë¦¬ë¥¼ í†µí•´ [ì˜í™”ì œ], (ë°°ìš° ë¶„) ê³¼ ê°™ì€ ì½”í¼ìŠ¤ë¥¼ ì œê±°, \n",
    "gensimì˜ summerize ë©”ì†Œë“œë¥¼ ì´ìš©í•´ ìš”ì•½ í›„ ë‹¨ì–´ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29dc6b49-edd5-40b8-b1e6-e2fa4af3dd18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.56.0-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (2.7.1+cu118)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /opt/conda/lib/python3.12/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /opt/conda/lib/python3.12/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /opt/conda/lib/python3.12/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.12/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /opt/conda/lib/python3.12/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /opt/conda/lib/python3.12/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /opt/conda/lib/python3.12/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /opt/conda/lib/python3.12/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.12/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Downloading transformers-4.56.0-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m802.0/802.0 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Installing collected packages: sentencepiece, safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7/7\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.9 huggingface-hub-0.34.4 regex-2025.9.1 safetensors-0.6.2 sentencepiece-0.2.1 tokenizers-0.22.0 transformers-4.56.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06556f7d-b5ff-4d1c-a633-44196a8b5e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import pipeline\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"gogamza/kobart-summarization\")\n",
    "okt = Okt()\n",
    "\n",
    "\n",
    "def clean_text(file_name) :\n",
    "    result = []\n",
    "    with open(data_dir + '/' + file_name, 'r') as fread:\n",
    "        while True:\n",
    "            line = fread.readline()\n",
    "            if not line: break\n",
    "            \n",
    "            #ê´„í˜¸ë¡œ ê°ì‹¸ì§„ ë‚´ìš© ì œê±°\n",
    "            cleaned_text = re.sub(r'\\[.*?\\]|\\(.*?\\)', '', line)\n",
    "            #í•œê¸€ê³¼ ê³µë°±ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì ì œê±°\n",
    "            cleaned_text = re.sub(r'[^ê°€-í£\\s]', '', cleaned_text)\n",
    "            #ë‹¤ì¤‘ ê³µë°±ì„ ë‹¨ì¼ê³µë°±ìœ¼ë¡œ ì¶•ì†Œ\n",
    "            cleaned_text = ' '.join(cleaned_text.split())\n",
    "            \n",
    "            if len(cleaned_text) < 50 :\n",
    "                continue\n",
    "            summary_result = summarizer(cleaned_text, max_length=100, min_length=20, do_sample=False)\n",
    "            summary_text = summary_result[0]['summary_text']\n",
    "\n",
    "            \n",
    "            tokenlist = okt.pos(summary_text, stem=True, norm=True)\n",
    "            \n",
    "            for word in tokenlist:\n",
    "                if word[1] in [\"Noun\"]:#, \"Adjective\", \"Verb\"]:\n",
    "                    result.append((word[0]))\n",
    "                    \n",
    "            return ' '.join(result)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5e03a-6ce8-49ab-9d72-731f044d185a",
   "metadata": {},
   "source": [
    "clean_text ë©”ì†Œë“œë¥¼ í†µí•´\n",
    "ê´„í˜¸ ì•ˆ ë…¸ì´ì¦ˆ, í•œê¸€ì„ ì œì™¸í•œ ë‚´ìš©, ë‹¤ì¤‘ ê³µë°±ì„ ì œê±°í•˜ì˜€ê³ \n",
    "summarizeë¥¼ í†µí•´ ì¢€ ë” í•µì‹¬ ë‹¨ì–´ë§Œ ì´ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "741c273d-ee2b-408d-9249-471d19ee4458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synopsis_art.txt íŒŒì¼ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "synopsis_gen.txt íŒŒì¼ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# 2ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ”ë° 10ë¶„ ê°€ëŸ‰ ê±¸ë¦½ë‹ˆë‹¤.\n",
    "cleaned_art = read_token(art_txt)\n",
    "cleaned_gen = read_token(gen_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e239a9a-f139-4332-8fdb-a408ef9d964f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synopsis_art.txt íŒŒì¼ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "synopsis_gen.txt íŒŒì¼ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# 2ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ”ë° 10ë¶„ ê°€ëŸ‰ ê±¸ë¦½ë‹ˆë‹¤.\n",
    "art = read_token(art_txt)\n",
    "gen = read_token(gen_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "441611a1-c005-44b4-8ce6-aa14ff237235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš´ë“œ ì—”ì§€ë‹ˆì–´ ìƒìš° ìœ ì§€íƒœ ë¶„ ì¹˜ë§¤\n",
      "ì‚¬ìš´ë“œ ì—”ì§€ë‹ˆì–´ ìƒìš° ìœ ì§€íƒœ ë¶„ ì¹˜ë§¤\n"
     ]
    }
   ],
   "source": [
    "print(art[:20])\n",
    "print(cleaned_art[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f92a3d5-0cb2-404d-b6c0-1067030fdab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 41082)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([art, gen])\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc34ed71-9321-4dd0-a842-5a38a68953e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 41082)\n"
     ]
    }
   ],
   "source": [
    "cleaned_vectorizer = TfidfVectorizer()\n",
    "cleaned_X = cleaned_vectorizer.fit_transform([cleaned_art, cleaned_gen])\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "398ce915-e920-440a-960f-2feb458f9168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜ˆìˆ ì˜í™”ë¥¼ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë“¤:\n",
      "ê·¸ë…€, ìì‹ , ì‹œì‘, ìœ„í•´, ì‚¬ë‘, ì‚¬ëŒ, ì˜í™”, ì¹œêµ¬, ë‚¨ì, ê°€ì¡±, ì´ì•¼ê¸°, ë§ˆì„, ì‚¬ê±´, ë§ˆìŒ, ì„¸ìƒ, ì•„ë²„ì§€, ì•„ì´, ì—„ë§ˆ, ëª¨ë“ , ì—¬ì, ëŒ€í•œ, ì„œë¡œ, ê³¼ì—°, ì‹œê°„, ë‹¤ì‹œ, ì•„ë“¤, ì†Œë…€, ì•„ë‚´, ë‹¤ë¥¸, ì˜í™”ì œ, ì‚¬ì´, ì„¸ê³„, ì‚¬ì‹¤, í•˜ë‚˜, ì ì , ë‚¨í¸, ê°ë…, ì—¬í–‰, ì¸ìƒ, ë°œê²¬, ëª¨ë‘, ìˆœê°„, ìš°ë¦¬, ê°€ì¥, ë§ˆì§€ë§‰, ì•„ë¹ , ìƒí™œ, í†µí•´, ëª¨ìŠµ, ê¸°ì–µ, ì£½ìŒ, ë¹„ë°€, í•™êµ, ìŒì•…, í•œí¸, ì†Œë…„, ìƒê°, ë„ì‹œ, ëª…ì˜, ê²°í˜¼, ì‚¬ê³ , ì „ìŸ, ìœ„ê¸°, ë•Œë¬¸, ì´ì œ, ìµœê³ , ì´ì, ê³¼ê±°, ì¼ìƒ, ê²½ì°°, ê°„ë‹¤, ìƒí™©, ë¯¸êµ­, ìš´ëª…, ê²°ì‹¬, ê´€ê³„, í˜„ì‹¤, ì§€ê¸ˆ, ë‹¨í¸, ì—¬ì¸, í•˜ë£¨, ì´ë¦„, ì´í›„, ì¤€ë¹„, ì¸ê°„, ë§Œë‚œ, ê°ì •, ì²˜ìŒ, êµ­ì œ, ëˆ„êµ¬, ì‚´ì¸, ì¶©ê²©, ë™ì•ˆ, ì¡´ì¬, ê·¸ë¦°, ì–´ë¨¸ë‹ˆ, ì—°ì¸, ê³„ì†, ë™ìƒ, ì‘í’ˆ, \n",
      "\n",
      "ì¼ë°˜ì˜í™”ë¥¼ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë“¤:\n",
      "ìì‹ , ê·¸ë…€, ì˜í™”ì œ, ìœ„í•´, ì‚¬ëŒ, ì‹œì‘, êµ­ì œ, ì˜í™”, ì¹œêµ¬, ì‚¬ë‘, ë‚¨ì, ì´ì•¼ê¸°, ëŒ€í•œ, ì„œìš¸, ì—¬ì, ì‚¬ê±´, ë‚¨í¸, ì•„ì´, ê°€ì¡±, ì•„ë²„ì§€, ë‹¤ë¥¸, ë§ˆì„, ì‹œê°„, ì—„ë§ˆ, ì•„ë“¤, ëª¨ë“ , ë‹¨í¸, ë§ˆìŒ, ì‚¬ì‹¤, ë‹¤ì‹œ, ì„¸ê³„, ëª¨ìŠµ, ì‘í’ˆ, í†µí•´, ìƒê°, ì„œë¡œ, ì„¸ìƒ, ë°œê²¬, ê°ë…, ì•„ë‚´, ê´€ê³„, ì†Œë…€, ì‚¬ì´, í•˜ë‚˜, ìš°ë¦¬, ì• ë‹ˆë©”ì´ì…˜, ë•Œë¬¸, ì—¬ì„±, ì£½ìŒ, ê³¼ì—°, ì ì , ì¸ê°„, ìƒí™œ, í•œí¸, ê²°í˜¼, ìƒí™©, ëª¨ë‘, ê¸°ì–µ, ëª…ì˜, ì†Œë…„, ì—¬í–‰, ê°€ì¥, ê°„ë‹¤, ìˆœê°„, ì´ì œ, ë„ì‹œ, ë¹„ë°€, í•™êµ, ê³¼ê±°, ê°€ì§€, ì´ì, ê²½ì°°, ë§ˆì§€ë§‰, ë¯¸êµ­, ë™ì•ˆ, ì „ìŸ, ì£¼ì¸ê³µ, ëŒ€í•´, ì¡´ì¬, í˜„ì‹¤, ì—°ì¶œ, ì‚¬ê³ , ì‚´ì¸, ì¼ìƒ, ì–´ë¨¸ë‹ˆ, ê³„ì†, ì‚¬íšŒ, ì¸ìƒ, ë‹¤íë©˜í„°ë¦¬, ë¶€ë¬¸, ì„¹ìŠ¤, ìµœê³ , ë°”ë¡œ, ì˜ë„, ë™ìƒ, í•˜ë£¨, ìœ„ê¸°, ê³„íš, ì •ì²´, í•œêµ­, "
     ]
    }
   ],
   "source": [
    "m1 = X[0].tocoo()   # artë¥¼ TF-IDFë¡œ í‘œí˜„í•œ sparse matrixë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "m2 = X[1].tocoo()   # genì„ TF-IDFë¡œ í‘œí˜„í•œ sparse matrixë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #artë¥¼ êµ¬ì„±í•˜ëŠ” ë‹¨ì–´ë“¤ì„ TF-IDFê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.\n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #genì„ êµ¬ì„±í•˜ëŠ” ë‹¨ì–´ë“¤ì„ TF-IDFê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "print('ì˜ˆìˆ ì˜í™”ë¥¼ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë“¤:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names_out()[w1[i][0]], end=', ')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('ì¼ë°˜ì˜í™”ë¥¼ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë“¤:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names_out()[w2[i][0]], end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94e159dc-547b-4f4b-904c-36a4c2e807fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì˜ˆìˆ ì˜í™”ë¥¼ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë“¤ ver.cleaned:\n",
      "ê·¸ë…€, ìì‹ , ì‹œì‘, ìœ„í•´, ì‚¬ë‘, ì‚¬ëŒ, ì˜í™”, ì¹œêµ¬, ë‚¨ì, ê°€ì¡±, ì´ì•¼ê¸°, ë§ˆì„, ì‚¬ê±´, ë§ˆìŒ, ì„¸ìƒ, ì•„ë²„ì§€, ì•„ì´, ì—„ë§ˆ, ëª¨ë“ , ì—¬ì, ëŒ€í•œ, ì„œë¡œ, ê³¼ì—°, ì‹œê°„, ë‹¤ì‹œ, ì•„ë“¤, ì†Œë…€, ì•„ë‚´, ë‹¤ë¥¸, ì˜í™”ì œ, ì‚¬ì´, ì„¸ê³„, ì‚¬ì‹¤, í•˜ë‚˜, ì ì , ë‚¨í¸, ê°ë…, ì—¬í–‰, ì¸ìƒ, ë°œê²¬, ëª¨ë‘, ìˆœê°„, ìš°ë¦¬, ê°€ì¥, ë§ˆì§€ë§‰, ì•„ë¹ , ìƒí™œ, í†µí•´, ëª¨ìŠµ, ê¸°ì–µ, ì£½ìŒ, ë¹„ë°€, í•™êµ, ìŒì•…, í•œí¸, ì†Œë…„, ìƒê°, ë„ì‹œ, ëª…ì˜, ê²°í˜¼, ì‚¬ê³ , ì „ìŸ, ìœ„ê¸°, ë•Œë¬¸, ì´ì œ, ìµœê³ , ì´ì, ê³¼ê±°, ì¼ìƒ, ê²½ì°°, ê°„ë‹¤, ìƒí™©, ë¯¸êµ­, ìš´ëª…, ê²°ì‹¬, ê´€ê³„, í˜„ì‹¤, ì§€ê¸ˆ, ë‹¨í¸, ì—¬ì¸, í•˜ë£¨, ì´ë¦„, ì´í›„, ì¤€ë¹„, ì¸ê°„, ë§Œë‚œ, ê°ì •, ì²˜ìŒ, êµ­ì œ, ëˆ„êµ¬, ì‚´ì¸, ì¶©ê²©, ë™ì•ˆ, ì¡´ì¬, ê·¸ë¦°, ì–´ë¨¸ë‹ˆ, ì—°ì¸, ê³„ì†, ë™ìƒ, ì‘í’ˆ, \n",
      "\n",
      "ì¼ë°˜ì˜í™”ë¥¼ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë“¤ ver.cleaned :\n",
      "ìì‹ , ê·¸ë…€, ì˜í™”ì œ, ìœ„í•´, ì‚¬ëŒ, ì‹œì‘, êµ­ì œ, ì˜í™”, ì¹œêµ¬, ì‚¬ë‘, ë‚¨ì, ì´ì•¼ê¸°, ëŒ€í•œ, ì„œìš¸, ì—¬ì, ì‚¬ê±´, ë‚¨í¸, ì•„ì´, ê°€ì¡±, ì•„ë²„ì§€, ë‹¤ë¥¸, ë§ˆì„, ì‹œê°„, ì—„ë§ˆ, ì•„ë“¤, ëª¨ë“ , ë‹¨í¸, ë§ˆìŒ, ì‚¬ì‹¤, ë‹¤ì‹œ, ì„¸ê³„, ëª¨ìŠµ, ì‘í’ˆ, í†µí•´, ìƒê°, ì„œë¡œ, ì„¸ìƒ, ë°œê²¬, ê°ë…, ì•„ë‚´, ê´€ê³„, ì†Œë…€, ì‚¬ì´, í•˜ë‚˜, ìš°ë¦¬, ì• ë‹ˆë©”ì´ì…˜, ë•Œë¬¸, ì—¬ì„±, ì£½ìŒ, ê³¼ì—°, ì ì , ì¸ê°„, ìƒí™œ, í•œí¸, ê²°í˜¼, ìƒí™©, ëª¨ë‘, ê¸°ì–µ, ëª…ì˜, ì†Œë…„, ì—¬í–‰, ê°€ì¥, ê°„ë‹¤, ìˆœê°„, ì´ì œ, ë„ì‹œ, ë¹„ë°€, í•™êµ, ê³¼ê±°, ê°€ì§€, ì´ì, ê²½ì°°, ë§ˆì§€ë§‰, ë¯¸êµ­, ë™ì•ˆ, ì „ìŸ, ì£¼ì¸ê³µ, ëŒ€í•´, ì¡´ì¬, í˜„ì‹¤, ì—°ì¶œ, ì‚¬ê³ , ì‚´ì¸, ì¼ìƒ, ì–´ë¨¸ë‹ˆ, ê³„ì†, ì‚¬íšŒ, ì¸ìƒ, ë‹¤íë©˜í„°ë¦¬, ë¶€ë¬¸, ì„¹ìŠ¤, ìµœê³ , ë°”ë¡œ, ì˜ë„, ë™ìƒ, í•˜ë£¨, ìœ„ê¸°, ê³„íš, ì •ì²´, í•œêµ­, "
     ]
    }
   ],
   "source": [
    "cleaned_m1 = X[0].tocoo()   # artë¥¼ TF-IDFë¡œ í‘œí˜„í•œ sparse matrixë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "cleaned_m2 = X[1].tocoo()   # genì„ TF-IDFë¡œ í‘œí˜„í•œ sparse matrixë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "\n",
    "cleaned_w1 = [[i, j] for i, j in zip(cleaned_m1.col, cleaned_m1.data)]\n",
    "cleaned_w2 = [[i, j] for i, j in zip(cleaned_m2.col, cleaned_m2.data)]\n",
    "\n",
    "cleaned_w1.sort(key=lambda x: x[1], reverse=True)   #artë¥¼ êµ¬ì„±í•˜ëŠ” ë‹¨ì–´ë“¤ì„ TF-IDFê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.\n",
    "cleaned_w2.sort(key=lambda x: x[1], reverse=True)   #genì„ êµ¬ì„±í•˜ëŠ” ë‹¨ì–´ë“¤ì„ TF-IDFê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "print('ì˜ˆìˆ ì˜í™”ë¥¼ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë“¤ ver.cleaned:')\n",
    "for i in range(100):\n",
    "    print(cleaned_vectorizer.get_feature_names_out()[cleaned_w1[i][0]], end=', ')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('ì¼ë°˜ì˜í™”ë¥¼ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë“¤ ver.cleaned :')\n",
    "for i in range(100):\n",
    "    print(cleaned_vectorizer.get_feature_names_out()[cleaned_w2[i][0]], end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c3a3ca6-278a-45f8-9d18-576341589054",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "w1_, w2_ = [], []\n",
    "for i in range(100):\n",
    "    w1_.append(vectorizer.get_feature_names_out()[w1[i][0]])\n",
    "    w2_.append(vectorizer.get_feature_names_out()[w2[i][0]])\n",
    "\n",
    "# w1ì—ë§Œ ìˆê³  w2ì—ëŠ” ì—†ëŠ”, ì˜ˆìˆ ì˜í™”ë¥¼ ì˜ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë¥¼ 15ê°œ ì¶”ì¶œí•œë‹¤.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break\n",
    "\n",
    "# w2ì—ë§Œ ìˆê³  w1ì—ëŠ” ì—†ëŠ”, ì¼ë°˜ì˜í™”ë¥¼ ì˜ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë¥¼ 15ê°œ ì¶”ì¶œí•œë‹¤.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3a3fb77-f480-4b19-84b1-68512d96956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "c_w1_, c_w2_ = [], []\n",
    "for i in range(100):\n",
    "    c_w1_.append(cleaned_vectorizer.get_feature_names_out()[cleaned_w1[i][0]])\n",
    "    c_w2_.append(cleaned_vectorizer.get_feature_names_out()[cleaned_w2[i][0]])\n",
    "\n",
    "# w1ì—ë§Œ ìˆê³  w2ì—ëŠ” ì—†ëŠ”, ì˜ˆìˆ ì˜í™”ë¥¼ ì˜ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë¥¼ 15ê°œ ì¶”ì¶œí•œë‹¤.\n",
    "c_target_art, c_target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (c_w1_[i] not in c_w2_) and (c_w1_[i] in model.wv): c_target_art.append(c_w1_[i])\n",
    "    if len(c_target_art) == n: break\n",
    "\n",
    "# w2ì—ë§Œ ìˆê³  w1ì—ëŠ” ì—†ëŠ”, ì¼ë°˜ì˜í™”ë¥¼ ì˜ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë¥¼ 15ê°œ ì¶”ì¶œí•œë‹¤.\n",
    "for i in range(100):\n",
    "    if (c_w2_[i] not in c_w1_) and (c_w2_[i] in model.wv): c_target_gen.append(c_w2_[i])\n",
    "    if len(c_target_gen) == n: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9302a6ff-3777-4c93-a7ad-db959d7a291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_art = ['ì•„ë¹ ', 'ìŒì•…', 'ìš´ëª…', 'ê²°ì‹¬', 'ì§€ê¸ˆ', 'ì—¬ì¸', 'ì´ë¦„', 'ì´í›„', 'ì¤€ë¹„', 'ë§Œë‚œ', 'ê°ì •', 'ì²˜ìŒ', 'ëˆ„êµ¬', 'ì¶©ê²©', 'ê·¸ë¦°']\n",
      "target_gen = ['ì„œìš¸', 'ì• ë‹ˆë©”ì´ì…˜', 'ì—¬ì„±', 'ê°€ì§€', 'ì£¼ì¸ê³µ', 'ëŒ€í•´', 'ì—°ì¶œ', 'ì‚¬íšŒ', 'ë‹¤íë©˜í„°ë¦¬', 'ë¶€ë¬¸', 'ì„¹ìŠ¤', 'ë°”ë¡œ', 'ì˜ë„', 'ê³„íš', 'ì •ì²´']\n",
      "---------------------------------------\n",
      "c_target_art = ['ì•„ë¹ ', 'ìŒì•…', 'ìš´ëª…', 'ê²°ì‹¬', 'ì§€ê¸ˆ', 'ì—¬ì¸', 'ì´ë¦„', 'ì´í›„', 'ì¤€ë¹„', 'ë§Œë‚œ', 'ê°ì •', 'ì²˜ìŒ', 'ëˆ„êµ¬', 'ì¶©ê²©', 'ê·¸ë¦°']\n",
      "c_target_gen = ['ì„œìš¸', 'ì• ë‹ˆë©”ì´ì…˜', 'ì—¬ì„±', 'ê°€ì§€', 'ì£¼ì¸ê³µ', 'ëŒ€í•´', 'ì—°ì¶œ', 'ì‚¬íšŒ', 'ë‹¤íë©˜í„°ë¦¬', 'ë¶€ë¬¸', 'ì„¹ìŠ¤', 'ë°”ë¡œ', 'ì˜ë„', 'ê³„íš', 'ì •ì²´']\n"
     ]
    }
   ],
   "source": [
    "print(f\"target_art = {target_art}\")\n",
    "print(f\"target_gen = {target_gen}\")\n",
    "\n",
    "print(\"---------------------------------------\")\n",
    "print(f\"c_target_art = {c_target_art}\")\n",
    "print(f\"c_target_gen = {c_target_gen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae4a3f4-e4af-4504-bb11-79b413348289",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "--------------------------------------------\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c470f9-7374-45e0-8624-b021d088e1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synopsis_SF.txt íŒŒì¼ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤.\n",
      "synopsis_SF.txt íŒŒì¼ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "genre_txt = ['synopsis_SF.txt', 'synopsis_family.txt', 'synopsis_show.txt', 'synopsis_horror.txt', 'synopsis_etc.txt',\n",
    "             'synopsis_documentary.txt', 'synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_musical.txt',\n",
    "             'synopsis_mystery.txt', 'synopsis_crime.txt', 'synopsis_historical.txt', 'synopsis_western.txt',\n",
    "             'synopsis_adult.txt', 'synopsis_thriller.txt', 'synopsis_animation.txt', 'synopsis_action.txt',\n",
    "             'synopsis_adventure.txt', 'synopsis_war.txt', 'synopsis_comedy.txt', 'synopsis_fantasy.txt']\n",
    "genre_name = ['SF', 'ê°€ì¡±', 'ê³µì—°', 'ê³µí¬(í˜¸ëŸ¬)', 'ê¸°íƒ€', 'ë‹¤íë©˜í„°ë¦¬', 'ë“œë¼ë§ˆ', 'ë©œë¡œë¡œë§¨ìŠ¤', 'ë®¤ì§€ì»¬', 'ë¯¸ìŠ¤í„°ë¦¬', 'ë²”ì£„', 'ì‚¬ê·¹', 'ì„œë¶€ê·¹(ì›¨ìŠ¤í„´)',\n",
    "         'ì„±ì¸ë¬¼(ì—ë¡œ)', 'ìŠ¤ë¦´ëŸ¬', 'ì• ë‹ˆë©”ì´ì…˜', 'ì•¡ì…˜', 'ì–´ë“œë²¤ì²˜', 'ì „ìŸ', 'ì½”ë¯¸ë””', 'íŒíƒ€ì§€']\n",
    "\n",
    "\n",
    "for text, name in zip(genre_txt, genre_name):\n",
    "       \n",
    "    genre_ = read_token(text)\n",
    "    \n",
    "    gnere_vectorizer = TfidfVectorizer()\n",
    "    genre_X = cleaned_vectorizer.fit_transform([genre_art, genre_gen])\n",
    "\n",
    "    genre_m1 = genre_X[0].tocoo()   # artë¥¼ TF-IDFë¡œ í‘œí˜„í•œ sparse matrixë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    genre_m2 = genre_X[1].tocoo()   # genì„ TF-IDFë¡œ í‘œí˜„í•œ sparse matrixë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    \n",
    "    genre_w1 = [[i, j] for i, j in zip(genre_m1.col, genre_m1.data)]\n",
    "    genre_w2 = [[i, j] for i, j in zip(genre_m2.col, genre_m2.data)]\n",
    "    \n",
    "    genre_w1.sort(key=lambda x: x[1], reverse=True)   #artë¥¼ êµ¬ì„±í•˜ëŠ” ë‹¨ì–´ë“¤ì„ TF-IDFê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.\n",
    "    genre_w2.sort(key=lambda x: x[1], reverse=True)   #genì„ êµ¬ì„±í•˜ëŠ” ë‹¨ì–´ë“¤ì„ TF-IDFê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    # print('ì˜ˆìˆ ì˜í™”ë¥¼ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë“¤ ver.cleaned:')\n",
    "    # for i in range(100):\n",
    "    #     print(cleaned_vectorizer.get_feature_names_out()[cleaned_w1[i][0]], end=', ')\n",
    "    \n",
    "    # print('\\n')\n",
    "    \n",
    "    # print('ì¼ë°˜ì˜í™”ë¥¼ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë“¤ ver.cleaned :')\n",
    "    \n",
    "    # for i in range(100):\n",
    "    #     print(cleaned_vectorizer.get_feature_names_out()[cleaned_w2[i][0]], end=', ')\n",
    "\n",
    "    n = 15\n",
    "    genre_target_art, genre_target_gen = [], []\n",
    "    for i in range(100):\n",
    "        # ğŸ’¥ ìˆ˜ì •: c_w1_, c_w2_ ë“± -> genre_w1_, genre_w2_ ë“±\n",
    "        if (genre_w1_[i] not in genre_w2_) and (genre_w1_[i] in model.wv): \n",
    "            genre_target_art.append(genre_w1_[i])\n",
    "        if len(genre_target_art) == n: \n",
    "            break\n",
    "    \n",
    "    # w2ì—ë§Œ ìˆê³  w1ì—ëŠ” ì—†ëŠ”, ì¼ë°˜ì˜í™”ë¥¼ ì˜ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´ë¥¼ 15ê°œ ì¶”ì¶œí•œë‹¤.\n",
    "    for i in range(100):\n",
    "        # ğŸ’¥ ìˆ˜ì •: c_w2_, c_w1_ ë“± -> genre_w2_, genre_w1_ ë“±\n",
    "        if (genre_w2_[i] not in genre_w1_) and (genre_w2_[i] in model.wv): \n",
    "            genre_target_gen.append(genre_w2_[i])\n",
    "        if len(genre_target_gen) == n: \n",
    "            break\n",
    "\n",
    "    # ğŸ’¥ ìµœì¢… ìˆ˜ì •: íŒŒì¼ ì´ë¦„(txt_file) ëŒ€ì‹  í•œê¸€ ì¥ë¥´ ì´ë¦„(name)ì„ ì‚¬ìš©í•´ ì¶œë ¥\n",
    "    print(f\"{name} target_art = {genre_target_art}\")\n",
    "    print(f\"{name} target_gen = {genre_target_gen}\")\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775424c-e94e-4f2c-9fa4-2ef79c77109a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
